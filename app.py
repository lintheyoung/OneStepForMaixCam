import streamlit as st
import subprocess
import threading
import os
import time
import json
import zipfile
import requests
import shutil
import yaml
import platform
import tempfile
from datetime import datetime
from pathlib import Path
from urllib.parse import urlparse
import glob
import random
import base64
import sys

# è·¨å¹³å°ä¿¡å·å¤„ç†
try:
    import signal
except ImportError:
    signal = None  # WindowsæŸäº›æƒ…å†µä¸‹å¯èƒ½ä¸æ”¯æŒæŸäº›ä¿¡å·

# çŠ¶æ€æ–‡ä»¶
STATUS_FILE = "test_status.json"
OUTPUT_FILE = "test_output.txt"
DATASET_INFO_FILE = "dataset_info.json"
CONVERSION_OUTPUT_FILE = "conversion_output.txt"
MAPPING_FILE = "pt_dataset_mapping.json"  # æ–°å¢: æ˜ å°„å…³ç³»æ–‡ä»¶

# Dockeré•œåƒé…ç½®
REQUIRED_DOCKER_IMAGES = [
    "lintheyoung/yolov11-trainer:latest",  # ç”¨äºè®­ç»ƒå’ŒONNXè½¬æ¢
    "lintheyoung/tpuc_dev_env_build"       # ç”¨äºCviModelè½¬æ¢
]

# ==================== è·¨å¹³å°å…¼å®¹æ€§å·¥å…·å‡½æ•° ====================

def get_platform_info():
    """è·å–å¹³å°ä¿¡æ¯"""
    return {
        "system": platform.system(),
        "machine": platform.machine(),
        "is_windows": platform.system() == "Windows",
        "is_linux": platform.system() == "Linux",
        "is_macos": platform.system() == "Darwin"
    }

def normalize_path_for_docker(local_path):
    """å°†æœ¬åœ°è·¯å¾„è½¬æ¢ä¸ºDockeræŒ‚è½½æ ¼å¼"""
    abs_path = os.path.abspath(local_path)
    
    if platform.system() == "Windows":
        # Windows: C:\path -> /c/path
        if len(abs_path) > 1 and abs_path[1] == ':':
            drive = abs_path[0].lower()
            path = abs_path[2:].replace('\\', '/')
            return f"/{drive}{path}"
    
    # Linux/Mac: ç›´æ¥ä½¿ç”¨ï¼Œä½†ç¡®ä¿ä½¿ç”¨æ­£æ–œæ 
    return abs_path.replace(os.sep, "/")

def safe_chmod(file_path, mode=0o755):
    """å®‰å…¨çš„chmodæ“ä½œï¼Œè·¨å¹³å°å…¼å®¹"""
    if platform.system() != "Windows":
        try:
            os.chmod(file_path, mode)
            return True
        except OSError as e:
            print(f"âš ï¸ è®¾ç½®æ–‡ä»¶æƒé™å¤±è´¥: {e}")
            return False
    return True  # Windowsä¸‹è·³è¿‡chmod

def terminate_process_cross_platform(pid):
    """è·¨å¹³å°è¿›ç¨‹ç»ˆæ­¢"""
    if not pid:
        return False
        
    try:
        if platform.system() == "Windows":
            # Windowsä½¿ç”¨taskkillï¼Œè®¾ç½®æ­£ç¡®çš„ç¼–ç 
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'
            env['CHCP'] = '65001'
            
            result = subprocess.run(f"taskkill /F /PID {pid}", 
                                  shell=True, check=False, 
                                  capture_output=True, text=True,
                                  encoding='utf-8', errors='replace', env=env)
            return result.returncode == 0
        else:
            # Linux/Macä½¿ç”¨ä¿¡å·
            if signal is None:
                print("ä¿¡å·æ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•ç»ˆæ­¢è¿›ç¨‹")
                return False
                
            try:
                os.kill(pid, signal.SIGTERM)  # å…ˆå°è¯•æ¸©å’Œç»ˆæ­¢
                time.sleep(2)
                # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿˜å­˜åœ¨
                os.kill(pid, 0)  # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜åœ¨
                os.kill(pid, signal.SIGKILL)  # å¼ºåˆ¶ç»ˆæ­¢
            except ProcessLookupError:
                pass  # è¿›ç¨‹å·²ç»ç»ˆæ­¢
            return True
    except Exception as e:
        print(f"ç»ˆæ­¢è¿›ç¨‹å¤±è´¥: {e}")
        return False

# ==================== æ·»åŠ ç¼–ç å®‰å…¨çš„subprocessåŒ…è£…å‡½æ•° ====================

def run_subprocess_safe(cmd, timeout=30, shell=False, cwd=None):
    """å®‰å…¨çš„subprocessè°ƒç”¨ï¼Œå¤„ç†ç¼–ç é—®é¢˜"""
    try:
        # è®¾ç½®ç¯å¢ƒå˜é‡å¼ºåˆ¶UTF-8è¾“å‡º
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        if platform.system() == "Windows":
            env['CHCP'] = '65001'  # UTF-8 code page for Windows
        
        if isinstance(cmd, str):
            shell = True
        
        result = subprocess.run(
            cmd,
            shell=shell,
            capture_output=True,
            text=True,
            timeout=timeout,
            encoding='utf-8',
            errors='replace',
            env=env,
            cwd=cwd
        )
        return result
    except Exception as e:
        print(f"subprocessæ‰§è¡Œå¤±è´¥: {e}")
        return None

def create_subprocess_safe(cmd, cwd=None):
    """åˆ›å»ºå®‰å…¨çš„subprocess.Popenï¼Œå¤„ç†ç¼–ç é—®é¢˜"""
    try:
        # è®¾ç½®ç¯å¢ƒå˜é‡å¼ºåˆ¶UTF-8è¾“å‡º
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        if platform.system() == "Windows":
            env['CHCP'] = '65001'  # UTF-8 code page for Windows
        
        process = subprocess.Popen(
            cmd,
            shell=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            encoding='utf-8',
            errors='replace',  # é‡åˆ°æ— æ³•è§£ç çš„å­—ç¬¦æ—¶æ›¿æ¢è€Œä¸æ˜¯æŠ¥é”™
            bufsize=1,
            env=env,
            cwd=cwd
        )
        return process
    except Exception as e:
        print(f"åˆ›å»ºsubprocesså¤±è´¥: {e}")
        return None

def create_directory_safe(directory_path):
    """å®‰å…¨åˆ›å»ºç›®å½•ï¼Œå¤„ç†æƒé™é—®é¢˜"""
    try:
        os.makedirs(directory_path, exist_ok=True)
        # åœ¨Linuxä¸‹è®¾ç½®åˆé€‚çš„æƒé™
        if platform.system() != "Windows":
            try:
                os.chmod(directory_path, 0o755)
            except OSError:
                pass
        return True
    except Exception as e:
        print(f"åˆ›å»ºç›®å½•å¤±è´¥ {directory_path}: {e}")
        return False

def get_temp_directory():
    """è·å–è·¨å¹³å°ä¸´æ—¶ç›®å½•"""
    return tempfile.gettempdir()

# ==================== Dockerç¯å¢ƒæ£€æŸ¥å‡½æ•° ====================

def check_docker_environment():
    """æ£€æŸ¥Dockerç¯å¢ƒæ˜¯å¦å¯ç”¨"""
    try:
        print("ğŸ” æ£€æŸ¥Dockerç¯å¢ƒ...")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡å¼ºåˆ¶UTF-8è¾“å‡ºï¼ˆWindowså…¼å®¹ï¼‰
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        if platform.system() == "Windows":
            env['CHCP'] = '65001'  # UTF-8 code page for Windows
        
        # æ£€æŸ¥Dockeræ˜¯å¦å®‰è£…
        result = subprocess.run(['docker', '--version'], 
                              capture_output=True, text=True, timeout=10,
                              encoding='utf-8', errors='replace', env=env)
        if result.returncode != 0:
            print("âŒ Dockeræœªå®‰è£…æˆ–æ— æ³•è®¿é—®")
            print("è¯·å®‰è£…Docker: https://docs.docker.com/get-docker/")
            return False
        
        print(f"âœ… Dockerå·²å®‰è£…: {result.stdout.strip()}")
        
        # æ£€æŸ¥Dockeræ˜¯å¦è¿è¡Œ
        result = subprocess.run(['docker', 'info'], 
                              capture_output=True, text=True, timeout=10,
                              encoding='utf-8', errors='replace', env=env)
        if result.returncode != 0:
            print("âŒ DockeræœåŠ¡æœªè¿è¡Œ")
            print("è¯·å¯åŠ¨DockeræœåŠ¡")
            return False
        
        print("âœ… DockeræœåŠ¡æ­£åœ¨è¿è¡Œ")
        
        # æ£€æŸ¥Dockeræƒé™ï¼ˆä¸»è¦é’ˆå¯¹Linuxï¼‰
        if not check_docker_permissions():
            return False
            
        return True
        
    except subprocess.TimeoutExpired:
        print("âŒ Dockerå‘½ä»¤è¶…æ—¶ï¼Œè¯·æ£€æŸ¥Dockeræ˜¯å¦æ­£å¸¸è¿è¡Œ")
        return False
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°Dockerå‘½ä»¤ï¼Œè¯·ç¡®è®¤Dockerå·²æ­£ç¡®å®‰è£…")
        return False
    except Exception as e:
        print(f"âŒ æ£€æŸ¥Dockerç¯å¢ƒæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return False

def check_docker_permissions():
    """æ£€æŸ¥Dockeræƒé™ï¼ˆLinuxç‰¹æœ‰é—®é¢˜ï¼‰"""
    try:
        # è®¾ç½®ç¯å¢ƒå˜é‡å¼ºåˆ¶UTF-8è¾“å‡º
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        if platform.system() == "Windows":
            env['CHCP'] = '65001'
        
        result = subprocess.run(['docker', 'ps'], 
                              capture_output=True, text=True, timeout=10,
                              encoding='utf-8', errors='replace', env=env)
        if result.returncode != 0:
            if "permission denied" in result.stderr.lower():
                print("âŒ Dockeræƒé™ä¸è¶³ï¼Œè¯·è¿è¡Œ:")
                print("  sudo usermod -aG docker $USER")
                print("  ç„¶åé‡æ–°ç™»å½•æˆ–é‡å¯ç³»ç»Ÿ")
                print("  æˆ–è€…ä½¿ç”¨sudoè¿è¡Œæ­¤åº”ç”¨")
                return False
            else:
                print(f"âŒ Dockerå‘½ä»¤æ‰§è¡Œå¤±è´¥: {result.stderr}")
                return False
        
        print("âœ… Dockeræƒé™æ£€æŸ¥é€šè¿‡")
        return True
    except Exception as e:
        print(f"âŒ æ£€æŸ¥Dockeræƒé™æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False

def check_nvidia_docker():
    """æ£€æŸ¥NVIDIA Dockeræ”¯æŒ"""
    try:
        print("ğŸ” æ£€æŸ¥NVIDIA Dockeræ”¯æŒ...")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡å¼ºåˆ¶UTF-8è¾“å‡º
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        if platform.system() == "Windows":
            env['CHCP'] = '65001'
        
        result = subprocess.run([
            'docker', 'run', '--rm', '--gpus', 'all', 
            'nvidia/cuda:11.8-base-ubuntu20.04', 'nvidia-smi'
        ], capture_output=True, text=True, timeout=30,
           encoding='utf-8', errors='replace', env=env)
        
        if result.returncode == 0:
            print("âœ… NVIDIA Dockeræ”¯æŒæ­£å¸¸")
            return True
        else:
            print("âš ï¸ NVIDIA Dockeræ”¯æŒä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
            return False
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥NVIDIA Dockeræ—¶å‡ºé”™: {e}")
        return False

def check_docker_image_exists(image_name):
    """æ£€æŸ¥Dockeré•œåƒæ˜¯å¦å­˜åœ¨"""
    try:
        # è®¾ç½®ç¯å¢ƒå˜é‡å¼ºåˆ¶UTF-8è¾“å‡º
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        if platform.system() == "Windows":
            env['CHCP'] = '65001'
        
        result = subprocess.run(['docker', 'images', '-q', image_name], 
                              capture_output=True, text=True, timeout=30,
                              encoding='utf-8', errors='replace', env=env)
        return result.returncode == 0 and result.stdout.strip() != ""
    except Exception as e:
        print(f"âŒ æ£€æŸ¥é•œåƒ {image_name} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return False

def pull_docker_image(image_name):
    """æ‹‰å–Dockeré•œåƒ"""
    try:
        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½Dockeré•œåƒ: {image_name}")
        print("è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡å¼ºåˆ¶UTF-8è¾“å‡º
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        if platform.system() == "Windows":
            env['CHCP'] = '65001'
        
        result = subprocess.run(['docker', 'pull', image_name], 
                              capture_output=True, text=True, timeout=1800,
                              encoding='utf-8', errors='replace', env=env)  # 30åˆ†é’Ÿè¶…æ—¶
        
        if result.returncode == 0:
            print(f"âœ… é•œåƒä¸‹è½½æˆåŠŸ: {image_name}")
            return True
        else:
            print(f"âŒ é•œåƒä¸‹è½½å¤±è´¥: {image_name}")
            print(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
            return False
            
    except subprocess.TimeoutExpired:
        print(f"âŒ ä¸‹è½½é•œåƒ {image_name} è¶…æ—¶")
        return False
    except Exception as e:
        print(f"âŒ ä¸‹è½½é•œåƒ {image_name} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return False

def check_and_pull_docker_images():
    """æ£€æŸ¥å¹¶ä¸‹è½½æ‰€éœ€çš„Dockeré•œåƒ"""
    print("ğŸ” æ£€æŸ¥æ‰€éœ€çš„Dockeré•œåƒ...")
    
    missing_images = []
    
    for image in REQUIRED_DOCKER_IMAGES:
        if check_docker_image_exists(image):
            print(f"âœ… é•œåƒå·²å­˜åœ¨: {image}")
        else:
            print(f"âš ï¸  é•œåƒä¸å­˜åœ¨: {image}")
            missing_images.append(image)
    
    if missing_images:
        print(f"\nğŸ“¥ éœ€è¦ä¸‹è½½ {len(missing_images)} ä¸ªé•œåƒ...")
        for image in missing_images:
            if not pull_docker_image(image):
                print(f"âŒ æ— æ³•ä¸‹è½½é•œåƒ: {image}")
                return False
    
    print("âœ… æ‰€æœ‰Dockeré•œåƒæ£€æŸ¥å®Œæˆ")
    return True

def initialize_environment():
    """åˆå§‹åŒ–ç¯å¢ƒæ£€æŸ¥"""
    platform_info = get_platform_info()
    
    print("=" * 50)
    print("ğŸš€ MaixCam YOLOv11è®­ç»ƒå¹³å° - ç¯å¢ƒåˆå§‹åŒ–")
    print("=" * 50)
    print(f"ğŸ–¥ï¸  æ“ä½œç³»ç»Ÿ: {platform_info['system']} ({platform_info['machine']})")
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    required_dirs = ["data", "models", "outputs", "transfer"]
    for dir_name in required_dirs:
        if not create_directory_safe(dir_name):
            print(f"âŒ åˆ›å»ºç›®å½•å¤±è´¥: {dir_name}")
            return False
    
    # æ£€æŸ¥Dockerç¯å¢ƒ
    if not check_docker_environment():
        print("âŒ Dockerç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œç¨‹åºå¯èƒ½æ— æ³•æ­£å¸¸è¿è¡Œ")
        return False
    
    # Linuxä¸‹æ£€æŸ¥NVIDIA Dockerï¼ˆå¯é€‰ï¼‰
    if platform_info['is_linux']:
        check_nvidia_docker()
    
    # æ£€æŸ¥å¹¶ä¸‹è½½Dockeré•œåƒ
    if not check_and_pull_docker_images():
        print("âŒ Dockeré•œåƒå‡†å¤‡å¤±è´¥ï¼Œç¨‹åºå¯èƒ½æ— æ³•æ­£å¸¸è¿è¡Œ")
        return False
    
    print("âœ… ç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼Œç¨‹åºå·²å‡†å¤‡å°±ç»ª")
    print("=" * 50)
    return True

# ==================== çŠ¶æ€ç®¡ç†å‡½æ•° ====================

def init_status():
    """åˆå§‹åŒ–çŠ¶æ€"""
    default_status = {
        "status": "idle",
        "pid": None,
        "timestamp": datetime.now().isoformat(),
        "current_run": None  # æ·»åŠ å½“å‰è¿è¡Œçš„ä»»åŠ¡æ ‡è¯†
    }
    
    if not os.path.exists(STATUS_FILE):
        with open(STATUS_FILE, 'w', encoding='utf-8') as f:
            json.dump(default_status, f)

def get_status():
    """è·å–çŠ¶æ€"""
    try:
        with open(STATUS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        init_status()
        return get_status()

def set_status(status, pid=None, current_run=None):
    """è®¾ç½®çŠ¶æ€"""
    status_data = get_status()
    status_data["status"] = status
    status_data["timestamp"] = datetime.now().isoformat()
    
    if pid is not None:
        status_data["pid"] = pid
        
    if current_run is not None:
        status_data["current_run"] = current_run
        
    with open(STATUS_FILE, 'w', encoding='utf-8') as f:
        json.dump(status_data, f)

# ==================== æ•°æ®é›†ç®¡ç†å‡½æ•° ====================

def save_dataset_info(info):
    """ä¿å­˜æ•°æ®é›†ä¿¡æ¯"""
    with open(DATASET_INFO_FILE, 'w', encoding='utf-8') as f:
        json.dump(info, f, ensure_ascii=False, indent=2)

def get_dataset_info():
    """è·å–æ•°æ®é›†ä¿¡æ¯"""
    try:
        if os.path.exists(DATASET_INFO_FILE):
            with open(DATASET_INFO_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        return None
    except:
        return None

def save_pt_dataset_mapping(pt_file_path, dataset_path, run_name):
    """ä¿å­˜ptæ–‡ä»¶å’Œæ•°æ®é›†çš„æ˜ å°„å…³ç³»"""
    try:
        # è¯»å–ç°æœ‰æ˜ å°„
        mapping = {}
        if os.path.exists(MAPPING_FILE):
            with open(MAPPING_FILE, 'r', encoding='utf-8') as f:
                mapping = json.load(f)
        
        # æ·»åŠ æ–°æ˜ å°„ - é€‚é…æ–°çš„æ•°æ®é›†ç»“æ„
        mapping[pt_file_path] = {
            "dataset_path": dataset_path,
            "run_name": run_name,
            "created_time": datetime.now().isoformat(),
            "images_path": os.path.join(dataset_path, "images")  # ç›´æ¥æŒ‡å‘imagesç›®å½•
        }
        
        # ä¿å­˜æ˜ å°„
        with open(MAPPING_FILE, 'w', encoding='utf-8') as f:
            json.dump(mapping, f, ensure_ascii=False, indent=2)
            
        return True
    except Exception as e:
        print(f"ä¿å­˜æ˜ å°„å…³ç³»å¤±è´¥: {e}")
        return False

def get_pt_dataset_mapping(pt_file_path):
    """è·å–ptæ–‡ä»¶å¯¹åº”çš„æ•°æ®é›†è·¯å¾„"""
    try:
        if os.path.exists(MAPPING_FILE):
            with open(MAPPING_FILE, 'r', encoding='utf-8') as f:
                mapping = json.load(f)
                
                # é¦–å…ˆå°è¯•ç›´æ¥åŒ¹é…
                if pt_file_path in mapping:
                    return mapping[pt_file_path]
                
                # å¦‚æœç›´æ¥åŒ¹é…å¤±è´¥ï¼Œå°è¯•ç»å¯¹è·¯å¾„åŒ¹é…
                abs_path = os.path.abspath(pt_file_path)
                if abs_path in mapping:
                    return mapping[abs_path]
                
                # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•è§„èŒƒåŒ–è·¯å¾„åŒ¹é…
                normalized_path = os.path.normpath(abs_path)
                if normalized_path in mapping:
                    return mapping[normalized_path]
                
                # æœ€åå°è¯•é€šè¿‡æ–‡ä»¶ååŒ¹é…ï¼ˆå¦‚æœè·¯å¾„åˆ†éš”ç¬¦ä¸åŒï¼‰
                for key in mapping.keys():
                    if os.path.normpath(key) == normalized_path:
                        return mapping[key]
                
        return None
    except Exception as e:
        print(f"è·å–æ˜ å°„å…³ç³»å¤±è´¥: {e}")
        return None

def get_dataset_labels():
    """ä»data.yamlä¸­è·å–æ ‡ç­¾åˆ—è¡¨"""
    try:
        data_yaml_path = "data/data.yaml"
        if os.path.exists(data_yaml_path):
            with open(data_yaml_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                if 'names' in data:
                    return data['names']
        return []
    except Exception as e:
        print(f"è·å–æ•°æ®é›†æ ‡ç­¾å¤±è´¥: {e}")
        return []

# ==================== MUDæ–‡ä»¶å’Œæ¨¡å‹åŒ…å¤„ç†å‡½æ•° ====================

def create_mud_file(cvimodel_path, conversion_name):
    """åˆ›å»ºMUDé…ç½®æ–‡ä»¶"""
    try:
        # è·å–cvimodelæ–‡ä»¶çš„ç›®å½•å’Œæ–‡ä»¶åï¼ˆä¸å¸¦æ‰©å±•åï¼‰
        cvimodel_dir = os.path.dirname(cvimodel_path)
        cvimodel_filename = os.path.basename(cvimodel_path)
        cvimodel_basename = os.path.splitext(cvimodel_filename)[0]
        
        # åˆ›å»ºmudæ–‡ä»¶è·¯å¾„
        mud_filename = f"{cvimodel_basename}.mud"
        mud_path = os.path.join(cvimodel_dir, mud_filename)
        
        # è·å–æ•°æ®é›†æ ‡ç­¾
        labels = get_dataset_labels()
        labels_str = ", ".join(labels) if labels else "object"
        
        # MUDæ–‡ä»¶å†…å®¹
        mud_content = f"""[basic]
type = cvimodel
model = {cvimodel_filename}

[extra]
model_type = yolo11
input_type = rgb
mean = 0, 0, 0
scale = 0.00392156862745098, 0.00392156862745098, 0.00392156862745098
anchors = 10,13, 16,30, 33,23, 30,61, 62,45, 59,119, 116,90, 156,198, 373,326
labels = {labels_str}
"""
        
        # å†™å…¥MUDæ–‡ä»¶
        with open(mud_path, 'w', encoding='utf-8') as f:
            f.write(mud_content)
        
        return mud_path, f"âœ… æˆåŠŸåˆ›å»ºMUDé…ç½®æ–‡ä»¶: {mud_filename}"
        
    except Exception as e:
        return None, f"âŒ åˆ›å»ºMUDæ–‡ä»¶å¤±è´¥: {str(e)}"

def create_model_package_zip(cvimodel_path, mud_path, conversion_name):
    """åˆ›å»ºæ¨¡å‹åŒ…ZIPæ–‡ä»¶"""
    try:
        # è·å–æ–‡ä»¶æ‰€åœ¨ç›®å½•
        model_dir = os.path.dirname(cvimodel_path)
        
        # è·å–æ–‡ä»¶åŸºç¡€åç§°ï¼ˆä¸å¸¦æ‰©å±•åï¼‰
        cvimodel_basename = os.path.splitext(os.path.basename(cvimodel_path))[0]
        zip_filename = f"{cvimodel_basename}.zip"
        zip_path = os.path.join(model_dir, zip_filename)
        
        # åˆ›å»ºZIPæ–‡ä»¶
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # æ·»åŠ cvimodelæ–‡ä»¶
            zipf.write(cvimodel_path, os.path.basename(cvimodel_path))
            # æ·»åŠ mudæ–‡ä»¶
            zipf.write(mud_path, os.path.basename(mud_path))
        
        # è·å–æ–‡ä»¶å¤§å°
        zip_size = os.path.getsize(zip_path) / (1024 * 1024)  # MB
        
        return zip_path, f"âœ… æˆåŠŸåˆ›å»ºæ¨¡å‹åŒ…: {zip_filename} ({zip_size:.2f} MB)"
        
    except Exception as e:
        return None, f"âŒ åˆ›å»ºæ¨¡å‹åŒ…å¤±è´¥: {str(e)}"

# ==================== å›¾ç‰‡å¤„ç†å‡½æ•° ====================

def collect_images_from_dataset(images_path, target_count=200):
    """ä»æ•°æ®é›†çš„imagesç›®å½•ä¸­æ”¶é›†å›¾ç‰‡"""
    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff', '*.webp']
    all_images = []
    
    # æ£€æŸ¥imagesç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(images_path):
        print(f"Imagesç›®å½•ä¸å­˜åœ¨: {images_path}")
        return []
    
    # æ”¶é›†imagesæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å›¾ç‰‡
    for ext in image_extensions:
        all_images.extend(glob.glob(os.path.join(images_path, ext)))
        all_images.extend(glob.glob(os.path.join(images_path, ext.upper())))
    
    # å»é‡å¹¶éšæœºæ‰“ä¹±
    all_images = list(set(all_images))
    random.shuffle(all_images)
    
    print(f"åœ¨ {images_path} ä¸­æ‰¾åˆ° {len(all_images)} å¼ å›¾ç‰‡")
    
    return all_images

def copy_images_to_transfer(images_list, target_dir, target_count=200):
    """å¤åˆ¶å›¾ç‰‡åˆ°transferç›®å½•"""
    try:
        # åˆ›å»ºimagesç›®å½•
        images_dir = os.path.join(target_dir, "images")
        create_directory_safe(images_dir)
        
        copied_images = []
        
        # å¦‚æœå›¾ç‰‡æ•°é‡è¶³å¤Ÿ
        if len(images_list) >= target_count:
            selected_images = images_list[:target_count]
            for i, img_path in enumerate(selected_images):
                if os.path.exists(img_path):
                    file_ext = os.path.splitext(img_path)[1]
                    target_name = f"image_{i+1:03d}{file_ext}"
                    target_path = os.path.join(images_dir, target_name)
                    shutil.copy2(img_path, target_path)
                    copied_images.append(target_path)
        
        # å¦‚æœå›¾ç‰‡æ•°é‡ä¸å¤Ÿï¼Œé‡å¤å¤åˆ¶å¹¶é‡å‘½å
        else:
            available_count = len(images_list)
            if available_count == 0:
                return [], None
            
            for i in range(target_count):
                source_img = images_list[i % available_count]  # å¾ªç¯ä½¿ç”¨ç°æœ‰å›¾ç‰‡
                if os.path.exists(source_img):
                    file_ext = os.path.splitext(source_img)[1]
                    target_name = f"image_{i+1:03d}{file_ext}"
                    target_path = os.path.join(images_dir, target_name)
                    shutil.copy2(source_img, target_path)
                    copied_images.append(target_path)
        
        # å¤åˆ¶ä¸€å¼ å›¾ç‰‡ä½œä¸ºtestå›¾ç‰‡
        test_image = None
        if copied_images:
            test_source = copied_images[0]
            file_ext = os.path.splitext(test_source)[1]
            test_image = os.path.join(target_dir, f"test{file_ext}")
            shutil.copy2(test_source, test_image)
        
        return copied_images, test_image
        
    except Exception as e:
        print(f"å¤åˆ¶å›¾ç‰‡å¤±è´¥: {e}")
        return [], None

# ==================== æ•°æ®é›†ä¸‹è½½å’Œå¤„ç†å‡½æ•° ====================

def download_file(url, local_filename, progress_placeholder=None):
    """ä¸‹è½½æ–‡ä»¶"""
    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        downloaded_size = 0
        
        with open(local_filename, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded_size += len(chunk)
                    
                    if progress_placeholder and total_size > 0:
                        progress = downloaded_size / total_size
                        progress_placeholder.progress(progress)
        
        return True
    except Exception as e:
        st.error(f"ä¸‹è½½å¤±è´¥: {str(e)}")
        return False

def extract_zip(zip_path, extract_to):
    """è§£å‹ZIPæ–‡ä»¶"""
    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_to)
        return True
    except Exception as e:
        st.error(f"è§£å‹å¤±è´¥: {str(e)}")
        return False

def find_data_yaml(directory):
    """é€’å½’æŸ¥æ‰¾data.yamlæ–‡ä»¶"""
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.lower() in ['data.yaml', 'data.yml']:
                return os.path.join(root, file)
    return None

def validate_dataset(data_yaml_path):
    """éªŒè¯æ•°æ®é›†æ ¼å¼"""
    try:
        with open(data_yaml_path, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        
        # æ£€æŸ¥å¿…è¦çš„å­—æ®µ
        required_fields = ['train', 'val', 'names']
        missing_fields = [field for field in required_fields if field not in data]
        
        if missing_fields:
            return False, f"ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}"
        
        # æ£€æŸ¥ç±»åˆ«æ•°é‡
        if 'nc' not in data:
            data['nc'] = len(data['names'])
        
        return True, data
    except Exception as e:
        return False, f"è§£æYAMLæ–‡ä»¶å¤±è´¥: {str(e)}"

def process_uploaded_dataset(uploaded_file):
    """å¤„ç†ä¸Šä¼ çš„æ•°æ®é›†"""
    try:
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = os.path.join(get_temp_directory(), "dataset_upload")
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        create_directory_safe(temp_dir)
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        zip_path = os.path.join(temp_dir, "dataset.zip")
        with open(zip_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        # è§£å‹æ–‡ä»¶
        extract_dir = os.path.join(temp_dir, "extracted")
        if not extract_zip(zip_path, extract_dir):
            return False, "è§£å‹å¤±è´¥"
        
        # æŸ¥æ‰¾data.yamlæ–‡ä»¶
        data_yaml_path = find_data_yaml(extract_dir)
        if not data_yaml_path:
            return False, "æœªæ‰¾åˆ°data.yamlæ–‡ä»¶"
        
        # éªŒè¯æ•°æ®é›†
        is_valid, result = validate_dataset(data_yaml_path)
        if not is_valid:
            return False, result
        
        # ç§»åŠ¨åˆ°dataç›®å½•
        data_dir = "data"
        if os.path.exists(data_dir):
            # å¤‡ä»½åŸæœ‰æ•°æ®
            backup_dir = f"data_backup_{int(time.time())}"
            shutil.move(data_dir, backup_dir)
            st.info(f"åŸæ•°æ®é›†å·²å¤‡ä»½åˆ°: {backup_dir}")
        
        # ç§»åŠ¨æ–°æ•°æ®é›†
        dataset_root = os.path.dirname(data_yaml_path)
        shutil.move(dataset_root, data_dir)
        
        # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            "source": "upload",
            "filename": uploaded_file.name,
            "upload_time": datetime.now().isoformat(),
            "classes": result['names'],
            "num_classes": len(result['names'])
        }
        save_dataset_info(dataset_info)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        shutil.rmtree(temp_dir)
        
        return True, "æ•°æ®é›†ä¸Šä¼ æˆåŠŸ"
        
    except Exception as e:
        return False, f"å¤„ç†ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {str(e)}"

def process_url_dataset(url):
    """å¤„ç†URLä¸‹è½½çš„æ•°æ®é›†"""
    try:
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = os.path.join(get_temp_directory(), "dataset_download")
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        create_directory_safe(temp_dir)
        
        # ä¸‹è½½æ–‡ä»¶
        parsed_url = urlparse(url)
        filename = os.path.basename(parsed_url.path) or "dataset.zip"
        zip_path = os.path.join(temp_dir, filename)
        
        # æ˜¾ç¤ºä¸‹è½½è¿›åº¦
        progress_placeholder = st.empty()
        progress_placeholder.text("å¼€å§‹ä¸‹è½½...")
        progress_bar = st.progress(0)
        
        # ä¸‹è½½
        response = requests.get(url, stream=True)
        response.raise_for_status()
        response.encoding = 'utf-8'  # æ˜ç¡®è®¾ç½®ç¼–ç 
        
        total_size = int(response.headers.get('content-length', 0))
        downloaded_size = 0
        
        with open(zip_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded_size += len(chunk)
                    
                    if total_size > 0:
                        progress = downloaded_size / total_size
                        progress_bar.progress(progress)
                        progress_placeholder.text(f"ä¸‹è½½ä¸­... {downloaded_size/(1024*1024):.1f}MB / {total_size/(1024*1024):.1f}MB")
        
        progress_placeholder.text("ä¸‹è½½å®Œæˆï¼Œå¼€å§‹è§£å‹...")
        
        # è§£å‹æ–‡ä»¶
        extract_dir = os.path.join(temp_dir, "extracted")
        if not extract_zip(zip_path, extract_dir):
            return False, "è§£å‹å¤±è´¥"
        
        # æŸ¥æ‰¾data.yamlæ–‡ä»¶
        data_yaml_path = find_data_yaml(extract_dir)
        if not data_yaml_path:
            return False, "æœªæ‰¾åˆ°data.yamlæ–‡ä»¶"
        
        # éªŒè¯æ•°æ®é›†
        is_valid, result = validate_dataset(data_yaml_path)
        if not is_valid:
            return False, result
        
        # ç§»åŠ¨åˆ°dataç›®å½•
        data_dir = "data"
        if os.path.exists(data_dir):
            # å¤‡ä»½åŸæœ‰æ•°æ®
            backup_dir = f"data_backup_{int(time.time())}"
            shutil.move(data_dir, backup_dir)
            st.info(f"åŸæ•°æ®é›†å·²å¤‡ä»½åˆ°: {backup_dir}")
        
        # ç§»åŠ¨æ–°æ•°æ®é›†
        dataset_root = os.path.dirname(data_yaml_path)
        shutil.move(dataset_root, data_dir)
        
        # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            "source": "url",
            "url": url,
            "filename": filename,
            "download_time": datetime.now().isoformat(),
            "classes": result['names'],
            "num_classes": len(result['names'])
        }
        save_dataset_info(dataset_info)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        shutil.rmtree(temp_dir)
        progress_placeholder.empty()
        progress_bar.empty()
        
        return True, "æ•°æ®é›†ä¸‹è½½å¹¶é…ç½®æˆåŠŸ"
        
    except Exception as e:
        return False, f"å¤„ç†URLæ•°æ®é›†å¤±è´¥: {str(e)}"

# ==================== è¾“å‡ºå¤„ç†å‡½æ•° ====================

def read_output():
    """è¯»å–è¾“å‡º"""
    try:
        if os.path.exists(OUTPUT_FILE):
            with open(OUTPUT_FILE, 'r', encoding='utf-8', errors='replace') as f:
                return f.read()
        return ""
    except Exception as e:
        return f"è¯»å–è¾“å‡ºå¤±è´¥: {str(e)}"

def read_conversion_output():
    """è¯»å–è½¬æ¢è¾“å‡º"""
    try:
        if os.path.exists(CONVERSION_OUTPUT_FILE):
            with open(CONVERSION_OUTPUT_FILE, 'r', encoding='utf-8', errors='replace') as f:
                return f.read()
        return ""
    except Exception as e:
        return f"è¯»å–è½¬æ¢è¾“å‡ºå¤±è´¥: {str(e)}"

def clear_output():
    """æ¸…ç©ºè¾“å‡º"""
    try:
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            f.write("")
    except Exception as e:
        print(f"æ¸…ç©ºè¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")

def clear_conversion_output():
    """æ¸…ç©ºè½¬æ¢è¾“å‡º"""
    try:
        with open(CONVERSION_OUTPUT_FILE, 'w', encoding='utf-8') as f:
            f.write("")
    except Exception as e:
        print(f"æ¸…ç©ºè½¬æ¢è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")

# ==================== CviModelå¤„ç†å‡½æ•° ====================

def find_and_move_cvimodel(transfer_dir, conversion_name, selected_model_name):
    """åœ¨workspaceç›®å½•ä¸­æŸ¥æ‰¾.cvimodelæ–‡ä»¶å¹¶ç§»åŠ¨åˆ°é¡¶å±‚ç›®å½•"""
    try:
        # ä»æ¨¡å‹æ–‡ä»¶åä¸­æå–åŸºæœ¬åç§°ï¼ˆä¾‹å¦‚ï¼šbest.pt -> bestï¼‰
        model_base_name = os.path.splitext(os.path.basename(selected_model_name))[0]
        
        # æŸ¥æ‰¾workspaceç›®å½•
        workspace_dir = os.path.join(transfer_dir, "workspace")
        if not os.path.exists(workspace_dir):
            return None, None, None, "æœªæ‰¾åˆ°workspaceç›®å½•"
        
        # æŸ¥æ‰¾.cvimodelæ–‡ä»¶
        cvimodel_files = []
        for file in os.listdir(workspace_dir):
            if file.endswith('.cvimodel'):
                cvimodel_files.append(file)
        
        if not cvimodel_files:
            return None, None, None, "æœªæ‰¾åˆ°.cvimodelæ–‡ä»¶"
        
        # å¯»æ‰¾åŒ¹é…çš„æ–‡ä»¶ï¼ˆä¼˜å…ˆæŸ¥æ‰¾åŒ…å«æ¨¡å‹åŸºæœ¬åç§°çš„æ–‡ä»¶ï¼‰
        target_cvimodel = None
        for file in cvimodel_files:
            if model_base_name in file:
                target_cvimodel = file
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª
        if not target_cvimodel:
            target_cvimodel = cvimodel_files[0]
        
        # æ„é€ æ–°çš„æ–‡ä»¶åï¼šexport_æ—¶é—´æˆ³_int8.cvimodel
        new_filename = f"{conversion_name}_int8.cvimodel"
        
        # æºæ–‡ä»¶è·¯å¾„å’Œç›®æ ‡æ–‡ä»¶è·¯å¾„
        source_path = os.path.join(workspace_dir, target_cvimodel)
        target_path = os.path.join(transfer_dir, new_filename)
        
        # ç§»åŠ¨å¹¶é‡å‘½åæ–‡ä»¶
        shutil.move(source_path, target_path)
        
        # åˆ›å»ºMUDæ–‡ä»¶
        mud_path, mud_message = create_mud_file(target_path, conversion_name)
        
        # åˆ›å»ºæ¨¡å‹åŒ…ZIPæ–‡ä»¶
        zip_path = None
        zip_message = ""
        if mud_path:
            zip_path, zip_message = create_model_package_zip(target_path, mud_path, conversion_name)
        
        return target_path, mud_path, zip_path, f"âœ… æˆåŠŸç§»åŠ¨å¹¶é‡å‘½å: {target_cvimodel} -> {new_filename}\n{mud_message}\n{zip_message}"
        
    except Exception as e:
        return None, None, None, f"âŒ ç§»åŠ¨.cvimodelæ–‡ä»¶å¤±è´¥: {str(e)}"

# ==================== Dockerå‘½ä»¤æ„å»ºå‡½æ•° ====================

def build_docker_training_command(model, epochs, imgsz, run_name):
    """æ„å»ºDockerè®­ç»ƒå‘½ä»¤"""
    # è·å–å½“å‰ç›®å½•çš„ç»å¯¹è·¯å¾„
    current_dir = os.getcwd()
    data_path = os.path.join(current_dir, "data")
    models_path = os.path.join(current_dir, "models")
    outputs_path = os.path.join(current_dir, "outputs")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    for path in [data_path, models_path, outputs_path]:
        create_directory_safe(path)
    
    # è½¬æ¢ä¸ºDockeræŒ‚è½½æ ¼å¼
    docker_data_path = normalize_path_for_docker(data_path)
    docker_models_path = normalize_path_for_docker(models_path)
    docker_outputs_path = normalize_path_for_docker(outputs_path)
    
    # æ„å»ºDockerå‘½ä»¤
    docker_command = f'''docker run --gpus all --name yolov11-{run_name} --rm --shm-size=4g -v "{docker_data_path}:/workspace/data" -v "{docker_models_path}:/workspace/models" -v "{docker_outputs_path}:/workspace/outputs" lintheyoung/yolov11-trainer:latest bash -c "cd /workspace/models && yolo train data=/workspace/data/data.yaml model={model} epochs={epochs} imgsz={imgsz} project=/workspace/outputs name={run_name}"'''
    
    return docker_command, data_path, models_path, outputs_path

def build_docker_conversion_command(model_path, format, imgsz_height, imgsz_width, opset, conversion_name):
    """æ„å»ºDockerè½¬æ¢å‘½ä»¤"""
    # è·å–å½“å‰ç›®å½•çš„ç»å¯¹è·¯å¾„
    current_dir = os.getcwd()
    data_path = os.path.join(current_dir, "data")
    models_path = os.path.join(current_dir, "models")
    outputs_path = os.path.join(current_dir, "outputs")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    for path in [data_path, models_path, outputs_path]:
        create_directory_safe(path)
    
    # è½¬æ¢æ¨¡å‹è·¯å¾„ä¸ºDockerå®¹å™¨å†…è·¯å¾„
    docker_model_path = model_path.replace(outputs_path, "/workspace/outputs")
    docker_model_path = docker_model_path.replace(os.sep, "/")
    
    # è½¬æ¢ä¸ºDockeræŒ‚è½½æ ¼å¼
    docker_data_path = normalize_path_for_docker(data_path)
    docker_models_path = normalize_path_for_docker(models_path)
    docker_outputs_path = normalize_path_for_docker(outputs_path)
    
    # æ„å»ºDockerå‘½ä»¤
    docker_command = f'''docker run --gpus all --name yolo-export-{conversion_name} --rm --shm-size=4g -v "{docker_data_path}:/workspace/data" -v "{docker_models_path}:/workspace/models" -v "{docker_outputs_path}:/workspace/outputs" lintheyoung/yolov11-trainer:latest bash -c "yolo export model={docker_model_path} format={format} imgsz={imgsz_height},{imgsz_width} opset={opset} batch=1"'''
    
    return docker_command

def build_docker_cvimodel_command(transfer_dir):
    """æ„å»ºCviModelè½¬æ¢å‘½ä»¤"""
    # è·å–transferç›®å½•çš„ç»å¯¹è·¯å¾„
    abs_transfer_dir = os.path.abspath(transfer_dir)
    docker_transfer_path = normalize_path_for_docker(abs_transfer_dir)
    
    # æ„å»ºDockerå‘½ä»¤
    docker_command = f'''docker run --rm -it -v "{docker_transfer_path}:/workspace" lintheyoung/tpuc_dev_env_build bash -c "cd /workspace && ./convert_cvimodel.sh"'''
    
    return docker_command

# ==================== è®­ç»ƒå’Œè½¬æ¢å‡½æ•° ====================

def run_docker_training(model, epochs, imgsz):
    """è¿è¡ŒDockerè®­ç»ƒ"""
    def training_task():
        try:
            # è·å–å½“å‰æ—¶é—´æˆ³ï¼ˆç²¾ç¡®åˆ°ç§’ï¼‰ä½œä¸ºè®­ç»ƒåç§°
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            run_name = f"train_{timestamp}"
            
            set_status("running", current_run=run_name)
            clear_output()
            
            # æ„å»ºDockerå‘½ä»¤
            docker_command, data_path, models_path, outputs_path = build_docker_training_command(
                model, epochs, imgsz, run_name
            )
            
            # å»ºç«‹æ˜ å°„å…³ç³» - è®­ç»ƒå¼€å§‹å‰å°±çŸ¥é“ptæ–‡ä»¶çš„æœ€ç»ˆä½ç½®
            future_weights_dir = os.path.join(outputs_path, run_name, "weights")
            future_best_pt = os.path.join(future_weights_dir, "best.pt")
            future_last_pt = os.path.join(future_weights_dir, "last.pt")
            
            # ä¿å­˜æ˜ å°„å…³ç³»
            save_pt_dataset_mapping(future_best_pt, data_path, run_name)
            save_pt_dataset_mapping(future_last_pt, data_path, run_name)
            
            # å¯åŠ¨è¿›ç¨‹ - ä½¿ç”¨å®‰å…¨çš„subprocessåˆ›å»ºå‡½æ•°
            process = create_subprocess_safe(docker_command)
            
            if process is None:
                set_status("failed")
                with open(OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                    f.write("\nâŒ æ— æ³•å¯åŠ¨Dockerè®­ç»ƒè¿›ç¨‹")
                return
            
            set_status("running", process.pid, run_name)
            
            # å®æ—¶è¯»å–è¾“å‡º
            with open(OUTPUT_FILE, 'w', encoding='utf-8', errors='replace') as f:
                f.write(f"å¼€å§‹æ‰§è¡Œå‘½ä»¤:\n{docker_command}\n\n")
                f.write(f"å·²å»ºç«‹æ˜ å°„å…³ç³»:\n")
                f.write(f"  - {future_best_pt} -> {data_path}\n")
                f.write(f"  - {future_last_pt} -> {data_path}\n\n")
                f.flush()
                
                for line in iter(process.stdout.readline, ''):
                    if line:
                        try:
                            f.write(line)
                            f.flush()
                        except UnicodeEncodeError:
                            # å¦‚æœé‡åˆ°ç¼–ç é—®é¢˜ï¼Œå°è¯•æ¸…ç†å­—ç¬¦
                            clean_line = line.encode('utf-8', errors='replace').decode('utf-8')
                            f.write(clean_line)
                            f.flush()
            
            # ç­‰å¾…å®Œæˆ
            return_code = process.wait()
            
            if return_code == 0:
                set_status("completed", current_run=run_name)
                with open(OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                    f.write("\nâœ… è®­ç»ƒå®Œæˆ!")
            else:
                set_status("failed", current_run=run_name)
                with open(OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                    f.write(f"\nâŒ è®­ç»ƒå¤±è´¥ï¼Œé€€å‡ºç : {return_code}")
                    
        except Exception as e:
            set_status("failed")
            with open(OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                f.write(f"\nâŒ æ‰§è¡Œå‡ºé”™: {str(e)}")
    
    # åå°çº¿ç¨‹è¿è¡Œ
    thread = threading.Thread(target=training_task)
    thread.daemon = True
    thread.start()

def run_model_conversion(model_path, format="onnx", opset=18):
    """è¿è¡Œæ¨¡å‹è½¬æ¢"""
    def conversion_task():
        try:
            # è·å–å½“å‰æ—¶é—´æˆ³ä½œä¸ºè½¬æ¢åç§°
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            conversion_name = f"export_{timestamp}"
            
            # è®¾ç½®çŠ¶æ€ä¸ºè½¬æ¢ä¸­
            set_status("converting", current_run=conversion_name)
            clear_conversion_output()
            
            # è·å–ptæ–‡ä»¶çš„æ˜ å°„å…³ç³»
            mapping_info = get_pt_dataset_mapping(model_path)
            
            # åˆ›å»ºtransferç›®å½•
            transfer_dir = os.path.join("transfer", conversion_name)
            create_directory_safe(transfer_dir)
            
            with open(CONVERSION_OUTPUT_FILE, 'w', encoding='utf-8', errors='replace') as f:
                f.write(f"å¼€å§‹æ¨¡å‹è½¬æ¢æµç¨‹ - {conversion_name}\n")
                f.write(f"æ¨¡å‹æ–‡ä»¶: {model_path}\n")
                f.write(f"ç»å¯¹è·¯å¾„: {os.path.abspath(model_path)}\n")
                f.write(f"Transferç›®å½•: {transfer_dir}\n\n")
                
                # è°ƒè¯•æ˜ å°„å…³ç³»æŸ¥æ‰¾è¿‡ç¨‹
                f.write("=== æŸ¥æ‰¾æ•°æ®é›†æ˜ å°„å…³ç³» ===\n")
                f.write(f"æŸ¥æ‰¾è·¯å¾„: {model_path}\n")
                f.write(f"ç»å¯¹è·¯å¾„: {os.path.abspath(model_path)}\n")
                
                # æ˜¾ç¤ºæ‰€æœ‰æ˜ å°„å…³ç³»
                if os.path.exists(MAPPING_FILE):
                    with open(MAPPING_FILE, 'r', encoding='utf-8') as map_f:
                        all_mappings = json.load(map_f)
                        f.write(f"æ˜ å°„æ–‡ä»¶ä¸­å…±æœ‰ {len(all_mappings)} æ¡è®°å½•:\n")
                        for key in all_mappings.keys():
                            f.write(f"  - {key}\n")
                else:
                    f.write("æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨\n")
                
                f.write(f"æ˜ å°„æŸ¥æ‰¾ç»“æœ: {'æ‰¾åˆ°' if mapping_info else 'æœªæ‰¾åˆ°'}\n\n")
                f.flush()
                
                # å¦‚æœæ‰¾åˆ°æ˜ å°„å…³ç³»ï¼Œå…ˆå¤åˆ¶æ•°æ®é›†å›¾ç‰‡
                if mapping_info:
                    f.write("=== æ•°æ®é›†å›¾ç‰‡æ”¶é›†ä¸å¤åˆ¶ ===\n")
                    f.write(f"æ•°æ®é›†è·¯å¾„: {mapping_info['dataset_path']}\n")
                    f.write(f"Imagesè·¯å¾„: {mapping_info['images_path']}\n\n")
                    f.flush()
                    
                    # æ”¶é›†å›¾ç‰‡ - ä½¿ç”¨æ–°çš„å‡½æ•°è°ƒç”¨æ–¹å¼
                    f.write("æ­£åœ¨æ”¶é›†å›¾ç‰‡...\n")
                    f.flush()
                    all_images = collect_images_from_dataset(
                        mapping_info['images_path'], 
                        target_count=200
                    )
                    
                    f.write(f"æ‰¾åˆ° {len(all_images)} å¼ å›¾ç‰‡\n")
                    f.flush()
                    
                    if all_images:
                        # å¤åˆ¶å›¾ç‰‡
                        f.write("æ­£åœ¨å¤åˆ¶å›¾ç‰‡åˆ°transferç›®å½•...\n")
                        f.flush()
                        copied_images, test_image = copy_images_to_transfer(all_images, transfer_dir, 200)
                        
                        f.write(f"æˆåŠŸå¤åˆ¶ {len(copied_images)} å¼ å›¾ç‰‡åˆ° images/ æ–‡ä»¶å¤¹\n")
                        if test_image:
                            f.write(f"åˆ›å»ºæµ‹è¯•å›¾ç‰‡: {os.path.basename(test_image)}\n")
                        f.write("å›¾ç‰‡å¤åˆ¶å®Œæˆ!\n\n")
                        f.flush()
                    else:
                        f.write("âš ï¸ æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶ï¼Œè·³è¿‡å›¾ç‰‡å¤åˆ¶æ­¥éª¤\n\n")
                        f.flush()
                else:
                    f.write("âš ï¸ æœªæ‰¾åˆ°æ•°æ®é›†æ˜ å°„å…³ç³»ï¼Œè·³è¿‡å›¾ç‰‡å¤åˆ¶æ­¥éª¤\n\n")
                    f.flush()
                
                # å¼€å§‹ONNXè½¬æ¢
                f.write("=== ONNXæ¨¡å‹è½¬æ¢ ===\n")
                f.flush()
            
            # å®šä¹‰æœŸæœ›çš„å›¾åƒå°ºå¯¸ï¼ˆç¬¦åˆMaixCamçš„å°ºå¯¸ï¼‰
            imgsz_height = 224
            imgsz_width = 320
            
            # æ„å»ºDockerå‘½ä»¤
            docker_command = build_docker_conversion_command(
                model_path, format, imgsz_height, imgsz_width, opset, conversion_name
            )
            
            # å¯åŠ¨è¿›ç¨‹ - ä½¿ç”¨å®‰å…¨çš„subprocessåˆ›å»ºå‡½æ•°
            process = create_subprocess_safe(docker_command)
            
            if process is None:
                set_status("failed", current_run=conversion_name)
                with open(CONVERSION_OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                    f.write("\nâŒ æ— æ³•å¯åŠ¨Dockerè½¬æ¢è¿›ç¨‹")
                return
            
            # è®°å½•è¿›ç¨‹ID
            set_status("converting", process.pid, conversion_name)
            
            # å®æ—¶è¯»å–è¾“å‡º
            with open(CONVERSION_OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                f.write(f"æ‰§è¡Œè½¬æ¢å‘½ä»¤:\n{docker_command}\n\n")
                f.flush()
                
                for line in iter(process.stdout.readline, ''):
                    if line:
                        try:
                            f.write(line)
                            f.flush()
                        except UnicodeEncodeError:
                            clean_line = line.encode('utf-8', errors='replace').decode('utf-8')
                            f.write(clean_line)
                            f.flush()
            
            # ç­‰å¾…å®Œæˆ
            return_code = process.wait()
            
            # è½¬æ¢å®Œæˆåï¼Œå¤åˆ¶ONNXæ¨¡å‹åˆ°transferç›®å½•
            with open(CONVERSION_OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                if return_code == 0:
                    f.write("\n=== ONNXè½¬æ¢æˆåŠŸï¼Œå¤åˆ¶æ¨¡å‹æ–‡ä»¶ ===\n")
                    
                    # æŸ¥æ‰¾ç”Ÿæˆçš„ONNXæ–‡ä»¶
                    model_dir = os.path.dirname(model_path)
                    onnx_files = []
                    
                    if os.path.exists(model_dir):
                        for file in os.listdir(model_dir):
                            if file.endswith(".onnx"):
                                onnx_path = os.path.join(model_dir, file)
                                onnx_files.append(onnx_path)
                    
                    if onnx_files:
                        for onnx_file in onnx_files:
                            target_onnx = os.path.join(transfer_dir, os.path.basename(onnx_file))
                            shutil.copy2(onnx_file, target_onnx)
                            f.write(f"å·²å¤åˆ¶ONNXæ¨¡å‹: {os.path.basename(onnx_file)}\n")
                        
                        f.write(f"\nâœ… ONNXè½¬æ¢å’Œæ–‡ä»¶å¤åˆ¶å®Œæˆ: {transfer_dir}\n")
                        
                        # å¤åˆ¶convert_cvimodel.shæ–‡ä»¶
                        f.write("\n=== å¤åˆ¶è½¬æ¢è„šæœ¬ ===\n")
                        f.flush()
                        
                        convert_script_path = "convert_cvimodel.sh"
                        if os.path.exists(convert_script_path):
                            target_script_path = os.path.join(transfer_dir, "convert_cvimodel.sh")
                            shutil.copy2(convert_script_path, target_script_path)
                            
                            # è®¾ç½®æ‰§è¡Œæƒé™
                            safe_chmod(target_script_path, 0o755)
                            
                            f.write(f"âœ… å·²å¤åˆ¶è½¬æ¢è„šæœ¬: {convert_script_path}\n")
                            f.flush()
                            
                            # æ‰§è¡ŒCviModelè½¬æ¢
                            f.write("\n=== æ‰§è¡ŒCviModelè½¬æ¢ ===\n")
                            f.write(f"åˆ‡æ¢åˆ°ç›®å½•: {transfer_dir}\n")
                            f.flush()
                            
                            # æ„å»ºdockerå‘½ä»¤
                            cvi_docker_command = build_docker_cvimodel_command(transfer_dir)
                            
                            f.write(f"æ‰§è¡Œå‘½ä»¤:\n{cvi_docker_command}\n\n")
                            f.flush()
                            
                            # å¯åŠ¨CviModelè½¬æ¢è¿›ç¨‹ - ä½¿ç”¨å®‰å…¨çš„subprocessåˆ›å»ºå‡½æ•°
                            cvi_process = create_subprocess_safe(cvi_docker_command, cwd=os.path.abspath(transfer_dir))
                            
                            if cvi_process is None:
                                f.write("âŒ æ— æ³•å¯åŠ¨CviModelè½¬æ¢è¿›ç¨‹\n")
                                f.write("ONNXæ¨¡å‹ä»å¯æ­£å¸¸ä½¿ç”¨\n")
                            else:
                                # å®æ—¶è¯»å–CviModelè½¬æ¢è¾“å‡º
                                f.write("CviModelè½¬æ¢è¾“å‡º:\n")
                                f.write("-" * 50 + "\n")
                                f.flush()
                                
                                for line in iter(cvi_process.stdout.readline, ''):
                                    if line:
                                        try:
                                            f.write(line)
                                            f.flush()
                                        except UnicodeEncodeError:
                                            clean_line = line.encode('utf-8', errors='replace').decode('utf-8')
                                            f.write(clean_line)
                                            f.flush()
                                
                                # ç­‰å¾…CviModelè½¬æ¢å®Œæˆ
                                cvi_return_code = cvi_process.wait()
                                
                                f.write("-" * 50 + "\n")
                                if cvi_return_code == 0:
                                    f.write("âœ… CviModelè½¬æ¢å®Œæˆ!\n")
                                    
                                    # æŸ¥æ‰¾å¹¶ç§»åŠ¨.cvimodelæ–‡ä»¶ï¼ŒåŒæ—¶åˆ›å»ºMUDæ–‡ä»¶å’ŒZIPåŒ…
                                    f.write("\n=== å¤„ç†CviModelæ–‡ä»¶ ===\n")
                                    f.flush()
                                    
                                    # ä»æ¨¡å‹è·¯å¾„ä¸­æå–æ–‡ä»¶å
                                    selected_model_name = os.path.basename(model_path)
                                    moved_file_path, mud_file_path, zip_file_path, move_message = find_and_move_cvimodel(
                                        transfer_dir, conversion_name, selected_model_name
                                    )
                                    
                                    f.write(f"{move_message}\n")
                                    
                                    if moved_file_path:
                                        f.write(f"CviModelæ–‡ä»¶è·¯å¾„: {moved_file_path}\n")
                                        f.write(f"CviModelæ–‡ä»¶å¤§å°: {os.path.getsize(moved_file_path) / (1024*1024):.2f} MB\n")
                                    
                                    if mud_file_path:
                                        f.write(f"MUDé…ç½®æ–‡ä»¶è·¯å¾„: {mud_file_path}\n")
                                        f.write(f"MUDæ–‡ä»¶å¤§å°: {os.path.getsize(mud_file_path) / 1024:.2f} KB\n")
                                    
                                    if zip_file_path:
                                        f.write(f"æ¨¡å‹åŒ…ZIPè·¯å¾„: {zip_file_path}\n")
                                        f.write(f"ZIPæ–‡ä»¶å¤§å°: {os.path.getsize(zip_file_path) / (1024*1024):.2f} MB\n")
                                    
                                    f.write(f"\nğŸ‰ å®Œæ•´çš„MaixCamæ¨¡å‹åŒ…å·²åˆ›å»º: {transfer_dir}\n")
                                    f.write("åŒ…å«å†…å®¹:\n")
                                    f.write("  - images/ (200å¼ è®­ç»ƒå›¾ç‰‡)\n")
                                    f.write("  - test.png/jpg (æµ‹è¯•å›¾ç‰‡)\n")
                                    f.write("  - *.onnx (ONNXæ¨¡å‹)\n")
                                    f.write("  - convert_cvimodel.sh (è½¬æ¢è„šæœ¬)\n")
                                    if moved_file_path:
                                        final_cvimodel_filename = os.path.basename(moved_file_path)
                                        f.write(f"  - {final_cvimodel_filename} (MaixCamä¼˜åŒ–æ¨¡å‹) ğŸ¯\n")
                                    if mud_file_path:
                                        final_mud_filename = os.path.basename(mud_file_path)
                                        f.write(f"  - {final_mud_filename} (MUDé…ç½®æ–‡ä»¶) ğŸ“‹\n")
                                    if zip_file_path:
                                        final_zip_filename = os.path.basename(zip_file_path)
                                        f.write(f"  - {final_zip_filename} (å®Œæ•´æ¨¡å‹åŒ…) ğŸ“¦\n")
                                    
                                else:
                                    f.write(f"âŒ CviModelè½¬æ¢å¤±è´¥ï¼Œé€€å‡ºç : {cvi_return_code}\n")
                                    f.write("ONNXæ¨¡å‹ä»å¯æ­£å¸¸ä½¿ç”¨\n")
                            
                        else:
                            f.write(f"âš ï¸ æœªæ‰¾åˆ°è½¬æ¢è„šæœ¬: {convert_script_path}\n")
                            f.write("è¯·ç¡®ä¿convert_cvimodel.shæ–‡ä»¶å­˜åœ¨äºåº”ç”¨æ ¹ç›®å½•\n")
                            f.write("ONNXè½¬æ¢å·²å®Œæˆï¼Œå¯æ‰‹åŠ¨è¿›è¡ŒCviModelè½¬æ¢\n")
                    else:
                        f.write("âš ï¸ æœªæ‰¾åˆ°ç”Ÿæˆçš„ONNXæ–‡ä»¶\n")
                    
                    set_status("completed", current_run=conversion_name)
                    f.write("\nâœ… æ¨¡å‹è½¬æ¢å’Œæ‰“åŒ…å®Œæˆ!")
                else:
                    set_status("failed", current_run=conversion_name)
                    f.write(f"\nâŒ æ¨¡å‹è½¬æ¢å¤±è´¥ï¼Œé€€å‡ºç : {return_code}")
                    
        except Exception as e:
            set_status("failed")
            with open(CONVERSION_OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                f.write(f"\nâŒ æ‰§è¡Œå‡ºé”™: {str(e)}")
    
    # åå°çº¿ç¨‹è¿è¡Œ
    thread = threading.Thread(target=conversion_task)
    thread.daemon = True
    thread.start()

def stop_training():
    """åœæ­¢è®­ç»ƒ"""
    status = get_status()
    pid = status.get("pid")
    
    if pid:
        try:
            success = terminate_process_cross_platform(pid)
            set_status("stopped")
            with open(OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                if success:
                    f.write(f"\nâ¹ï¸ è®­ç»ƒå·²æ‰‹åŠ¨åœæ­¢ (PID: {pid})")
                else:
                    f.write(f"\nâš ï¸ å°è¯•åœæ­¢è®­ç»ƒ (PID: {pid})ï¼Œè¯·æ£€æŸ¥è¿›ç¨‹çŠ¶æ€")
                
        except Exception as e:
            with open(OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                f.write(f"\nâŒ åœæ­¢å¤±è´¥: {str(e)}")

def stop_conversion():
    """åœæ­¢è½¬æ¢è¿‡ç¨‹"""
    status = get_status()
    pid = status.get("pid")
    
    if pid:
        try:
            success = terminate_process_cross_platform(pid)
            set_status("stopped")
            with open(CONVERSION_OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                if success:
                    f.write(f"\nâ¹ï¸ æ¨¡å‹è½¬æ¢å·²æ‰‹åŠ¨åœæ­¢ (PID: {pid})")
                else:
                    f.write(f"\nâš ï¸ å°è¯•åœæ­¢æ¨¡å‹è½¬æ¢ (PID: {pid})ï¼Œè¯·æ£€æŸ¥è¿›ç¨‹çŠ¶æ€")
                
        except Exception as e:
            with open(CONVERSION_OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                f.write(f"\nâŒ åœæ­¢å¤±è´¥: {str(e)}")

# ==================== ä¿¡æ¯æå–å’Œæ˜¾ç¤ºå‡½æ•° ====================

def extract_training_info(output_content):
    """æå–è®­ç»ƒå…³é”®ä¿¡æ¯"""
    lines = output_content.split('\n')
    info = {
        "current_epoch": None,
        "total_epochs": None,
        "latest_metrics": None,
        "progress_percentage": 0
    }
    
    # ä»æœ€æ–°çš„å‡ è¡Œä¸­æå–ä¿¡æ¯
    for line in reversed(lines[-20:]):
        # æå–epochä¿¡æ¯
        if "Epoch" in line and "/" in line:
            try:
                epoch_part = line.split("Epoch")[1].split()[0]
                if "/" in epoch_part:
                    current, total = epoch_part.split("/")
                    info["current_epoch"] = int(current)
                    info["total_epochs"] = int(total)
                    info["progress_percentage"] = (int(current) / int(total)) * 100
                    break
            except:
                pass
    
    # æå–æœ€æ–°çš„æŒ‡æ ‡ä¿¡æ¯
    for line in reversed(lines[-10:]):
        if "mAP50" in line and "all" in line:
            info["latest_metrics"] = line.strip()
            break
    
    return info

def find_training_models():
    """æŸ¥æ‰¾è®­ç»ƒäº§ç”Ÿçš„æ¨¡å‹æ–‡ä»¶"""
    models = []
    outputs_dir = "./outputs"
    
    if os.path.exists(outputs_dir):
        for train_dir in os.listdir(outputs_dir):
            weights_dir = os.path.join(outputs_dir, train_dir, "weights")
            if os.path.isdir(weights_dir):
                # æŸ¥æ‰¾best.ptå’Œlast.ptæ–‡ä»¶
                for weight_file in ["best.pt", "last.pt"]:
                    weight_path = os.path.join(weights_dir, weight_file)
                    if os.path.exists(weight_path):
                        # æ·»åŠ æ¨¡å‹ä¿¡æ¯
                        model_info = {
                            "name": f"{train_dir}/{weight_file}",
                            "path": weight_path,
                            "size": os.path.getsize(weight_path) / (1024 * 1024),  # MB
                            "time": datetime.fromtimestamp(os.path.getmtime(weight_path))
                        }
                        models.append(model_info)
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢
    models.sort(key=lambda x: x["time"], reverse=True)
    return models

def find_converted_cvimodels():
    """æŸ¥æ‰¾è½¬æ¢å®Œæˆçš„.cvimodelæ–‡ä»¶"""
    cvimodels = []
    transfer_dir = "transfer"
    
    if os.path.exists(transfer_dir):
        for export_dir in os.listdir(transfer_dir):
            if export_dir.startswith('export_'):
                export_path = os.path.join(transfer_dir, export_dir)
                if os.path.isdir(export_path):
                    # æŸ¥æ‰¾.cvimodelæ–‡ä»¶
                    for file in os.listdir(export_path):
                        if file.endswith('.cvimodel'):
                            cvimodel_path = os.path.join(export_path, file)
                            if os.path.isfile(cvimodel_path):
                                # æ·»åŠ æ¨¡å‹ä¿¡æ¯
                                cvimodel_info = {
                                    "name": file,
                                    "path": cvimodel_path,
                                    "size": os.path.getsize(cvimodel_path) / (1024 * 1024),  # MB
                                    "time": datetime.fromtimestamp(os.path.getmtime(cvimodel_path)),
                                    "export_dir": export_dir
                                }
                                cvimodels.append(cvimodel_info)
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢
    cvimodels.sort(key=lambda x: x["time"], reverse=True)
    return cvimodels

def find_model_packages():
    """æŸ¥æ‰¾æ¨¡å‹åŒ…ZIPæ–‡ä»¶"""
    packages = []
    transfer_dir = "transfer"
    
    if os.path.exists(transfer_dir):
        for export_dir in os.listdir(transfer_dir):
            if export_dir.startswith('export_'):
                export_path = os.path.join(transfer_dir, export_dir)
                if os.path.isdir(export_path):
                    # æŸ¥æ‰¾ZIPæ–‡ä»¶
                    for file in os.listdir(export_path):
                        if file.endswith('.zip') and '_int8.zip' in file:
                            zip_path = os.path.join(export_path, file)
                            if os.path.isfile(zip_path):
                                # æ·»åŠ åŒ…ä¿¡æ¯
                                package_info = {
                                    "name": file,
                                    "path": zip_path,
                                    "size": os.path.getsize(zip_path) / (1024 * 1024),  # MB
                                    "time": datetime.fromtimestamp(os.path.getmtime(zip_path)),
                                    "export_dir": export_dir
                                }
                                packages.append(package_info)
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢
    packages.sort(key=lambda x: x["time"], reverse=True)
    return packages

def display_results():
    """æ˜¾ç¤ºè®­ç»ƒç»“æœ"""
    # è·å–å½“å‰è¿è¡ŒçŠ¶æ€
    status = get_status()
    current_run = status.get("current_run")
    current_status = status.get("status")
    
    # å¦‚æœå½“å‰æ­£åœ¨è¿è¡Œè®­ç»ƒï¼Œåˆ™ä¸æ˜¾ç¤ºç»“æœ
    if current_status == "running":
        st.info("ğŸ”„ è®­ç»ƒè¿›è¡Œä¸­ï¼Œå®Œæˆåå°†æ˜¾ç¤ºç»“æœ")
        return
    
    # å¦‚æœæ²¡æœ‰å½“å‰è¿è¡Œçš„ä»»åŠ¡ï¼Œåˆ™å¯»æ‰¾æœ€æ–°çš„ç»“æœ
    if not current_run and current_status != "completed":
        outputs_dir = "./outputs"
        if os.path.exists(outputs_dir):
            # æŸ¥æ‰¾æœ€æ–°çš„è®­ç»ƒç»“æœ
            train_dirs = []
            for item in os.listdir(outputs_dir):
                item_path = os.path.join(outputs_dir, item)
                if os.path.isdir(item_path) and item.startswith('train_'):
                    train_dirs.append(item)
            
            if train_dirs:
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„
                current_run = max(train_dirs, key=lambda x: os.path.getctime(os.path.join(outputs_dir, x)))
    
    # å¦‚æœæœ‰å½“å‰ä»»åŠ¡æˆ–æ‰¾åˆ°äº†æœ€æ–°çš„ç»“æœ
    if current_run:
        results_path = os.path.join("./outputs", current_run)
        
        if os.path.exists(results_path):
            st.subheader("ğŸ“Š è®­ç»ƒç»“æœ")
            st.write(f"ç»“æœç›®å½•: {results_path}")
            
            # æ˜¾ç¤ºç»“æœå›¾ç‰‡
            image_files = ['results.png', 'confusion_matrix.png', 'F1_curve.png', 'PR_curve.png']
            
            cols = st.columns(2)
            col_idx = 0
            
            for img_file in image_files:
                img_path = os.path.join(results_path, img_file)
                if os.path.exists(img_path):
                    with cols[col_idx % 2]:
                        st.image(img_path, caption=img_file.replace('.png', '').replace('_', ' ').title())
                    col_idx += 1
            
            # æ˜¾ç¤ºæƒé‡æ–‡ä»¶
            weights_dir = os.path.join(results_path, 'weights')
            if os.path.exists(weights_dir):
                st.subheader("ğŸ’¾ æ¨¡å‹æƒé‡")
                for weight_file in os.listdir(weights_dir):
                    weight_path = os.path.join(weights_dir, weight_file)
                    if os.path.isfile(weight_path):
                        file_size = os.path.getsize(weight_path) / (1024 * 1024)  # MB
                        st.write(f"ğŸ“ {weight_file} ({file_size:.1f} MB)")
        else:
            st.info("æš‚æ— è®­ç»ƒç»“æœï¼ˆå¯ä»¥åˆ·æ–°ä¸€ä¸‹ï¼‰")
    else:
        st.info("æš‚æ— è®­ç»ƒç»“æœï¼ˆå¯ä»¥åˆ·æ–°ä¸€ä¸‹ï¼‰")

# ==================== UIéƒ¨åˆ†å‡½æ•° ====================

def dataset_management_section():
    """æ•°æ®é›†ç®¡ç†éƒ¨åˆ†"""
    st.subheader("ğŸ“¦ æ•°æ®é›†ç®¡ç†")
    
    # æ˜¾ç¤ºå½“å‰æ•°æ®é›†ä¿¡æ¯
    dataset_info = get_dataset_info()
    data_yaml_exists = os.path.exists("data/data.yaml")
    
    if data_yaml_exists and dataset_info:
        st.success("âœ… æ•°æ®é›†å·²é…ç½®")
        
        col1, col2 = st.columns(2)
        with col1:
            st.info(f"**æ¥æº:** {'æ–‡ä»¶ä¸Šä¼ ' if dataset_info['source'] == 'upload' else 'URLä¸‹è½½'}")
            if dataset_info['source'] == 'upload':
                st.info(f"**æ–‡ä»¶å:** {dataset_info['filename']}")
                st.info(f"**ä¸Šä¼ æ—¶é—´:** {dataset_info['upload_time'][:19]}")
            else:
                st.info(f"**URL:** {dataset_info['url']}")
                st.info(f"**ä¸‹è½½æ—¶é—´:** {dataset_info['download_time'][:19]}")
        
        with col2:
            st.info(f"**ç±»åˆ«æ•°é‡:** {dataset_info['num_classes']}")
            st.info(f"**ç±»åˆ«åç§°:** {', '.join(dataset_info['classes'][:5])}{'...' if len(dataset_info['classes']) > 5 else ''}")
        
        # æ˜¾ç¤ºæ•°æ®é›†è¯¦ç»†ä¿¡æ¯
        with st.expander("ğŸ“‹ æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯"):
            try:
                with open("data/data.yaml", 'r', encoding='utf-8') as f:
                    yaml_content = f.read()
                st.code(yaml_content, language='yaml')
            except:
                st.error("æ— æ³•è¯»å–data.yamlæ–‡ä»¶")
                
    elif data_yaml_exists:
        st.warning("âš ï¸ å‘ç°æ•°æ®é›†æ–‡ä»¶ä½†æ— é…ç½®ä¿¡æ¯")
    else:
        st.warning("âš ï¸ æœªé…ç½®æ•°æ®é›†")
    
    # æ•°æ®é›†é…ç½®é€‰é¡¹
    st.markdown("### ğŸ”§ é…ç½®æ–°æ•°æ®é›†")
    
    # é€‰æ‹©æ•°æ®é›†æ¥æº
    dataset_source = st.radio(
        "é€‰æ‹©æ•°æ®é›†æ¥æº:",
        ["ğŸ“ ä¸Šä¼ ZIPæ–‡ä»¶", "ğŸŒ ä»URLä¸‹è½½"],
        horizontal=True
    )
    
    if dataset_source == "ğŸ“ ä¸Šä¼ ZIPæ–‡ä»¶":
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ æ•°æ®é›†ZIPæ–‡ä»¶",
            type=['zip'],
            help="è¯·ä¸Šä¼ åŒ…å«data.yamlé…ç½®æ–‡ä»¶çš„YOLOæ ¼å¼æ•°æ®é›†"
        )
        
        if uploaded_file is not None:
            st.write(f"æ–‡ä»¶å: {uploaded_file.name}")
            st.write(f"æ–‡ä»¶å¤§å°: {uploaded_file.size / (1024*1024):.1f} MB")
            
            if st.button("ğŸš€ å¤„ç†ä¸Šä¼ çš„æ•°æ®é›†", type="primary", key="process_uploaded_dataset_btn"):
                with st.spinner("å¤„ç†ä¸­..."):
                    success, message = process_uploaded_dataset(uploaded_file)
                    if success:
                        st.success(message)
                        st.rerun()
                    else:
                        st.error(message)
    
    else:  # URLä¸‹è½½
        dataset_url = st.text_input(
            "è¾“å…¥æ•°æ®é›†ä¸‹è½½URL:",
            placeholder="https://example.com/dataset.zip",
            help="è¯·æä¾›ç›´æ¥ä¸‹è½½é“¾æ¥ï¼Œæ–‡ä»¶åº”ä¸ºåŒ…å«data.yamlçš„ZIPæ ¼å¼"
        )
        
        if dataset_url:
            if st.button("ğŸš€ ä¸‹è½½å¹¶å¤„ç†æ•°æ®é›†", type="primary", key="process_url_dataset_btn"):
                with st.spinner("ä¸‹è½½å¹¶å¤„ç†ä¸­..."):
                    success, message = process_url_dataset(dataset_url)
                    if success:
                        st.success(message)
                        st.rerun()
                    else:
                        st.error(message)

def extract_conversion_info(output_content):
    """æå–è½¬æ¢å…³é”®ä¿¡æ¯"""
    lines = output_content.split('\n')
    info = {
        "current_step": None,
        "progress_percentage": 0,
        "latest_status": None,
        "conversion_name": None,
        "model_path": None,
        "images_collected": None,
        "images_copied": None,
        "onnx_conversion_status": None,
        "cvimodel_conversion_status": None,
        "mud_file_created": None,
        "zip_package_created": None
    }
    
    # è½¬æ¢æ­¥éª¤æ˜ å°„
    steps_map = {
        "å¼€å§‹æ¨¡å‹è½¬æ¢æµç¨‹": (1, 10),
        "æŸ¥æ‰¾æ•°æ®é›†æ˜ å°„å…³ç³»": (2, 20),
        "æ•°æ®é›†å›¾ç‰‡æ”¶é›†ä¸å¤åˆ¶": (3, 30),
        "ONNXæ¨¡å‹è½¬æ¢": (4, 40),
        "ONNXè½¬æ¢æˆåŠŸ": (5, 60),
        "å¤åˆ¶è½¬æ¢è„šæœ¬": (6, 70),
        "æ‰§è¡ŒCviModelè½¬æ¢": (7, 80),
        "å¤„ç†CviModelæ–‡ä»¶": (8, 90),
        "å®Œæ•´çš„MaixCamæ¨¡å‹åŒ…å·²åˆ›å»º": (9, 100)
    }
    
    # ä»æ—¥å¿—ä¸­æå–ä¿¡æ¯
    for line in lines:
        line = line.strip()
        
        # æå–è½¬æ¢åç§°
        if "å¼€å§‹æ¨¡å‹è½¬æ¢æµç¨‹" in line and "export_" in line:
            try:
                parts = line.split("export_")
                if len(parts) > 1:
                    info["conversion_name"] = "export_" + parts[1].split()[0]
            except:
                pass
        
        # æå–æ¨¡å‹è·¯å¾„
        if "æ¨¡å‹æ–‡ä»¶:" in line:
            try:
                info["model_path"] = line.split("æ¨¡å‹æ–‡ä»¶:")[1].strip()
            except:
                pass
        
        # æ£€æŸ¥æ­¥éª¤è¿›åº¦
        for step_text, (step_num, progress) in steps_map.items():
            if step_text in line:
                info["current_step"] = step_text
                info["progress_percentage"] = progress
                break
        
        # æå–å›¾ç‰‡æ”¶é›†ä¿¡æ¯
        if "æ‰¾åˆ°" in line and "å¼ å›¾ç‰‡" in line:
            try:
                import re
                numbers = re.findall(r'\d+', line)
                if numbers:
                    info["images_collected"] = int(numbers[0])
            except:
                pass
        
        # æå–å›¾ç‰‡å¤åˆ¶ä¿¡æ¯
        if "æˆåŠŸå¤åˆ¶" in line and "å¼ å›¾ç‰‡" in line:
            try:
                import re
                numbers = re.findall(r'\d+', line)
                if numbers:
                    info["images_copied"] = int(numbers[0])
            except:
                pass
        
        # ONNXè½¬æ¢çŠ¶æ€
        if "ONNXè½¬æ¢æˆåŠŸ" in line:
            info["onnx_conversion_status"] = "æˆåŠŸ"
        elif "ONNXè½¬æ¢å¤±è´¥" in line:
            info["onnx_conversion_status"] = "å¤±è´¥"
        
        # CviModelè½¬æ¢çŠ¶æ€
        if "CviModelè½¬æ¢å®Œæˆ" in line:
            info["cvimodel_conversion_status"] = "æˆåŠŸ"
        elif "CviModelè½¬æ¢å¤±è´¥" in line:
            info["cvimodel_conversion_status"] = "å¤±è´¥"
        
        # MUDæ–‡ä»¶åˆ›å»ºçŠ¶æ€
        if "æˆåŠŸåˆ›å»ºMUDé…ç½®æ–‡ä»¶" in line:
            info["mud_file_created"] = "æˆåŠŸ"
        elif "åˆ›å»ºMUDæ–‡ä»¶å¤±è´¥" in line:
            info["mud_file_created"] = "å¤±è´¥"
        
        # ZIPåŒ…åˆ›å»ºçŠ¶æ€
        if "æˆåŠŸåˆ›å»ºæ¨¡å‹åŒ…" in line:
            info["zip_package_created"] = "æˆåŠŸ"
        elif "åˆ›å»ºæ¨¡å‹åŒ…å¤±è´¥" in line:
            info["zip_package_created"] = "å¤±è´¥"
    
    # æå–æœ€æ–°çŠ¶æ€
    for line in reversed(lines[-10:]):
        line = line.strip()
        if line and not line.startswith("="):
            info["latest_status"] = line
            break
    
    return info

def model_conversion_section():
    """æ¨¡å‹è½¬æ¢éƒ¨åˆ† - ä¼˜åŒ–ç‰ˆæœ¬"""
    st.subheader("ğŸ”„ è½¬æ¢ptä¸ºMaixCamæ¨¡å‹")
    
    # è·å–å½“å‰çŠ¶æ€
    status = get_status()
    current_status = status.get("status")
    
    # æ˜¾ç¤ºçŠ¶æ€
    status_icons = {
        "idle": "âšª å¾…æœºä¸­",
        "running": "ğŸŸ¢ è®­ç»ƒä¸­...",
        "converting": "ğŸ”„ è½¬æ¢ä¸­...",
        "completed": "âœ… å®Œæˆ",
        "failed": "âŒ å¤±è´¥",
        "stopped": "â¹ï¸ å·²åœæ­¢"
    }
    
    status_text = status_icons.get(current_status, current_status)
    st.write(f"**å½“å‰çŠ¶æ€:** {status_text}")
    
    # æŸ¥æ‰¾å¯ç”¨çš„æ¨¡å‹
    available_models = find_training_models()
    
    if not available_models:
        st.warning("âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚è¯·å…ˆå®Œæˆæ¨¡å‹è®­ç»ƒã€‚")
    else:
        st.success(f"âœ… å‘ç° {len(available_models)} ä¸ªå¯ç”¨æ¨¡å‹")
        
        # åˆ›å»ºæ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
        model_options = [f"{model['name']} ({model['size']:.1f} MB, {model['time'].strftime('%Y-%m-%d %H:%M')})" for model in available_models]
        selected_model_idx = st.selectbox(
            "é€‰æ‹©è¦è½¬æ¢çš„æ¨¡å‹:",
            range(len(model_options)),
            format_func=lambda i: model_options[i],
            help="é€‰æ‹©best.ptè·å¾—æ›´å¥½çš„ç²¾åº¦ï¼Œæˆ–é€‰æ‹©last.ptè·å¾—æœ€æ–°çš„è®­ç»ƒç»“æœ"
        )
        
        selected_model = available_models[selected_model_idx]
        st.info(f"å·²é€‰æ‹©: **{selected_model['name']}**")
        st.info(f"æ¨¡å‹è·¯å¾„: `{selected_model['path']}`")
        st.info(f"ç»å¯¹è·¯å¾„: `{os.path.abspath(selected_model['path'])}`")
        
        # æ˜¾ç¤ºæ˜ å°„å…³ç³»ä¿¡æ¯
        mapping_info = get_pt_dataset_mapping(selected_model["path"])
        if mapping_info:
            st.success("âœ… æ‰¾åˆ°æ•°æ®é›†æ˜ å°„å…³ç³»")
            with st.expander("ğŸ“‹ æŸ¥çœ‹æ˜ å°„ä¿¡æ¯"):
                st.json(mapping_info)
        else:
            st.warning("âš ï¸ æœªæ‰¾åˆ°æ•°æ®é›†æ˜ å°„å…³ç³»ï¼Œå°†è·³è¿‡å›¾ç‰‡å¤åˆ¶æ­¥éª¤")
            
            # è°ƒè¯•ä¿¡æ¯
            with st.expander("ğŸ” è°ƒè¯•æ˜ å°„å…³ç³»"):
                st.write("**æŸ¥æ‰¾çš„è·¯å¾„:**")
                st.code(selected_model["path"])
                st.write("**ç»å¯¹è·¯å¾„:**")
                st.code(os.path.abspath(selected_model["path"]))
                
                if os.path.exists(MAPPING_FILE):
                    with open(MAPPING_FILE, 'r', encoding='utf-8') as f:
                        all_mappings = json.load(f)
                        st.write("**æ˜ å°„æ–‡ä»¶ä¸­çš„æ‰€æœ‰è·¯å¾„:**")
                        for key in all_mappings.keys():
                            st.code(key)
                else:
                    st.error("æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨")
        
        # æ£€æŸ¥convert_cvimodel.shæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        convert_script_exists = os.path.exists("convert_cvimodel.sh")
        if convert_script_exists:
            st.success("âœ… æ‰¾åˆ°è½¬æ¢è„šæœ¬: convert_cvimodel.sh")
        else:
            st.warning("âš ï¸ æœªæ‰¾åˆ°è½¬æ¢è„šæœ¬: convert_cvimodel.sh")
            st.info("è¯·ç¡®ä¿convert_cvimodel.shæ–‡ä»¶å­˜åœ¨äºåº”ç”¨æ ¹ç›®å½•ï¼Œå¦åˆ™å°†è·³è¿‡CviModelè½¬æ¢æ­¥éª¤")
        
        # æ˜¾ç¤ºæ•°æ®é›†æ ‡ç­¾é¢„è§ˆ
        labels = get_dataset_labels()
        if labels:
            st.success(f"âœ… æ£€æµ‹åˆ°æ•°æ®é›†æ ‡ç­¾: {', '.join(labels[:5])}{'...' if len(labels) > 5 else ''}")
        else:
            st.warning("âš ï¸ æœªæ‰¾åˆ°æ•°æ®é›†æ ‡ç­¾ï¼ŒMUDæ–‡ä»¶å°†ä½¿ç”¨é»˜è®¤æ ‡ç­¾")
        
        # ONNXç›¸å…³å‚æ•°è®¾ç½®
        st.markdown("### âš™ï¸ ONNXè½¬æ¢å‚æ•°")
        
        # ONNX Opsetç‰ˆæœ¬ï¼ˆå›ºå®šï¼‰
        opset_version = 18  # å›ºå®šå€¼
        st.info(f"**ONNX Opsetç‰ˆæœ¬:** {opset_version} (å›ºå®šå‚æ•°ï¼Œä¸“ä¸ºMaixCamä¼˜åŒ–)")
        
        # æ˜¾ç¤ºé«˜çº§å‚æ•°
        with st.expander("é«˜çº§å‚æ•°è®¾ç½®"):
            st.markdown("ONNXè½¬æ¢çš„é«˜çº§å‚æ•°")
            
            # è¿™äº›å‚æ•°æš‚æ—¶ä¸ä¼šå®é™…ä½¿ç”¨ï¼Œä½†ä¿ç•™UIå…ƒç´ ä¾›æœªæ¥æ‰©å±•
            st.markdown("ä»¥ä¸‹å‚æ•°å½“å‰å›ºå®š:")
            st.code("""
batch=1                  # æ‰¹æ¬¡å¤§å°å›ºå®šä¸º1ï¼Œé€‚åˆè®¾å¤‡æ¨ç†
include=['onnx']         # ä»…å¯¼å‡ºONNXæ ¼å¼
half=True                # ä½¿ç”¨FP16åŠç²¾åº¦
int8=False               # ä¸ä½¿ç”¨INT8é‡åŒ–
device=0                 # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPUè®¾å¤‡
""", language="bash")
        
        # è½¬æ¢æŒ‰é’®
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if current_status not in ["converting", "running"]:
                if st.button("ğŸš€ å¼€å§‹è½¬æ¢", type="primary", key="start_conversion_btn"):
                    # æ‰§è¡Œè½¬æ¢
                    run_model_conversion(
                        model_path=selected_model["path"], 
                        format="onnx", 
                        opset=opset_version
                    )
                    st.success("æ¨¡å‹è½¬æ¢å·²å¼€å§‹!")
                    st.rerun()
            else:
                st.button("ğŸš€ å¼€å§‹è½¬æ¢", disabled=True, key="start_conversion_btn_disabled")
        
        with col2:
            if current_status == "converting":
                if st.button("â¹ï¸ åœæ­¢è½¬æ¢", type="secondary", key="stop_conversion_btn"):
                    stop_conversion()
                    st.rerun()
            else:
                st.button("â¹ï¸ åœæ­¢è½¬æ¢", disabled=True, key="stop_conversion_btn_disabled")
        
        with col3:
            if st.button("ğŸ”„ åˆ·æ–°çŠ¶æ€", key="refresh_conversion_status_btn"):
                st.rerun()
        
        # ===== ä¼˜åŒ–åçš„è½¬æ¢è¾“å‡ºæ—¥å¿—æ˜¾ç¤ºéƒ¨åˆ† =====
        st.markdown("### ğŸ“„ è½¬æ¢è¾“å‡ºæ—¥å¿—")
        conversion_output = read_conversion_output()
        
        if conversion_output:
            # æå–è½¬æ¢å…³é”®ä¿¡æ¯
            conversion_info = extract_conversion_info(conversion_output)
            
            # æ˜¾ç¤ºè½¬æ¢è¿›åº¦æ‘˜è¦
            if conversion_info["current_step"]:
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ğŸ”„ å½“å‰æ­¥éª¤", conversion_info["current_step"])
                with col2:
                    st.metric("ğŸ“ˆ è½¬æ¢è¿›åº¦", f"{conversion_info['progress_percentage']:.0f}%")
                with col3:
                    if conversion_info["conversion_name"]:
                        st.metric("ğŸ“¦ è½¬æ¢ä»»åŠ¡", conversion_info["conversion_name"])
                
                # æ˜¾ç¤ºè¿›åº¦æ¡
                progress_bar = st.progress(conversion_info['progress_percentage'] / 100)
            
            # æ˜¾ç¤ºè½¬æ¢çŠ¶æ€æ‘˜è¦
            if any([conversion_info["images_collected"], conversion_info["onnx_conversion_status"], 
                   conversion_info["cvimodel_conversion_status"], conversion_info["mud_file_created"]]):
                
                st.markdown("**ğŸ¯ è½¬æ¢çŠ¶æ€æ‘˜è¦:**")
                
                status_cols = st.columns(4)
                
                with status_cols[0]:
                    if conversion_info["images_collected"]:
                        st.info(f"ğŸ“¸ å›¾ç‰‡æ”¶é›†: {conversion_info['images_collected']}å¼ ")
                    if conversion_info["images_copied"]:
                        st.info(f"ğŸ“‹ å›¾ç‰‡å¤åˆ¶: {conversion_info['images_copied']}å¼ ")
                
                with status_cols[1]:
                    if conversion_info["onnx_conversion_status"]:
                        if conversion_info["onnx_conversion_status"] == "æˆåŠŸ":
                            st.success(f"ğŸ”„ ONNX: {conversion_info['onnx_conversion_status']}")
                        else:
                            st.error(f"ğŸ”„ ONNX: {conversion_info['onnx_conversion_status']}")
                
                with status_cols[2]:
                    if conversion_info["cvimodel_conversion_status"]:
                        if conversion_info["cvimodel_conversion_status"] == "æˆåŠŸ":
                            st.success(f"ğŸ¯ CviModel: {conversion_info['cvimodel_conversion_status']}")
                        else:
                            st.error(f"ğŸ¯ CviModel: {conversion_info['cvimodel_conversion_status']}")
                
                with status_cols[3]:
                    if conversion_info["mud_file_created"]:
                        if conversion_info["mud_file_created"] == "æˆåŠŸ":
                            st.success(f"ğŸ“‹ MUD: {conversion_info['mud_file_created']}")
                        else:
                            st.error(f"ğŸ“‹ MUD: {conversion_info['mud_file_created']}")
                    if conversion_info["zip_package_created"]:
                        if conversion_info["zip_package_created"] == "æˆåŠŸ":
                            st.success(f"ğŸ“¦ ZIP: {conversion_info['zip_package_created']}")
                        else:
                            st.error(f"ğŸ“¦ ZIP: {conversion_info['zip_package_created']}")
            
            # æ˜¾ç¤ºæœ€æ–°çš„å‡ è¡Œæ—¥å¿—ï¼ˆç½®é¡¶æ˜¾ç¤ºï¼‰
            st.markdown("**ğŸ”¥ æœ€æ–°æ—¥å¿—:**")
            lines = conversion_output.split('\n')
            
            # è¿‡æ»¤æ‰ç©ºè¡Œå’Œåˆ†éš”çº¿ï¼Œå–æœ€å10è¡Œæœ‰å†…å®¹çš„æ—¥å¿—
            non_empty_lines = [line for line in lines if line.strip() and not line.strip().startswith('=') and not line.strip().startswith('-')]
            recent_lines = non_empty_lines[-10:] if len(non_empty_lines) >= 10 else non_empty_lines
            
            # åè½¬æ˜¾ç¤ºé¡ºåºï¼Œæœ€æ–°çš„åœ¨ä¸Šé¢
            recent_lines_reversed = list(reversed(recent_lines))
            recent_content = '\n'.join(recent_lines_reversed)
            
            # ä½¿ç”¨ä»£ç å—æ˜¾ç¤ºæœ€æ–°æ—¥å¿—
            log_container = st.container()
            with log_container:
                st.code(recent_content, language=None)
            
            # æ˜¾ç¤ºé€‰é¡¹
            col1, col2 = st.columns(2)
            with col1:
                show_full_conversion_log = st.checkbox("æ˜¾ç¤ºå®Œæ•´è½¬æ¢æ—¥å¿—", value=False, key="show_full_conversion_logs")
            with col2:
                auto_refresh_conversion = st.checkbox("è‡ªåŠ¨åˆ·æ–°", value=True, key="auto_refresh_conversion_logs")
            
            # æ˜¾ç¤ºå®Œæ•´æ—¥å¿—ï¼ˆå¯é€‰ï¼‰
            if show_full_conversion_log:
                st.markdown("**ğŸ“‹ å®Œæ•´è½¬æ¢æ—¥å¿—:**")
                st.text_area(
                    "è½¬æ¢æ—¥å¿—:",
                    value=conversion_output,
                    height=400,
                    key="conversion_output_area"
                )
            
            # æ˜¾ç¤ºæ—¥å¿—ç»Ÿè®¡
            total_lines = len([line for line in lines if line.strip()])
            st.caption(f"ğŸ“Š æ€»è®¡ {total_lines} è¡Œæœ‰æ•ˆæ—¥å¿— | ğŸ•’ æœ€åæ›´æ–°: {datetime.now().strftime('%H:%M:%S')}")
            
            # å¦‚æœæ­£åœ¨è½¬æ¢ï¼Œè‡ªåŠ¨åˆ·æ–°
            if current_status == "converting" and auto_refresh_conversion:
                time.sleep(2)  # æ¯2ç§’åˆ·æ–°ä¸€æ¬¡
                st.rerun()
                
        else:
            st.info("æš‚æ— è½¬æ¢æ—¥å¿—")
        
        # ===== æ–°å¢ï¼šæ˜¾ç¤ºè½¬æ¢ç»“æœå’Œä¸‹è½½åŠŸèƒ½ =====
        st.markdown("### ğŸ“¦ è½¬æ¢ç»“æœå’Œä¸‹è½½")
        
        # æŸ¥æ‰¾å·²è½¬æ¢çš„æ¨¡å‹åŒ…
        converted_packages = find_model_packages()
        converted_cvimodels = find_converted_cvimodels()
        
        if converted_packages:
            st.success(f"âœ… å‘ç° {len(converted_packages)} ä¸ªå®Œæ•´æ¨¡å‹åŒ…")
            
            # æ˜¾ç¤ºæ¨¡å‹åŒ…åˆ—è¡¨
            for i, package in enumerate(converted_packages):
                with st.expander(f"ğŸ“¦ {package['name']} ({package['size']:.2f} MB) - {package['time'].strftime('%Y-%m-%d %H:%M:%S')}", expanded=(i==0)):
                    col1, col2, col3 = st.columns([2, 1, 1])
                    
                    with col1:
                        st.info(f"**æ–‡ä»¶è·¯å¾„:** `{package['path']}`")
                        st.info(f"**è½¬æ¢ä»»åŠ¡:** {package['export_dir']}")
                        st.info(f"**æ–‡ä»¶å¤§å°:** {package['size']:.2f} MB")
                        st.info(f"**åˆ›å»ºæ—¶é—´:** {package['time'].strftime('%Y-%m-%d %H:%M:%S')}")
                    
                    with col2:
                        # ä½¿ç”¨Streamlitçš„download_buttonæä¾›ä¸‹è½½åŠŸèƒ½
                        try:
                            with open(package['path'], "rb") as file:
                                file_bytes = file.read()
                            
                            st.download_button(
                                label="ğŸ“¥ ä¸‹è½½å®Œæ•´æ¨¡å‹åŒ…",
                                data=file_bytes,
                                file_name=package['name'],
                                mime="application/zip",
                                key=f"download_package_{i}",
                                type="primary"
                            )
                        except Exception as e:
                            st.error(f"å‡†å¤‡ä¸‹è½½å¤±è´¥: {str(e)}")
                    
                    with col3:
                        # æ˜¾ç¤ºåŒ…å«å†…å®¹é¢„è§ˆ
                        if st.button(f"ğŸ” æŸ¥çœ‹å†…å®¹", key=f"show_content_{i}"):
                            try:
                                import zipfile
                                with zipfile.ZipFile(package['path'], 'r') as zip_ref:
                                    file_list = zip_ref.namelist()
                                    st.markdown("**ZIPåŒ…å†…å®¹:**")
                                    for file in file_list:
                                        st.text(f"ğŸ“„ {file}")
                            except Exception as e:
                                st.error(f"è¯»å–ZIPå†…å®¹å¤±è´¥: {str(e)}")
        
        elif converted_cvimodels:
            st.warning("âš ï¸ å‘ç°CviModelæ–‡ä»¶ä½†æ— å®Œæ•´æ¨¡å‹åŒ…")
            
            # æ˜¾ç¤ºCviModelæ–‡ä»¶åˆ—è¡¨
            for i, cvimodel in enumerate(converted_cvimodels):
                with st.expander(f"ğŸ¯ {cvimodel['name']} ({cvimodel['size']:.2f} MB) - {cvimodel['time'].strftime('%Y-%m-%d %H:%M:%S')}", expanded=(i==0)):
                    col1, col2 = st.columns([2, 1])
                    
                    with col1:
                        st.info(f"**æ–‡ä»¶è·¯å¾„:** `{cvimodel['path']}`")
                        st.info(f"**è½¬æ¢ä»»åŠ¡:** {cvimodel['export_dir']}")
                        st.info(f"**æ–‡ä»¶å¤§å°:** {cvimodel['size']:.2f} MB")
                        st.info(f"**åˆ›å»ºæ—¶é—´:** {cvimodel['time'].strftime('%Y-%m-%d %H:%M:%S')}")
                    
                    with col2:
                        # ä¸‹è½½å•ç‹¬çš„CviModelæ–‡ä»¶
                        try:
                            with open(cvimodel['path'], "rb") as file:
                                file_bytes = file.read()
                            
                            st.download_button(
                                label="ğŸ“¥ ä¸‹è½½CviModel",
                                data=file_bytes,
                                file_name=cvimodel['name'],
                                mime="application/octet-stream",
                                key=f"download_cvimodel_{i}",
                                type="secondary"
                            )
                        except Exception as e:
                            st.error(f"å‡†å¤‡ä¸‹è½½å¤±è´¥: {str(e)}")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„MUDæ–‡ä»¶
                    mud_file_path = cvimodel['path'].replace('.cvimodel', '.mud')
                    if os.path.exists(mud_file_path):
                        try:
                            with open(mud_file_path, "rb") as file:
                                mud_bytes = file.read()
                            
                            st.download_button(
                                label="ğŸ“‹ ä¸‹è½½MUDé…ç½®",
                                data=mud_bytes,
                                file_name=os.path.basename(mud_file_path),
                                mime="text/plain",
                                key=f"download_mud_{i}",
                                type="secondary"
                            )
                        except Exception as e:
                            st.error(f"å‡†å¤‡MUDä¸‹è½½å¤±è´¥: {str(e)}")
        
        else:
            st.info("ğŸ’¡ æš‚æ— è½¬æ¢å®Œæˆçš„æ¨¡å‹åŒ…ã€‚å®Œæˆæ¨¡å‹è½¬æ¢åï¼Œä¸‹è½½æŒ‰é’®å°†åœ¨æ­¤å¤„æ˜¾ç¤ºã€‚")
            
            # æ˜¾ç¤ºtransferç›®å½•çš„æ‰€æœ‰å†…å®¹ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            transfer_dir = "transfer"
            if os.path.exists(transfer_dir):
                with st.expander("ğŸ” è°ƒè¯•ï¼šæŸ¥çœ‹transferç›®å½•å†…å®¹"):
                    for item in os.listdir(transfer_dir):
                        item_path = os.path.join(transfer_dir, item)
                        if os.path.isdir(item_path):
                            st.write(f"ğŸ“ {item}/")
                            # æ˜¾ç¤ºå­ç›®å½•å†…å®¹
                            for subitem in os.listdir(item_path):
                                subitem_path = os.path.join(item_path, subitem)
                                if os.path.isfile(subitem_path):
                                    size_mb = os.path.getsize(subitem_path) / (1024 * 1024)
                                    st.write(f"   ğŸ“„ {subitem} ({size_mb:.2f} MB)")
                                else:
                                    st.write(f"   ğŸ“ {subitem}/")
            else:
                st.info("transferç›®å½•ä¸å­˜åœ¨")

def main():
    """ä¸»åº”ç”¨ç¨‹åº"""
    st.set_page_config(
        page_title="MaixCamçš„YOLOv11è®­ç»ƒå¹³å°",
        page_icon="ğŸ§ª",
        layout="wide"
    )
    
    st.title("ğŸ§ª MaixCamçš„YOLOv11è®­ç»ƒå¹³å°")
    st.markdown("æ”¯æŒæ•°æ®é›†ä¸Šä¼ /ä¸‹è½½ã€å‚æ•°è®¾ç½®ã€æ¨¡å‹è½¬æ¢å’ŒMaixCam CviModelç”Ÿæˆçš„å¢å¼ºç‰ˆè®­ç»ƒå¹³å°")
    
    # æ˜¾ç¤ºå¹³å°ä¿¡æ¯
    platform_info = get_platform_info()
    st.sidebar.markdown(f"**ç³»ç»Ÿä¿¡æ¯:**")
    st.sidebar.info(f"æ“ä½œç³»ç»Ÿ: {platform_info['system']}")
    st.sidebar.info(f"æ¶æ„: {platform_info['machine']}")
    
    # åˆå§‹åŒ–
    init_status()
    current_status = get_status()
    
    # ä¸»è¦å†…å®¹åŒºåŸŸ
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ“¦ æ•°æ®é›†ç®¡ç†", "ğŸš€ è®­ç»ƒæ§åˆ¶", "ğŸ“º å®æ—¶è¾“å‡º", "ğŸ“Š è®­ç»ƒç»“æœ", "ğŸ“¤ è½¬æ¢ptä¸ºMaixCamæ¨¡å‹"])
    
    with tab1:
        dataset_management_section()
    
    with tab2:
        st.subheader("ğŸš€ è®­ç»ƒæ§åˆ¶")
        
        # çŠ¶æ€æ˜¾ç¤º
        status_icons = {
            "idle": "âšª å¾…æœºä¸­",
            "running": "ğŸŸ¢ è®­ç»ƒä¸­...",
            "converting": "ğŸ”„ è½¬æ¢ä¸­...",
            "completed": "âœ… è®­ç»ƒå®Œæˆ",
            "failed": "âŒ è®­ç»ƒå¤±è´¥",
            "stopped": "â¹ï¸ å·²åœæ­¢"
        }
        
        status_text = status_icons.get(current_status["status"], current_status["status"])
        st.write(f"**å½“å‰çŠ¶æ€:** {status_text}")
        
        # æ·»åŠ è®­ç»ƒå‚æ•°è®¾ç½®
        st.markdown("### âš™ï¸ è®­ç»ƒå‚æ•°è®¾ç½®")
        
        # æ¨¡å‹é€‰æ‹©
        # "yolo11s.pt", "yolo11m.pt", "yolo11l.pt", "yolo11x.pt"ï¼Œæš‚æ—¶åªæ˜¯æ”¯æŒyolo11n
        model_options = ["yolo11n.pt"]
        selected_model = st.selectbox(
            "é€‰æ‹©æ¨¡å‹:",
            model_options,
            index=0,
            help="é€‰æ‹©YOLOv11æ¨¡å‹ç‰ˆæœ¬ï¼Œn(nano)æœ€å°ï¼Œx(xlarge)æœ€å¤§"
        )
        
        # Epochè®¾ç½®
        epochs = st.slider(
            "è®­ç»ƒè½®æ•° (Epochs):",
            min_value=5,
            max_value=300,
            value=20,
            step=5,
            help="è®­ç»ƒå¾ªç¯çš„æ€»è½®æ•°ï¼Œæ›´å¤šçš„è½®æ•°å¯èƒ½è·å¾—æ›´å¥½çš„ç»“æœï¼Œä½†è®­ç»ƒæ—¶é—´æ›´é•¿"
        )
        
        # å›¾ç‰‡å°ºå¯¸è®¾ç½®
        img_size_options = [320, 416, 512, 640, 768, 896, 1024, 1280]
        selected_img_size = st.select_slider(
            "å›¾ç‰‡å°ºå¯¸ (Image Size):",
            options=img_size_options,
            value=640,
            help="è®­ç»ƒå›¾ç‰‡å°ºå¯¸ï¼Œæ›´å¤§çš„å°ºå¯¸å¯èƒ½æé«˜å‡†ç¡®ç‡ï¼Œä½†ä¼šå¢åŠ æ˜¾å­˜éœ€æ±‚å’Œè®­ç»ƒæ—¶é—´"
        )
        
        # é«˜çº§å‚æ•°
        with st.expander("é«˜çº§å‚æ•°è®¾ç½®"):
            st.markdown("ä»¥ä¸‹æ˜¯å½“å‰å›ºå®šçš„é«˜çº§å‚æ•°ï¼Œå°†åœ¨æœªæ¥ç‰ˆæœ¬ä¸­å¼€æ”¾è®¾ç½®")
            st.code("""
batch=16                 # æ‰¹æ¬¡å¤§å°
patience=50              # æ—©åœè€å¿ƒå€¼
optimizer='auto'         # ä¼˜åŒ–å™¨
lr0=0.01                 # åˆå§‹å­¦ä¹ ç‡
cos_lr=True              # æ˜¯å¦ä½¿ç”¨ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦
weight_decay=0.0005      # æƒé‡è¡°å‡
dropout=0.0              # ä¸¢å¼ƒç‡
label_smoothing=0.0      # æ ‡ç­¾å¹³æ»‘
""", language="bash")
        
        # æ§åˆ¶æŒ‰é’®
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if current_status["status"] in ["idle", "completed", "failed", "stopped"]:
                if st.button("ğŸš€ å¼€å§‹è®­ç»ƒ", type="primary", key="start_training_btn"):
                    # æ£€æŸ¥åŸºæœ¬æ–‡ä»¶
                    if not os.path.exists("data/data.yaml"):
                        st.error("âŒ æœªæ‰¾åˆ° data/data.yaml æ–‡ä»¶!")
                        st.info("è¯·å…ˆåœ¨'æ•°æ®é›†ç®¡ç†'æ ‡ç­¾é¡µé…ç½®æ•°æ®é›†")
                    else:
                        run_docker_training(selected_model, epochs, selected_img_size)
                        st.success("è®­ç»ƒå·²å¼€å§‹!")
                        st.rerun()
            else:
                st.button("ğŸš€ å¼€å§‹è®­ç»ƒ", disabled=True, key="start_training_btn_disabled")
        
        with col2:
            if current_status["status"] == "running":
                if st.button("â¹ï¸ åœæ­¢è®­ç»ƒ", type="secondary", key="stop_training_btn"):
                    stop_training()
                    st.rerun()
            else:
                st.button("â¹ï¸ åœæ­¢è®­ç»ƒ", disabled=True, key="stop_training_btn_disabled")
        
        with col3:
            if st.button("ğŸ”„ åˆ·æ–°çŠ¶æ€", key="refresh_training_status_btn"):
                st.rerun()
        
        with col4:
            if st.button("ğŸ§¹ æ¸…ç©ºæ—¥å¿—", key="clear_logs_btn"):
                clear_output()
                st.success("æ—¥å¿—å·²æ¸…ç©º")
                st.rerun()
        
        # æ˜¾ç¤ºDockerå‘½ä»¤
        with st.expander("ğŸ” æŸ¥çœ‹æ‰§è¡Œçš„Dockerå‘½ä»¤"):
            # ä½¿ç”¨å½“å‰é€‰æ‹©çš„å‚æ•°ç”Ÿæˆå‘½ä»¤é¢„è§ˆ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            run_name = f"train_{timestamp}"
            
            docker_cmd, _, _, _ = build_docker_training_command(
                selected_model, epochs, selected_img_size, run_name
            )
            st.code(docker_cmd, language='bash')
    
    with tab3:
        st.subheader("ğŸ“º è®­ç»ƒè¾“å‡º")
        
        output_content = read_output()
        if output_content:
            # æå–è®­ç»ƒå…³é”®ä¿¡æ¯
            training_info = extract_training_info(output_content)
            
            # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦æ‘˜è¦
            if training_info["current_epoch"]:
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ğŸ“Š å½“å‰Epoch", f"{training_info['current_epoch']}/{training_info['total_epochs']}")
                with col2:
                    st.metric("ğŸ“ˆ è®­ç»ƒè¿›åº¦", f"{training_info['progress_percentage']:.1f}%")
                with col3:
                    if training_info["latest_metrics"]:
                        # ç®€åŒ–æ˜¾ç¤ºæœ€æ–°æŒ‡æ ‡
                        if "mAP50-95" in training_info["latest_metrics"]:
                            try:
                                map_value = training_info["latest_metrics"].split()[-1]
                                st.metric("ğŸ¯ mAP50-95", map_value)
                            except:
                                st.metric("ğŸ¯ æœ€æ–°æŒ‡æ ‡", "è®¡ç®—ä¸­...")
                
                # æ˜¾ç¤ºè¿›åº¦æ¡
                progress_bar = st.progress(training_info['progress_percentage'] / 100)
            
            # æ˜¾ç¤ºæœ€æ–°çš„å‡ è¡Œæ—¥å¿—ï¼ˆç½®é¡¶æ˜¾ç¤ºï¼‰
            st.markdown("**ğŸ”¥ æœ€æ–°æ—¥å¿—:**")
            lines = output_content.split('\n')
            
            # è¿‡æ»¤æ‰ç©ºè¡Œï¼Œå–æœ€å10è¡Œæœ‰å†…å®¹çš„æ—¥å¿—
            non_empty_lines = [line for line in lines if line.strip()]
            recent_lines = non_empty_lines[-10:] if len(non_empty_lines) >= 10 else non_empty_lines
            
            # åè½¬æ˜¾ç¤ºé¡ºåºï¼Œæœ€æ–°çš„åœ¨ä¸Šé¢
            recent_lines_reversed = list(reversed(recent_lines))
            recent_content = '\n'.join(recent_lines_reversed)
            
            # ä½¿ç”¨ä¸åŒçš„æ˜¾ç¤ºæ–¹å¼
            log_container = st.container()
            with log_container:
                st.code(recent_content, language=None)
            
            # æ˜¾ç¤ºé€‰é¡¹
            col1, col2 = st.columns(2)
            with col1:
                show_full_log = st.checkbox("æ˜¾ç¤ºå®Œæ•´æ—¥å¿—", value=False, key="show_full_logs_checkbox")
            with col2:
                auto_scroll = st.checkbox("è‡ªåŠ¨åˆ·æ–°", value=True, key="auto_refresh_logs_checkbox")
            
            # æ˜¾ç¤ºå®Œæ•´æ—¥å¿—ï¼ˆå¯é€‰ï¼‰
            if show_full_log:
                st.markdown("**ğŸ“‹ å®Œæ•´è®­ç»ƒæ—¥å¿—:**")
                st.text_area(
                    "æ‰€æœ‰æ—¥å¿—å†…å®¹:",
                    value=output_content,
                    height=300,
                    key="full_output_area"
                )
            
            # æ˜¾ç¤ºæ—¥å¿—ç»Ÿè®¡
            total_lines = len([line for line in lines if line.strip()])
            st.caption(f"ğŸ“Š æ€»è®¡ {total_lines} è¡Œæœ‰æ•ˆæ—¥å¿— | ğŸ•’ æœ€åæ›´æ–°: {datetime.now().strftime('%H:%M:%S')}")
            
        else:
            st.info("æš‚æ— è¾“å‡ºå†…å®¹ï¼ˆå¯ä»¥æµè§ˆå™¨åˆ·æ–°ä¸€ä¸‹ï¼‰")
        
        # å¦‚æœæ­£åœ¨è¿è¡Œï¼Œè‡ªåŠ¨åˆ·æ–°ï¼ˆé»˜è®¤å¼€å¯ï¼‰
        if current_status["status"] == "running" and 'auto_scroll' in locals() and auto_scroll:
            time.sleep(2)  # æ¯2ç§’åˆ·æ–°ä¸€æ¬¡
            st.rerun()
    
    with tab4:
        display_results()
    
    with tab5:
        model_conversion_section()


if __name__ == "__main__":
    # Windowsç³»ç»Ÿç¼–ç è®¾ç½®
    if platform.system() == "Windows":
        # è®¾ç½®æ§åˆ¶å°ç¼–ç ä¸ºUTF-8
        os.system('chcp 65001 >nul 2>&1')
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['PYTHONIOENCODING'] = 'utf-8'
        os.environ['CHCP'] = '65001'
    
    # åœ¨åº”ç”¨å¯åŠ¨æ—¶è¿›è¡Œç¯å¢ƒåˆå§‹åŒ–
    print("æ­£åœ¨å¯åŠ¨MaixCam YOLOv11è®­ç»ƒå¹³å°...")
    
    platform_info = get_platform_info()
    print(f"æ£€æµ‹åˆ°æ“ä½œç³»ç»Ÿ: {platform_info['system']} ({platform_info['machine']})")
    
    # åœ¨åº”ç”¨å¯åŠ¨æ—¶è¿›è¡Œç¯å¢ƒåˆå§‹åŒ–ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼Œé¿å…é˜»å¡Streamlitå¯åŠ¨ï¼‰
    def background_env_check():
        """åå°ç¯å¢ƒæ£€æŸ¥"""
        try:
            initialize_environment()
        except Exception as e:
            print(f"ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥: {e}")
            print("ç¨‹åºä»å°†å¯åŠ¨ï¼Œä½†æŸäº›åŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
    
    # åˆ›å»ºåå°çº¿ç¨‹è¿›è¡Œç¯å¢ƒæ£€æŸ¥
    env_check_thread = threading.Thread(target=background_env_check)
    env_check_thread.daemon = True
    env_check_thread.start()
    
    print("ğŸš€ å¯åŠ¨Streamlitåº”ç”¨...")
    
    # å¯åŠ¨Streamlitåº”ç”¨
    main()