import streamlit as st
import subprocess
import threading
import os
import time
import json
import zipfile
import requests
import shutil
import yaml
from datetime import datetime
from pathlib import Path
from urllib.parse import urlparse
import glob
import random
import base64

# çŠ¶æ€æ–‡ä»¶
STATUS_FILE = "test_status.json"
OUTPUT_FILE = "test_output.txt"
DATASET_INFO_FILE = "dataset_info.json"
CONVERSION_OUTPUT_FILE = "conversion_output.txt"
MAPPING_FILE = "pt_dataset_mapping.json"  # æ–°å¢: æ˜ å°„å…³ç³»æ–‡ä»¶

def init_status():
    """åˆå§‹åŒ–çŠ¶æ€"""
    default_status = {
        "status": "idle",
        "pid": None,
        "timestamp": datetime.now().isoformat(),
        "current_run": None  # æ·»åŠ å½“å‰è¿è¡Œçš„ä»»åŠ¡æ ‡è¯†
    }
    
    if not os.path.exists(STATUS_FILE):
        with open(STATUS_FILE, 'w', encoding='utf-8') as f:
            json.dump(default_status, f)

def get_status():
    """è·å–çŠ¶æ€"""
    try:
        with open(STATUS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        init_status()
        return get_status()

def set_status(status, pid=None, current_run=None):
    """è®¾ç½®çŠ¶æ€"""
    status_data = get_status()
    status_data["status"] = status
    status_data["timestamp"] = datetime.now().isoformat()
    
    if pid is not None:
        status_data["pid"] = pid
        
    if current_run is not None:
        status_data["current_run"] = current_run
        
    with open(STATUS_FILE, 'w', encoding='utf-8') as f:
        json.dump(status_data, f)

def save_dataset_info(info):
    """ä¿å­˜æ•°æ®é›†ä¿¡æ¯"""
    with open(DATASET_INFO_FILE, 'w', encoding='utf-8') as f:
        json.dump(info, f, ensure_ascii=False, indent=2)

def get_dataset_info():
    """è·å–æ•°æ®é›†ä¿¡æ¯"""
    try:
        if os.path.exists(DATASET_INFO_FILE):
            with open(DATASET_INFO_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        return None
    except:
        return None

def save_pt_dataset_mapping(pt_file_path, dataset_path, run_name):
    """ä¿å­˜ptæ–‡ä»¶å’Œæ•°æ®é›†çš„æ˜ å°„å…³ç³»"""
    try:
        # è¯»å–ç°æœ‰æ˜ å°„
        mapping = {}
        if os.path.exists(MAPPING_FILE):
            with open(MAPPING_FILE, 'r', encoding='utf-8') as f:
                mapping = json.load(f)
        
        # æ·»åŠ æ–°æ˜ å°„ - é€‚é…æ–°çš„æ•°æ®é›†ç»“æ„
        mapping[pt_file_path] = {
            "dataset_path": dataset_path,
            "run_name": run_name,
            "created_time": datetime.now().isoformat(),
            "images_path": os.path.join(dataset_path, "images")  # ç›´æ¥æŒ‡å‘imagesç›®å½•
        }
        
        # ä¿å­˜æ˜ å°„
        with open(MAPPING_FILE, 'w', encoding='utf-8') as f:
            json.dump(mapping, f, ensure_ascii=False, indent=2)
            
        return True
    except Exception as e:
        print(f"ä¿å­˜æ˜ å°„å…³ç³»å¤±è´¥: {e}")
        return False

def get_pt_dataset_mapping(pt_file_path):
    """è·å–ptæ–‡ä»¶å¯¹åº”çš„æ•°æ®é›†è·¯å¾„"""
    try:
        if os.path.exists(MAPPING_FILE):
            with open(MAPPING_FILE, 'r', encoding='utf-8') as f:
                mapping = json.load(f)
                
                # é¦–å…ˆå°è¯•ç›´æ¥åŒ¹é…
                if pt_file_path in mapping:
                    return mapping[pt_file_path]
                
                # å¦‚æœç›´æ¥åŒ¹é…å¤±è´¥ï¼Œå°è¯•ç»å¯¹è·¯å¾„åŒ¹é…
                abs_path = os.path.abspath(pt_file_path)
                if abs_path in mapping:
                    return mapping[abs_path]
                
                # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•è§„èŒƒåŒ–è·¯å¾„åŒ¹é…
                normalized_path = os.path.normpath(abs_path)
                if normalized_path in mapping:
                    return mapping[normalized_path]
                
                # æœ€åå°è¯•é€šè¿‡æ–‡ä»¶ååŒ¹é…ï¼ˆå¦‚æœè·¯å¾„åˆ†éš”ç¬¦ä¸åŒï¼‰
                for key in mapping.keys():
                    if os.path.normpath(key) == normalized_path:
                        return mapping[key]
                
        return None
    except Exception as e:
        print(f"è·å–æ˜ å°„å…³ç³»å¤±è´¥: {e}")
        return None

def get_dataset_labels():
    """ä»data.yamlä¸­è·å–æ ‡ç­¾åˆ—è¡¨"""
    try:
        data_yaml_path = "data/data.yaml"
        if os.path.exists(data_yaml_path):
            with open(data_yaml_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                if 'names' in data:
                    return data['names']
        return []
    except Exception as e:
        print(f"è·å–æ•°æ®é›†æ ‡ç­¾å¤±è´¥: {e}")
        return []

def create_mud_file(cvimodel_path, conversion_name):
    """åˆ›å»ºMUDé…ç½®æ–‡ä»¶"""
    try:
        # è·å–cvimodelæ–‡ä»¶çš„ç›®å½•å’Œæ–‡ä»¶åï¼ˆä¸å¸¦æ‰©å±•åï¼‰
        cvimodel_dir = os.path.dirname(cvimodel_path)
        cvimodel_filename = os.path.basename(cvimodel_path)
        cvimodel_basename = os.path.splitext(cvimodel_filename)[0]
        
        # åˆ›å»ºmudæ–‡ä»¶è·¯å¾„
        mud_filename = f"{cvimodel_basename}.mud"
        mud_path = os.path.join(cvimodel_dir, mud_filename)
        
        # è·å–æ•°æ®é›†æ ‡ç­¾
        labels = get_dataset_labels()
        labels_str = ", ".join(labels) if labels else "object"
        
        # MUDæ–‡ä»¶å†…å®¹
        mud_content = f"""[basic]
type = cvimodel
model = {cvimodel_filename}

[extra]
model_type = yolo11
input_type = rgb
mean = 0, 0, 0
scale = 0.00392156862745098, 0.00392156862745098, 0.00392156862745098
anchors = 10,13, 16,30, 33,23, 30,61, 62,45, 59,119, 116,90, 156,198, 373,326
labels = {labels_str}
"""
        
        # å†™å…¥MUDæ–‡ä»¶
        with open(mud_path, 'w', encoding='utf-8') as f:
            f.write(mud_content)
        
        return mud_path, f"âœ… æˆåŠŸåˆ›å»ºMUDé…ç½®æ–‡ä»¶: {mud_filename}"
        
    except Exception as e:
        return None, f"âŒ åˆ›å»ºMUDæ–‡ä»¶å¤±è´¥: {str(e)}"

def create_model_package_zip(cvimodel_path, mud_path, conversion_name):
    """åˆ›å»ºæ¨¡å‹åŒ…ZIPæ–‡ä»¶"""
    try:
        # è·å–æ–‡ä»¶æ‰€åœ¨ç›®å½•
        model_dir = os.path.dirname(cvimodel_path)
        
        # è·å–æ–‡ä»¶åŸºç¡€åç§°ï¼ˆä¸å¸¦æ‰©å±•åï¼‰
        cvimodel_basename = os.path.splitext(os.path.basename(cvimodel_path))[0]
        zip_filename = f"{cvimodel_basename}.zip"
        zip_path = os.path.join(model_dir, zip_filename)
        
        # åˆ›å»ºZIPæ–‡ä»¶
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # æ·»åŠ cvimodelæ–‡ä»¶
            zipf.write(cvimodel_path, os.path.basename(cvimodel_path))
            # æ·»åŠ mudæ–‡ä»¶
            zipf.write(mud_path, os.path.basename(mud_path))
        
        # è·å–æ–‡ä»¶å¤§å°
        zip_size = os.path.getsize(zip_path) / (1024 * 1024)  # MB
        
        return zip_path, f"âœ… æˆåŠŸåˆ›å»ºæ¨¡å‹åŒ…: {zip_filename} ({zip_size:.2f} MB)"
        
    except Exception as e:
        return None, f"âŒ åˆ›å»ºæ¨¡å‹åŒ…å¤±è´¥: {str(e)}"

def collect_images_from_dataset(images_path, target_count=200):
    """ä»æ•°æ®é›†çš„imagesç›®å½•ä¸­æ”¶é›†å›¾ç‰‡"""
    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff', '*.webp']
    all_images = []
    
    # æ£€æŸ¥imagesç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(images_path):
        print(f"Imagesç›®å½•ä¸å­˜åœ¨: {images_path}")
        return []
    
    # æ”¶é›†imagesæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å›¾ç‰‡
    for ext in image_extensions:
        all_images.extend(glob.glob(os.path.join(images_path, ext)))
        all_images.extend(glob.glob(os.path.join(images_path, ext.upper())))
    
    # å»é‡å¹¶éšæœºæ‰“ä¹±
    all_images = list(set(all_images))
    random.shuffle(all_images)
    
    print(f"åœ¨ {images_path} ä¸­æ‰¾åˆ° {len(all_images)} å¼ å›¾ç‰‡")
    
    return all_images

def copy_images_to_transfer(images_list, target_dir, target_count=200):
    """å¤åˆ¶å›¾ç‰‡åˆ°transferç›®å½•"""
    try:
        # åˆ›å»ºimagesç›®å½•
        images_dir = os.path.join(target_dir, "images")
        os.makedirs(images_dir, exist_ok=True)
        
        copied_images = []
        
        # å¦‚æœå›¾ç‰‡æ•°é‡è¶³å¤Ÿ
        if len(images_list) >= target_count:
            selected_images = images_list[:target_count]
            for i, img_path in enumerate(selected_images):
                if os.path.exists(img_path):
                    file_ext = os.path.splitext(img_path)[1]
                    target_name = f"image_{i+1:03d}{file_ext}"
                    target_path = os.path.join(images_dir, target_name)
                    shutil.copy2(img_path, target_path)
                    copied_images.append(target_path)
        
        # å¦‚æœå›¾ç‰‡æ•°é‡ä¸å¤Ÿï¼Œé‡å¤å¤åˆ¶å¹¶é‡å‘½å
        else:
            available_count = len(images_list)
            if available_count == 0:
                return [], None
            
            for i in range(target_count):
                source_img = images_list[i % available_count]  # å¾ªç¯ä½¿ç”¨ç°æœ‰å›¾ç‰‡
                if os.path.exists(source_img):
                    file_ext = os.path.splitext(source_img)[1]
                    target_name = f"image_{i+1:03d}{file_ext}"
                    target_path = os.path.join(images_dir, target_name)
                    shutil.copy2(source_img, target_path)
                    copied_images.append(target_path)
        
        # å¤åˆ¶ä¸€å¼ å›¾ç‰‡ä½œä¸ºtestå›¾ç‰‡
        test_image = None
        if copied_images:
            test_source = copied_images[0]
            file_ext = os.path.splitext(test_source)[1]
            test_image = os.path.join(target_dir, f"test{file_ext}")
            shutil.copy2(test_source, test_image)
        
        return copied_images, test_image
        
    except Exception as e:
        print(f"å¤åˆ¶å›¾ç‰‡å¤±è´¥: {e}")
        return [], None

def download_file(url, local_filename, progress_placeholder=None):
    """ä¸‹è½½æ–‡ä»¶"""
    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        downloaded_size = 0
        
        with open(local_filename, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded_size += len(chunk)
                    
                    if progress_placeholder and total_size > 0:
                        progress = downloaded_size / total_size
                        progress_placeholder.progress(progress)
        
        return True
    except Exception as e:
        st.error(f"ä¸‹è½½å¤±è´¥: {str(e)}")
        return False

def extract_zip(zip_path, extract_to):
    """è§£å‹ZIPæ–‡ä»¶"""
    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_to)
        return True
    except Exception as e:
        st.error(f"è§£å‹å¤±è´¥: {str(e)}")
        return False

def find_data_yaml(directory):
    """é€’å½’æŸ¥æ‰¾data.yamlæ–‡ä»¶"""
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.lower() in ['data.yaml', 'data.yml']:
                return os.path.join(root, file)
    return None

def validate_dataset(data_yaml_path):
    """éªŒè¯æ•°æ®é›†æ ¼å¼"""
    try:
        with open(data_yaml_path, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        
        # æ£€æŸ¥å¿…è¦çš„å­—æ®µ
        required_fields = ['train', 'val', 'names']
        missing_fields = [field for field in required_fields if field not in data]
        
        if missing_fields:
            return False, f"ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}"
        
        # æ£€æŸ¥ç±»åˆ«æ•°é‡
        if 'nc' not in data:
            data['nc'] = len(data['names'])
        
        return True, data
    except Exception as e:
        return False, f"è§£æYAMLæ–‡ä»¶å¤±è´¥: {str(e)}"

def process_uploaded_dataset(uploaded_file):
    """å¤„ç†ä¸Šä¼ çš„æ•°æ®é›†"""
    try:
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = "temp_dataset"
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        os.makedirs(temp_dir)
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        zip_path = os.path.join(temp_dir, "dataset.zip")
        with open(zip_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        # è§£å‹æ–‡ä»¶
        extract_dir = os.path.join(temp_dir, "extracted")
        if not extract_zip(zip_path, extract_dir):
            return False, "è§£å‹å¤±è´¥"
        
        # æŸ¥æ‰¾data.yamlæ–‡ä»¶
        data_yaml_path = find_data_yaml(extract_dir)
        if not data_yaml_path:
            return False, "æœªæ‰¾åˆ°data.yamlæ–‡ä»¶"
        
        # éªŒè¯æ•°æ®é›†
        is_valid, result = validate_dataset(data_yaml_path)
        if not is_valid:
            return False, result
        
        # ç§»åŠ¨åˆ°dataç›®å½•
        data_dir = "data"
        if os.path.exists(data_dir):
            # å¤‡ä»½åŸæœ‰æ•°æ®
            backup_dir = f"data_backup_{int(time.time())}"
            shutil.move(data_dir, backup_dir)
            st.info(f"åŸæ•°æ®é›†å·²å¤‡ä»½åˆ°: {backup_dir}")
        
        # ç§»åŠ¨æ–°æ•°æ®é›†
        dataset_root = os.path.dirname(data_yaml_path)
        shutil.move(dataset_root, data_dir)
        
        # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            "source": "upload",
            "filename": uploaded_file.name,
            "upload_time": datetime.now().isoformat(),
            "classes": result['names'],
            "num_classes": len(result['names'])
        }
        save_dataset_info(dataset_info)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        shutil.rmtree(temp_dir)
        
        return True, "æ•°æ®é›†ä¸Šä¼ æˆåŠŸ"
        
    except Exception as e:
        return False, f"å¤„ç†ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {str(e)}"

def process_url_dataset(url):
    """å¤„ç†URLä¸‹è½½çš„æ•°æ®é›†"""
    try:
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = "temp_dataset"
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        os.makedirs(temp_dir)
        
        # ä¸‹è½½æ–‡ä»¶
        parsed_url = urlparse(url)
        filename = os.path.basename(parsed_url.path) or "dataset.zip"
        zip_path = os.path.join(temp_dir, filename)
        
        # æ˜¾ç¤ºä¸‹è½½è¿›åº¦
        progress_placeholder = st.empty()
        progress_placeholder.text("å¼€å§‹ä¸‹è½½...")
        progress_bar = st.progress(0)
        
        # ä¸‹è½½
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        downloaded_size = 0
        
        with open(zip_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded_size += len(chunk)
                    
                    if total_size > 0:
                        progress = downloaded_size / total_size
                        progress_bar.progress(progress)
                        progress_placeholder.text(f"ä¸‹è½½ä¸­... {downloaded_size/(1024*1024):.1f}MB / {total_size/(1024*1024):.1f}MB")
        
        progress_placeholder.text("ä¸‹è½½å®Œæˆï¼Œå¼€å§‹è§£å‹...")
        
        # è§£å‹æ–‡ä»¶
        extract_dir = os.path.join(temp_dir, "extracted")
        if not extract_zip(zip_path, extract_dir):
            return False, "è§£å‹å¤±è´¥"
        
        # æŸ¥æ‰¾data.yamlæ–‡ä»¶
        data_yaml_path = find_data_yaml(extract_dir)
        if not data_yaml_path:
            return False, "æœªæ‰¾åˆ°data.yamlæ–‡ä»¶"
        
        # éªŒè¯æ•°æ®é›†
        is_valid, result = validate_dataset(data_yaml_path)
        if not is_valid:
            return False, result
        
        # ç§»åŠ¨åˆ°dataç›®å½•
        data_dir = "data"
        if os.path.exists(data_dir):
            # å¤‡ä»½åŸæœ‰æ•°æ®
            backup_dir = f"data_backup_{int(time.time())}"
            shutil.move(data_dir, backup_dir)
            st.info(f"åŸæ•°æ®é›†å·²å¤‡ä»½åˆ°: {backup_dir}")
        
        # ç§»åŠ¨æ–°æ•°æ®é›†
        dataset_root = os.path.dirname(data_yaml_path)
        shutil.move(dataset_root, data_dir)
        
        # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            "source": "url",
            "url": url,
            "filename": filename,
            "download_time": datetime.now().isoformat(),
            "classes": result['names'],
            "num_classes": len(result['names'])
        }
        save_dataset_info(dataset_info)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        shutil.rmtree(temp_dir)
        progress_placeholder.empty()
        progress_bar.empty()
        
        return True, "æ•°æ®é›†ä¸‹è½½å¹¶é…ç½®æˆåŠŸ"
        
    except Exception as e:
        return False, f"å¤„ç†URLæ•°æ®é›†å¤±è´¥: {str(e)}"

def read_output():
    """è¯»å–è¾“å‡º"""
    try:
        if os.path.exists(OUTPUT_FILE):
            with open(OUTPUT_FILE, 'r', encoding='utf-8', errors='replace') as f:
                return f.read()
        return ""
    except Exception as e:
        return f"è¯»å–è¾“å‡ºå¤±è´¥: {str(e)}"

def read_conversion_output():
    """è¯»å–è½¬æ¢è¾“å‡º"""
    try:
        if os.path.exists(CONVERSION_OUTPUT_FILE):
            with open(CONVERSION_OUTPUT_FILE, 'r', encoding='utf-8', errors='replace') as f:
                return f.read()
        return ""
    except Exception as e:
        return f"è¯»å–è½¬æ¢è¾“å‡ºå¤±è´¥: {str(e)}"

def clear_output():
    """æ¸…ç©ºè¾“å‡º"""
    try:
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            f.write("")
    except Exception as e:
        print(f"æ¸…ç©ºè¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")

def clear_conversion_output():
    """æ¸…ç©ºè½¬æ¢è¾“å‡º"""
    try:
        with open(CONVERSION_OUTPUT_FILE, 'w', encoding='utf-8') as f:
            f.write("")
    except Exception as e:
        print(f"æ¸…ç©ºè½¬æ¢è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")

def find_and_move_cvimodel(transfer_dir, conversion_name, selected_model_name):
    """åœ¨workspaceç›®å½•ä¸­æŸ¥æ‰¾.cvimodelæ–‡ä»¶å¹¶ç§»åŠ¨åˆ°é¡¶å±‚ç›®å½•"""
    try:
        # ä»æ¨¡å‹æ–‡ä»¶åä¸­æå–åŸºæœ¬åç§°ï¼ˆä¾‹å¦‚ï¼šbest.pt -> bestï¼‰
        model_base_name = os.path.splitext(os.path.basename(selected_model_name))[0]
        
        # æŸ¥æ‰¾workspaceç›®å½•
        workspace_dir = os.path.join(transfer_dir, "workspace")
        if not os.path.exists(workspace_dir):
            return None, None, None, "æœªæ‰¾åˆ°workspaceç›®å½•"
        
        # æŸ¥æ‰¾.cvimodelæ–‡ä»¶
        cvimodel_files = []
        for file in os.listdir(workspace_dir):
            if file.endswith('.cvimodel'):
                cvimodel_files.append(file)
        
        if not cvimodel_files:
            return None, None, None, "æœªæ‰¾åˆ°.cvimodelæ–‡ä»¶"
        
        # å¯»æ‰¾åŒ¹é…çš„æ–‡ä»¶ï¼ˆä¼˜å…ˆæŸ¥æ‰¾åŒ…å«æ¨¡å‹åŸºæœ¬åç§°çš„æ–‡ä»¶ï¼‰
        target_cvimodel = None
        for file in cvimodel_files:
            if model_base_name in file:
                target_cvimodel = file
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª
        if not target_cvimodel:
            target_cvimodel = cvimodel_files[0]
        
        # æ„é€ æ–°çš„æ–‡ä»¶åï¼šexport_æ—¶é—´æˆ³_int8.cvimodel
        new_filename = f"{conversion_name}_int8.cvimodel"
        
        # æºæ–‡ä»¶è·¯å¾„å’Œç›®æ ‡æ–‡ä»¶è·¯å¾„
        source_path = os.path.join(workspace_dir, target_cvimodel)
        target_path = os.path.join(transfer_dir, new_filename)
        
        # ç§»åŠ¨å¹¶é‡å‘½åæ–‡ä»¶
        shutil.move(source_path, target_path)
        
        # åˆ›å»ºMUDæ–‡ä»¶
        mud_path, mud_message = create_mud_file(target_path, conversion_name)
        
        # åˆ›å»ºæ¨¡å‹åŒ…ZIPæ–‡ä»¶
        zip_path = None
        zip_message = ""
        if mud_path:
            zip_path, zip_message = create_model_package_zip(target_path, mud_path, conversion_name)
        
        return target_path, mud_path, zip_path, f"âœ… æˆåŠŸç§»åŠ¨å¹¶é‡å‘½å: {target_cvimodel} -> {new_filename}\n{mud_message}\n{zip_message}"
        
    except Exception as e:
        return None, None, None, f"âŒ ç§»åŠ¨.cvimodelæ–‡ä»¶å¤±è´¥: {str(e)}"

def get_download_link(file_path, file_name):
    """ç”Ÿæˆæ–‡ä»¶ä¸‹è½½é“¾æ¥"""
    try:
        with open(file_path, "rb") as f:
            bytes_data = f.read()
        b64 = base64.b64encode(bytes_data).decode()
        href = f'<a href="data:application/octet-stream;base64,{b64}" download="{file_name}">ğŸ“¥ ä¸‹è½½ {file_name}</a>'
        return href
    except Exception as e:
        return f"ç”Ÿæˆä¸‹è½½é“¾æ¥å¤±è´¥: {str(e)}"

def run_docker_training(model, epochs, imgsz):
    """è¿è¡ŒDockerè®­ç»ƒ"""
    def training_task():
        try:
            # è·å–å½“å‰æ—¶é—´æˆ³ï¼ˆç²¾ç¡®åˆ°ç§’ï¼‰ä½œä¸ºè®­ç»ƒåç§°
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            run_name = f"train_{timestamp}"
            
            set_status("running", current_run=run_name)
            clear_output()
            
            # è·å–å½“å‰ç›®å½•çš„ç»å¯¹è·¯å¾„
            current_dir = os.getcwd()
            data_path = os.path.join(current_dir, "data")
            models_path = os.path.join(current_dir, "models")
            outputs_path = os.path.join(current_dir, "outputs")
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(data_path, exist_ok=True)
            os.makedirs(models_path, exist_ok=True)
            os.makedirs(outputs_path, exist_ok=True)
            
            # å»ºç«‹æ˜ å°„å…³ç³» - è®­ç»ƒå¼€å§‹å‰å°±çŸ¥é“ptæ–‡ä»¶çš„æœ€ç»ˆä½ç½®
            future_weights_dir = os.path.join(outputs_path, run_name, "weights")
            future_best_pt = os.path.join(future_weights_dir, "best.pt")
            future_last_pt = os.path.join(future_weights_dir, "last.pt")
            
            # ä¿å­˜æ˜ å°„å…³ç³»
            save_pt_dataset_mapping(future_best_pt, data_path, run_name)
            save_pt_dataset_mapping(future_last_pt, data_path, run_name)
            
            # Dockerå‘½ä»¤ï¼Œä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„å‚æ•°
            docker_command = f'''docker run --gpus all --name yolov11-{run_name} --rm --shm-size=4g -v "{data_path}:/workspace/data" -v "{models_path}:/workspace/models" -v "{outputs_path}:/workspace/outputs" yolov11-trainer:latest bash -c "cd /workspace/models && yolo train data=/workspace/data/data.yaml model={model} epochs={epochs} imgsz={imgsz} project=/workspace/outputs name={run_name}"'''
            
            # å¯åŠ¨è¿›ç¨‹ - æ˜ç¡®æŒ‡å®šUTF-8ç¼–ç 
            process = subprocess.Popen(
                docker_command,
                shell=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                encoding='utf-8',
                errors='replace',  # é‡åˆ°æ— æ³•è§£ç çš„å­—ç¬¦æ—¶æ›¿æ¢è€Œä¸æ˜¯æŠ¥é”™
                bufsize=1
            )
            
            set_status("running", process.pid, run_name)
            
            # å®æ—¶è¯»å–è¾“å‡º
            with open(OUTPUT_FILE, 'w', encoding='utf-8', errors='replace') as f:
                f.write(f"å¼€å§‹æ‰§è¡Œå‘½ä»¤:\n{docker_command}\n\n")
                f.write(f"å·²å»ºç«‹æ˜ å°„å…³ç³»:\n")
                f.write(f"  - {future_best_pt} -> {data_path}\n")
                f.write(f"  - {future_last_pt} -> {data_path}\n\n")
                f.flush()
                
                for line in iter(process.stdout.readline, ''):
                    if line:
                        try:
                            f.write(line)
                            f.flush()
                        except UnicodeEncodeError:
                            # å¦‚æœé‡åˆ°ç¼–ç é—®é¢˜ï¼Œå°è¯•æ¸…ç†å­—ç¬¦
                            clean_line = line.encode('utf-8', errors='replace').decode('utf-8')
                            f.write(clean_line)
                            f.flush()
            
            # ç­‰å¾…å®Œæˆ
            return_code = process.wait()
            
            if return_code == 0:
                set_status("completed", current_run=run_name)
                with open(OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                    f.write("\nâœ… è®­ç»ƒå®Œæˆ!")
            else:
                set_status("failed", current_run=run_name)
                with open(OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                    f.write(f"\nâŒ è®­ç»ƒå¤±è´¥ï¼Œé€€å‡ºç : {return_code}")
                    
        except Exception as e:
            set_status("failed")
            with open(OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                f.write(f"\nâŒ æ‰§è¡Œå‡ºé”™: {str(e)}")
    
    # åå°çº¿ç¨‹è¿è¡Œ
    thread = threading.Thread(target=training_task)
    thread.daemon = True
    thread.start()

def run_model_conversion(model_path, format="onnx", opset=18):
    """è¿è¡Œæ¨¡å‹è½¬æ¢"""
    def conversion_task():
        try:
            # è·å–å½“å‰æ—¶é—´æˆ³ä½œä¸ºè½¬æ¢åç§°
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            conversion_name = f"export_{timestamp}"
            
            # è®¾ç½®çŠ¶æ€ä¸ºè½¬æ¢ä¸­
            set_status("converting", current_run=conversion_name)
            clear_conversion_output()
            
            # è·å–ptæ–‡ä»¶çš„æ˜ å°„å…³ç³»
            mapping_info = get_pt_dataset_mapping(model_path)
            
            # åˆ›å»ºtransferç›®å½•
            transfer_dir = os.path.join("transfer", conversion_name)
            os.makedirs(transfer_dir, exist_ok=True)
            
            with open(CONVERSION_OUTPUT_FILE, 'w', encoding='utf-8', errors='replace') as f:
                f.write(f"å¼€å§‹æ¨¡å‹è½¬æ¢æµç¨‹ - {conversion_name}\n")
                f.write(f"æ¨¡å‹æ–‡ä»¶: {model_path}\n")
                f.write(f"ç»å¯¹è·¯å¾„: {os.path.abspath(model_path)}\n")
                f.write(f"Transferç›®å½•: {transfer_dir}\n\n")
                
                # è°ƒè¯•æ˜ å°„å…³ç³»æŸ¥æ‰¾è¿‡ç¨‹
                f.write("=== æŸ¥æ‰¾æ•°æ®é›†æ˜ å°„å…³ç³» ===\n")
                f.write(f"æŸ¥æ‰¾è·¯å¾„: {model_path}\n")
                f.write(f"ç»å¯¹è·¯å¾„: {os.path.abspath(model_path)}\n")
                
                # æ˜¾ç¤ºæ‰€æœ‰æ˜ å°„å…³ç³»
                if os.path.exists(MAPPING_FILE):
                    with open(MAPPING_FILE, 'r', encoding='utf-8') as map_f:
                        all_mappings = json.load(map_f)
                        f.write(f"æ˜ å°„æ–‡ä»¶ä¸­å…±æœ‰ {len(all_mappings)} æ¡è®°å½•:\n")
                        for key in all_mappings.keys():
                            f.write(f"  - {key}\n")
                else:
                    f.write("æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨\n")
                
                f.write(f"æ˜ å°„æŸ¥æ‰¾ç»“æœ: {'æ‰¾åˆ°' if mapping_info else 'æœªæ‰¾åˆ°'}\n\n")
                f.flush()
                
                # å¦‚æœæ‰¾åˆ°æ˜ å°„å…³ç³»ï¼Œå…ˆå¤åˆ¶æ•°æ®é›†å›¾ç‰‡
                if mapping_info:
                    f.write("=== æ•°æ®é›†å›¾ç‰‡æ”¶é›†ä¸å¤åˆ¶ ===\n")
                    f.write(f"æ•°æ®é›†è·¯å¾„: {mapping_info['dataset_path']}\n")
                    f.write(f"Imagesè·¯å¾„: {mapping_info['images_path']}\n\n")
                    f.flush()
                    
                    # æ”¶é›†å›¾ç‰‡ - ä½¿ç”¨æ–°çš„å‡½æ•°è°ƒç”¨æ–¹å¼
                    f.write("æ­£åœ¨æ”¶é›†å›¾ç‰‡...\n")
                    f.flush()
                    all_images = collect_images_from_dataset(
                        mapping_info['images_path'], 
                        target_count=200
                    )
                    
                    f.write(f"æ‰¾åˆ° {len(all_images)} å¼ å›¾ç‰‡\n")
                    f.flush()
                    
                    if all_images:
                        # å¤åˆ¶å›¾ç‰‡
                        f.write("æ­£åœ¨å¤åˆ¶å›¾ç‰‡åˆ°transferç›®å½•...\n")
                        f.flush()
                        copied_images, test_image = copy_images_to_transfer(all_images, transfer_dir, 200)
                        
                        f.write(f"æˆåŠŸå¤åˆ¶ {len(copied_images)} å¼ å›¾ç‰‡åˆ° images/ æ–‡ä»¶å¤¹\n")
                        if test_image:
                            f.write(f"åˆ›å»ºæµ‹è¯•å›¾ç‰‡: {os.path.basename(test_image)}\n")
                        f.write("å›¾ç‰‡å¤åˆ¶å®Œæˆ!\n\n")
                        f.flush()
                    else:
                        f.write("âš ï¸ æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶ï¼Œè·³è¿‡å›¾ç‰‡å¤åˆ¶æ­¥éª¤\n\n")
                        f.flush()
                else:
                    f.write("âš ï¸ æœªæ‰¾åˆ°æ•°æ®é›†æ˜ å°„å…³ç³»ï¼Œè·³è¿‡å›¾ç‰‡å¤åˆ¶æ­¥éª¤\n\n")
                    f.flush()
                
                # å¼€å§‹ONNXè½¬æ¢
                f.write("=== ONNXæ¨¡å‹è½¬æ¢ ===\n")
                f.flush()
            
            # è·å–å½“å‰ç›®å½•çš„ç»å¯¹è·¯å¾„
            current_dir = os.getcwd()
            data_path = os.path.join(current_dir, "data")
            models_path = os.path.join(current_dir, "models")
            outputs_path = os.path.join(current_dir, "outputs")
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(data_path, exist_ok=True)
            os.makedirs(models_path, exist_ok=True)
            os.makedirs(outputs_path, exist_ok=True)
            
            # ç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡® (ç›¸å¯¹è·¯å¾„è½¬ä¸ºdockerå®¹å™¨å†…è·¯å¾„)
            docker_model_path = model_path.replace(outputs_path, "/workspace/outputs").replace("\\", "/")

            # å®šä¹‰æœŸæœ›çš„å›¾åƒå°ºå¯¸
            # ç¬¦åˆMaixCamçš„å°ºå¯¸
            imgsz_height = 224
            imgsz_width = 320
            
            # Dockerå‘½ä»¤ï¼Œä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„å‚æ•°
            docker_command = f'''docker run --gpus all --name yolo-export-{conversion_name} --rm --shm-size=4g -v "{data_path}:/workspace/data" -v "{models_path}:/workspace/models" -v "{outputs_path}:/workspace/outputs" yolov11-trainer:latest bash -c "yolo export model={docker_model_path} format={format} imgsz={imgsz_height},{imgsz_width} opset={opset} batch=1"'''
            
            # å¯åŠ¨è¿›ç¨‹ - æ˜ç¡®æŒ‡å®šUTF-8ç¼–ç 
            process = subprocess.Popen(
                docker_command,
                shell=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                encoding='utf-8',
                errors='replace',
                bufsize=1
            )
            
            # è®°å½•è¿›ç¨‹ID
            set_status("converting", process.pid, conversion_name)
            
            # å®æ—¶è¯»å–è¾“å‡º
            with open(CONVERSION_OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                f.write(f"æ‰§è¡Œè½¬æ¢å‘½ä»¤:\n{docker_command}\n\n")
                f.flush()
                
                for line in iter(process.stdout.readline, ''):
                    if line:
                        try:
                            f.write(line)
                            f.flush()
                        except UnicodeEncodeError:
                            clean_line = line.encode('utf-8', errors='replace').decode('utf-8')
                            f.write(clean_line)
                            f.flush()
            
            # ç­‰å¾…å®Œæˆ
            return_code = process.wait()
            
            # è½¬æ¢å®Œæˆåï¼Œå¤åˆ¶ONNXæ¨¡å‹åˆ°transferç›®å½•
            with open(CONVERSION_OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                if return_code == 0:
                    f.write("\n=== ONNXè½¬æ¢æˆåŠŸï¼Œå¤åˆ¶æ¨¡å‹æ–‡ä»¶ ===\n")
                    
                    # æŸ¥æ‰¾ç”Ÿæˆçš„ONNXæ–‡ä»¶
                    model_dir = os.path.dirname(model_path)
                    onnx_files = []
                    
                    if os.path.exists(model_dir):
                        for file in os.listdir(model_dir):
                            if file.endswith(".onnx"):
                                onnx_path = os.path.join(model_dir, file)
                                onnx_files.append(onnx_path)
                    
                    if onnx_files:
                        for onnx_file in onnx_files:
                            target_onnx = os.path.join(transfer_dir, os.path.basename(onnx_file))
                            shutil.copy2(onnx_file, target_onnx)
                            f.write(f"å·²å¤åˆ¶ONNXæ¨¡å‹: {os.path.basename(onnx_file)}\n")
                        
                        f.write(f"\nâœ… ONNXè½¬æ¢å’Œæ–‡ä»¶å¤åˆ¶å®Œæˆ: {transfer_dir}\n")
                        
                        # æ–°å¢ï¼šå¤åˆ¶convert_cvimodel.shæ–‡ä»¶
                        f.write("\n=== å¤åˆ¶è½¬æ¢è„šæœ¬ ===\n")
                        f.flush()
                        
                        convert_script_path = "convert_cvimodel.sh"
                        if os.path.exists(convert_script_path):
                            target_script_path = os.path.join(transfer_dir, "convert_cvimodel.sh")
                            shutil.copy2(convert_script_path, target_script_path)
                            
                            # ç¡®ä¿è„šæœ¬æœ‰æ‰§è¡Œæƒé™ï¼ˆLinux/Macï¼‰
                            try:
                                os.chmod(target_script_path, 0o755)
                            except:
                                pass  # Windowsç³»ç»Ÿå¯èƒ½ä¸æ”¯æŒchmod
                            
                            f.write(f"âœ… å·²å¤åˆ¶è½¬æ¢è„šæœ¬: {convert_script_path}\n")
                            f.flush()
                            
                            # æ–°å¢ï¼šæ‰§è¡ŒCviModelè½¬æ¢
                            f.write("\n=== æ‰§è¡ŒCviModelè½¬æ¢ ===\n")
                            f.write(f"åˆ‡æ¢åˆ°ç›®å½•: {transfer_dir}\n")
                            f.flush()
                            
                            # è·å–transferç›®å½•çš„ç»å¯¹è·¯å¾„
                            abs_transfer_dir = os.path.abspath(transfer_dir)
                            
                            # æ„å»ºdockerå‘½ä»¤
                            cvi_docker_command = f'''docker run --rm -it -v "{abs_transfer_dir}:/workspace" lintheyoung/tpuc_dev_env_build bash -c "cd /workspace && ./convert_cvimodel.sh"'''
                            
                            f.write(f"æ‰§è¡Œå‘½ä»¤:\n{cvi_docker_command}\n\n")
                            f.flush()
                            
                            # å¯åŠ¨CviModelè½¬æ¢è¿›ç¨‹
                            cvi_process = subprocess.Popen(
                                cvi_docker_command,
                                shell=True,
                                stdout=subprocess.PIPE,
                                stderr=subprocess.STDOUT,
                                universal_newlines=True,
                                encoding='utf-8',
                                errors='replace',
                                bufsize=1,
                                cwd=abs_transfer_dir  # è®¾ç½®å·¥ä½œç›®å½•
                            )
                            
                            # å®æ—¶è¯»å–CviModelè½¬æ¢è¾“å‡º
                            f.write("CviModelè½¬æ¢è¾“å‡º:\n")
                            f.write("-" * 50 + "\n")
                            f.flush()
                            
                            for line in iter(cvi_process.stdout.readline, ''):
                                if line:
                                    try:
                                        f.write(line)
                                        f.flush()
                                    except UnicodeEncodeError:
                                        clean_line = line.encode('utf-8', errors='replace').decode('utf-8')
                                        f.write(clean_line)
                                        f.flush()
                            
                            # ç­‰å¾…CviModelè½¬æ¢å®Œæˆ
                            cvi_return_code = cvi_process.wait()
                            
                            f.write("-" * 50 + "\n")
                            if cvi_return_code == 0:
                                f.write("âœ… CviModelè½¬æ¢å®Œæˆ!\n")
                                
                                # æ–°å¢ï¼šæŸ¥æ‰¾å¹¶ç§»åŠ¨.cvimodelæ–‡ä»¶ï¼ŒåŒæ—¶åˆ›å»ºMUDæ–‡ä»¶å’ŒZIPåŒ…
                                f.write("\n=== å¤„ç†CviModelæ–‡ä»¶ ===\n")
                                f.flush()
                                
                                # ä»æ¨¡å‹è·¯å¾„ä¸­æå–æ–‡ä»¶å
                                selected_model_name = os.path.basename(model_path)
                                moved_file_path, mud_file_path, zip_file_path, move_message = find_and_move_cvimodel(
                                    transfer_dir, conversion_name, selected_model_name
                                )
                                
                                f.write(f"{move_message}\n")
                                
                                if moved_file_path:
                                    f.write(f"CviModelæ–‡ä»¶è·¯å¾„: {moved_file_path}\n")
                                    f.write(f"CviModelæ–‡ä»¶å¤§å°: {os.path.getsize(moved_file_path) / (1024*1024):.2f} MB\n")
                                
                                if mud_file_path:
                                    f.write(f"MUDé…ç½®æ–‡ä»¶è·¯å¾„: {mud_file_path}\n")
                                    f.write(f"MUDæ–‡ä»¶å¤§å°: {os.path.getsize(mud_file_path) / 1024:.2f} KB\n")
                                
                                if zip_file_path:
                                    f.write(f"æ¨¡å‹åŒ…ZIPè·¯å¾„: {zip_file_path}\n")
                                    f.write(f"ZIPæ–‡ä»¶å¤§å°: {os.path.getsize(zip_file_path) / (1024*1024):.2f} MB\n")
                                
                                f.write(f"\nğŸ‰ å®Œæ•´çš„MaixCamæ¨¡å‹åŒ…å·²åˆ›å»º: {transfer_dir}\n")
                                f.write("åŒ…å«å†…å®¹:\n")
                                f.write("  - images/ (200å¼ è®­ç»ƒå›¾ç‰‡)\n")
                                f.write("  - test.png/jpg (æµ‹è¯•å›¾ç‰‡)\n")
                                f.write("  - *.onnx (ONNXæ¨¡å‹)\n")
                                f.write("  - convert_cvimodel.sh (è½¬æ¢è„šæœ¬)\n")
                                if moved_file_path:
                                    final_cvimodel_filename = os.path.basename(moved_file_path)
                                    f.write(f"  - {final_cvimodel_filename} (MaixCamä¼˜åŒ–æ¨¡å‹) ğŸ¯\n")
                                if mud_file_path:
                                    final_mud_filename = os.path.basename(mud_file_path)
                                    f.write(f"  - {final_mud_filename} (MUDé…ç½®æ–‡ä»¶) ğŸ“‹\n")
                                if zip_file_path:
                                    final_zip_filename = os.path.basename(zip_file_path)
                                    f.write(f"  - {final_zip_filename} (å®Œæ•´æ¨¡å‹åŒ…) ğŸ“¦\n")
                                
                            else:
                                f.write(f"âŒ CviModelè½¬æ¢å¤±è´¥ï¼Œé€€å‡ºç : {cvi_return_code}\n")
                                f.write("ONNXæ¨¡å‹ä»å¯æ­£å¸¸ä½¿ç”¨\n")
                            
                        else:
                            f.write(f"âš ï¸ æœªæ‰¾åˆ°è½¬æ¢è„šæœ¬: {convert_script_path}\n")
                            f.write("è¯·ç¡®ä¿convert_cvimodel.shæ–‡ä»¶å­˜åœ¨äºåº”ç”¨æ ¹ç›®å½•\n")
                            f.write("ONNXè½¬æ¢å·²å®Œæˆï¼Œå¯æ‰‹åŠ¨è¿›è¡ŒCviModelè½¬æ¢\n")
                    else:
                        f.write("âš ï¸ æœªæ‰¾åˆ°ç”Ÿæˆçš„ONNXæ–‡ä»¶\n")
                    
                    set_status("completed", current_run=conversion_name)
                    f.write("\nâœ… æ¨¡å‹è½¬æ¢å’Œæ‰“åŒ…å®Œæˆ!")
                else:
                    set_status("failed", current_run=conversion_name)
                    f.write(f"\nâŒ æ¨¡å‹è½¬æ¢å¤±è´¥ï¼Œé€€å‡ºç : {return_code}")
                    
        except Exception as e:
            set_status("failed")
            with open(CONVERSION_OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                f.write(f"\nâŒ æ‰§è¡Œå‡ºé”™: {str(e)}")
    
    # åå°çº¿ç¨‹è¿è¡Œ
    thread = threading.Thread(target=conversion_task)
    thread.daemon = True
    thread.start()

def stop_training():
    """åœæ­¢è®­ç»ƒ"""
    status = get_status()
    pid = status.get("pid")
    
    if pid:
        try:
            # å°è¯•æ€æ­»è¿›ç¨‹
            import platform
            if platform.system() == "Windows":
                subprocess.run(f"taskkill /F /PID {pid}", shell=True, check=False)
            else:
                os.kill(pid, 9)
            
            set_status("stopped")
            with open(OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                f.write(f"\nâ¹ï¸ è®­ç»ƒå·²æ‰‹åŠ¨åœæ­¢ (PID: {pid})")
                
        except Exception as e:
            with open(OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                f.write(f"\nâŒ åœæ­¢å¤±è´¥: {str(e)}")

def stop_conversion():
    """åœæ­¢è½¬æ¢è¿‡ç¨‹"""
    status = get_status()
    pid = status.get("pid")
    
    if pid:
        try:
            # å°è¯•æ€æ­»è¿›ç¨‹
            import platform
            if platform.system() == "Windows":
                subprocess.run(f"taskkill /F /PID {pid}", shell=True, check=False)
            else:
                os.kill(pid, 9)
            
            set_status("stopped")
            with open(CONVERSION_OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                f.write(f"\nâ¹ï¸ æ¨¡å‹è½¬æ¢å·²æ‰‹åŠ¨åœæ­¢ (PID: {pid})")
                
        except Exception as e:
            with open(CONVERSION_OUTPUT_FILE, 'a', encoding='utf-8', errors='replace') as f:
                f.write(f"\nâŒ åœæ­¢å¤±è´¥: {str(e)}")

def extract_training_info(output_content):
    """æå–è®­ç»ƒå…³é”®ä¿¡æ¯"""
    lines = output_content.split('\n')
    info = {
        "current_epoch": None,
        "total_epochs": None,
        "latest_metrics": None,
        "progress_percentage": 0
    }
    
    # ä»æœ€æ–°çš„å‡ è¡Œä¸­æå–ä¿¡æ¯
    for line in reversed(lines[-20:]):
        # æå–epochä¿¡æ¯
        if "Epoch" in line and "/" in line:
            try:
                epoch_part = line.split("Epoch")[1].split()[0]
                if "/" in epoch_part:
                    current, total = epoch_part.split("/")
                    info["current_epoch"] = int(current)
                    info["total_epochs"] = int(total)
                    info["progress_percentage"] = (int(current) / int(total)) * 100
                    break
            except:
                pass
    
    # æå–æœ€æ–°çš„æŒ‡æ ‡ä¿¡æ¯
    for line in reversed(lines[-10:]):
        if "mAP50" in line and "all" in line:
            info["latest_metrics"] = line.strip()
            break
    
    return info

def find_training_models():
    """æŸ¥æ‰¾è®­ç»ƒäº§ç”Ÿçš„æ¨¡å‹æ–‡ä»¶"""
    models = []
    outputs_dir = "./outputs"
    
    if os.path.exists(outputs_dir):
        for train_dir in os.listdir(outputs_dir):
            weights_dir = os.path.join(outputs_dir, train_dir, "weights")
            if os.path.isdir(weights_dir):
                # æŸ¥æ‰¾best.ptå’Œlast.ptæ–‡ä»¶
                for weight_file in ["best.pt", "last.pt"]:
                    weight_path = os.path.join(weights_dir, weight_file)
                    if os.path.exists(weight_path):
                        # æ·»åŠ æ¨¡å‹ä¿¡æ¯
                        model_info = {
                            "name": f"{train_dir}/{weight_file}",
                            "path": weight_path,
                            "size": os.path.getsize(weight_path) / (1024 * 1024),  # MB
                            "time": datetime.fromtimestamp(os.path.getmtime(weight_path))
                        }
                        models.append(model_info)
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢
    models.sort(key=lambda x: x["time"], reverse=True)
    return models

def find_converted_cvimodels():
    """æŸ¥æ‰¾è½¬æ¢å®Œæˆçš„.cvimodelæ–‡ä»¶"""
    cvimodels = []
    transfer_dir = "transfer"
    
    if os.path.exists(transfer_dir):
        for export_dir in os.listdir(transfer_dir):
            if export_dir.startswith('export_'):
                export_path = os.path.join(transfer_dir, export_dir)
                if os.path.isdir(export_path):
                    # æŸ¥æ‰¾.cvimodelæ–‡ä»¶
                    for file in os.listdir(export_path):
                        if file.endswith('.cvimodel'):
                            cvimodel_path = os.path.join(export_path, file)
                            if os.path.isfile(cvimodel_path):
                                # æ·»åŠ æ¨¡å‹ä¿¡æ¯
                                cvimodel_info = {
                                    "name": file,
                                    "path": cvimodel_path,
                                    "size": os.path.getsize(cvimodel_path) / (1024 * 1024),  # MB
                                    "time": datetime.fromtimestamp(os.path.getmtime(cvimodel_path)),
                                    "export_dir": export_dir
                                }
                                cvimodels.append(cvimodel_info)
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢
    cvimodels.sort(key=lambda x: x["time"], reverse=True)
    return cvimodels

def find_model_packages():
    """æŸ¥æ‰¾æ¨¡å‹åŒ…ZIPæ–‡ä»¶"""
    packages = []
    transfer_dir = "transfer"
    
    if os.path.exists(transfer_dir):
        for export_dir in os.listdir(transfer_dir):
            if export_dir.startswith('export_'):
                export_path = os.path.join(transfer_dir, export_dir)
                if os.path.isdir(export_path):
                    # æŸ¥æ‰¾ZIPæ–‡ä»¶
                    for file in os.listdir(export_path):
                        if file.endswith('.zip') and '_int8.zip' in file:
                            zip_path = os.path.join(export_path, file)
                            if os.path.isfile(zip_path):
                                # æ·»åŠ åŒ…ä¿¡æ¯
                                package_info = {
                                    "name": file,
                                    "path": zip_path,
                                    "size": os.path.getsize(zip_path) / (1024 * 1024),  # MB
                                    "time": datetime.fromtimestamp(os.path.getmtime(zip_path)),
                                    "export_dir": export_dir
                                }
                                packages.append(package_info)
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢
    packages.sort(key=lambda x: x["time"], reverse=True)
    return packages

def display_results():
    """æ˜¾ç¤ºè®­ç»ƒç»“æœ"""
    # è·å–å½“å‰è¿è¡ŒçŠ¶æ€
    status = get_status()
    current_run = status.get("current_run")
    current_status = status.get("status")
    
    # å¦‚æœå½“å‰æ­£åœ¨è¿è¡Œè®­ç»ƒï¼Œåˆ™ä¸æ˜¾ç¤ºç»“æœ
    if current_status == "running":
        st.info("ğŸ”„ è®­ç»ƒè¿›è¡Œä¸­ï¼Œå®Œæˆåå°†æ˜¾ç¤ºç»“æœ")
        return
    
    # å¦‚æœæ²¡æœ‰å½“å‰è¿è¡Œçš„ä»»åŠ¡ï¼Œåˆ™å¯»æ‰¾æœ€æ–°çš„ç»“æœ
    if not current_run and current_status != "completed":
        outputs_dir = "./outputs"
        if os.path.exists(outputs_dir):
            # æŸ¥æ‰¾æœ€æ–°çš„è®­ç»ƒç»“æœ
            train_dirs = []
            for item in os.listdir(outputs_dir):
                item_path = os.path.join(outputs_dir, item)
                if os.path.isdir(item_path) and item.startswith('train_'):
                    train_dirs.append(item)
            
            if train_dirs:
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„
                current_run = max(train_dirs, key=lambda x: os.path.getctime(os.path.join(outputs_dir, x)))
    
    # å¦‚æœæœ‰å½“å‰ä»»åŠ¡æˆ–æ‰¾åˆ°äº†æœ€æ–°çš„ç»“æœ
    if current_run:
        results_path = os.path.join("./outputs", current_run)
        
        if os.path.exists(results_path):
            st.subheader("ğŸ“Š è®­ç»ƒç»“æœ")
            st.write(f"ç»“æœç›®å½•: {results_path}")
            
            # æ˜¾ç¤ºç»“æœå›¾ç‰‡
            image_files = ['results.png', 'confusion_matrix.png', 'F1_curve.png', 'PR_curve.png']
            
            cols = st.columns(2)
            col_idx = 0
            
            for img_file in image_files:
                img_path = os.path.join(results_path, img_file)
                if os.path.exists(img_path):
                    with cols[col_idx % 2]:
                        st.image(img_path, caption=img_file.replace('.png', '').replace('_', ' ').title())
                    col_idx += 1
            
            # æ˜¾ç¤ºæƒé‡æ–‡ä»¶
            weights_dir = os.path.join(results_path, 'weights')
            if os.path.exists(weights_dir):
                st.subheader("ğŸ’¾ æ¨¡å‹æƒé‡")
                for weight_file in os.listdir(weights_dir):
                    weight_path = os.path.join(weights_dir, weight_file)
                    if os.path.isfile(weight_path):
                        file_size = os.path.getsize(weight_path) / (1024 * 1024)  # MB
                        st.write(f"ğŸ“ {weight_file} ({file_size:.1f} MB)")
        else:
            st.info("æš‚æ— è®­ç»ƒç»“æœï¼ˆå¯ä»¥åˆ·æ–°ä¸€ä¸‹ï¼‰")
    else:
        st.info("æš‚æ— è®­ç»ƒç»“æœï¼ˆå¯ä»¥åˆ·æ–°ä¸€ä¸‹ï¼‰")

def dataset_management_section():
    """æ•°æ®é›†ç®¡ç†éƒ¨åˆ†"""
    st.subheader("ğŸ“¦ æ•°æ®é›†ç®¡ç†")
    
    # æ˜¾ç¤ºå½“å‰æ•°æ®é›†ä¿¡æ¯
    dataset_info = get_dataset_info()
    data_yaml_exists = os.path.exists("data/data.yaml")
    
    if data_yaml_exists and dataset_info:
        st.success("âœ… æ•°æ®é›†å·²é…ç½®")
        
        col1, col2 = st.columns(2)
        with col1:
            st.info(f"**æ¥æº:** {'æ–‡ä»¶ä¸Šä¼ ' if dataset_info['source'] == 'upload' else 'URLä¸‹è½½'}")
            if dataset_info['source'] == 'upload':
                st.info(f"**æ–‡ä»¶å:** {dataset_info['filename']}")
                st.info(f"**ä¸Šä¼ æ—¶é—´:** {dataset_info['upload_time'][:19]}")
            else:
                st.info(f"**URL:** {dataset_info['url']}")
                st.info(f"**ä¸‹è½½æ—¶é—´:** {dataset_info['download_time'][:19]}")
        
        with col2:
            st.info(f"**ç±»åˆ«æ•°é‡:** {dataset_info['num_classes']}")
            st.info(f"**ç±»åˆ«åç§°:** {', '.join(dataset_info['classes'][:5])}{'...' if len(dataset_info['classes']) > 5 else ''}")
        
        # æ˜¾ç¤ºæ•°æ®é›†è¯¦ç»†ä¿¡æ¯
        with st.expander("ğŸ“‹ æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯"):
            try:
                with open("data/data.yaml", 'r', encoding='utf-8') as f:
                    yaml_content = f.read()
                st.code(yaml_content, language='yaml')
            except:
                st.error("æ— æ³•è¯»å–data.yamlæ–‡ä»¶")
                
    elif data_yaml_exists:
        st.warning("âš ï¸ å‘ç°æ•°æ®é›†æ–‡ä»¶ä½†æ— é…ç½®ä¿¡æ¯")
    else:
        st.warning("âš ï¸ æœªé…ç½®æ•°æ®é›†")
    
    # æ•°æ®é›†é…ç½®é€‰é¡¹
    st.markdown("### ğŸ”§ é…ç½®æ–°æ•°æ®é›†")
    
    # é€‰æ‹©æ•°æ®é›†æ¥æº
    dataset_source = st.radio(
        "é€‰æ‹©æ•°æ®é›†æ¥æº:",
        ["ğŸ“ ä¸Šä¼ ZIPæ–‡ä»¶", "ğŸŒ ä»URLä¸‹è½½"],
        horizontal=True
    )
    
    if dataset_source == "ğŸ“ ä¸Šä¼ ZIPæ–‡ä»¶":
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ æ•°æ®é›†ZIPæ–‡ä»¶",
            type=['zip'],
            help="è¯·ä¸Šä¼ åŒ…å«data.yamlé…ç½®æ–‡ä»¶çš„YOLOæ ¼å¼æ•°æ®é›†"
        )
        
        if uploaded_file is not None:
            st.write(f"æ–‡ä»¶å: {uploaded_file.name}")
            st.write(f"æ–‡ä»¶å¤§å°: {uploaded_file.size / (1024*1024):.1f} MB")
            
            if st.button("ğŸš€ å¤„ç†ä¸Šä¼ çš„æ•°æ®é›†", type="primary", key="process_uploaded_dataset_btn"):
                with st.spinner("å¤„ç†ä¸­..."):
                    success, message = process_uploaded_dataset(uploaded_file)
                    if success:
                        st.success(message)
                        st.rerun()
                    else:
                        st.error(message)
    
    else:  # URLä¸‹è½½
        dataset_url = st.text_input(
            "è¾“å…¥æ•°æ®é›†ä¸‹è½½URL:",
            placeholder="https://example.com/dataset.zip",
            help="è¯·æä¾›ç›´æ¥ä¸‹è½½é“¾æ¥ï¼Œæ–‡ä»¶åº”ä¸ºåŒ…å«data.yamlçš„ZIPæ ¼å¼"
        )
        
        if dataset_url:
            if st.button("ğŸš€ ä¸‹è½½å¹¶å¤„ç†æ•°æ®é›†", type="primary", key="process_url_dataset_btn"):
                with st.spinner("ä¸‹è½½å¹¶å¤„ç†ä¸­..."):
                    success, message = process_url_dataset(dataset_url)
                    if success:
                        st.success(message)
                        st.rerun()
                    else:
                        st.error(message)

def model_conversion_section():
    """æ¨¡å‹è½¬æ¢éƒ¨åˆ†"""
    st.subheader("ğŸ”„ è½¬æ¢ptä¸ºMaixCamæ¨¡å‹")
    
    # è·å–å½“å‰çŠ¶æ€
    status = get_status()
    current_status = status.get("status")
    
    # æ˜¾ç¤ºçŠ¶æ€
    status_icons = {
        "idle": "âšª å¾…æœºä¸­",
        "running": "ğŸŸ¢ è®­ç»ƒä¸­...",
        "converting": "ğŸ”„ è½¬æ¢ä¸­...",
        "completed": "âœ… å®Œæˆ",
        "failed": "âŒ å¤±è´¥",
        "stopped": "â¹ï¸ å·²åœæ­¢"
    }
    
    status_text = status_icons.get(current_status, current_status)
    st.write(f"**å½“å‰çŠ¶æ€:** {status_text}")
    
    # æŸ¥æ‰¾å¯ç”¨çš„æ¨¡å‹
    available_models = find_training_models()
    
    if not available_models:
        st.warning("âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚è¯·å…ˆå®Œæˆæ¨¡å‹è®­ç»ƒã€‚")
    else:
        st.success(f"âœ… å‘ç° {len(available_models)} ä¸ªå¯ç”¨æ¨¡å‹")
        
        # åˆ›å»ºæ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
        model_options = [f"{model['name']} ({model['size']:.1f} MB, {model['time'].strftime('%Y-%m-%d %H:%M')})" for model in available_models]
        selected_model_idx = st.selectbox(
            "é€‰æ‹©è¦è½¬æ¢çš„æ¨¡å‹:",
            range(len(model_options)),
            format_func=lambda i: model_options[i],
            help="é€‰æ‹©best.ptè·å¾—æ›´å¥½çš„ç²¾åº¦ï¼Œæˆ–é€‰æ‹©last.ptè·å¾—æœ€æ–°çš„è®­ç»ƒç»“æœ"
        )
        
        selected_model = available_models[selected_model_idx]
        st.info(f"å·²é€‰æ‹©: **{selected_model['name']}**")
        st.info(f"æ¨¡å‹è·¯å¾„: `{selected_model['path']}`")
        st.info(f"ç»å¯¹è·¯å¾„: `{os.path.abspath(selected_model['path'])}`")
        
        # æ˜¾ç¤ºæ˜ å°„å…³ç³»ä¿¡æ¯
        mapping_info = get_pt_dataset_mapping(selected_model["path"])
        if mapping_info:
            st.success("âœ… æ‰¾åˆ°æ•°æ®é›†æ˜ å°„å…³ç³»")
            with st.expander("ğŸ“‹ æŸ¥çœ‹æ˜ å°„ä¿¡æ¯"):
                st.json(mapping_info)
        else:
            st.warning("âš ï¸ æœªæ‰¾åˆ°æ•°æ®é›†æ˜ å°„å…³ç³»ï¼Œå°†è·³è¿‡å›¾ç‰‡å¤åˆ¶æ­¥éª¤")
            
            # è°ƒè¯•ä¿¡æ¯
            with st.expander("ğŸ” è°ƒè¯•æ˜ å°„å…³ç³»"):
                st.write("**æŸ¥æ‰¾çš„è·¯å¾„:**")
                st.code(selected_model["path"])
                st.write("**ç»å¯¹è·¯å¾„:**")
                st.code(os.path.abspath(selected_model["path"]))
                
                if os.path.exists(MAPPING_FILE):
                    with open(MAPPING_FILE, 'r', encoding='utf-8') as f:
                        all_mappings = json.load(f)
                        st.write("**æ˜ å°„æ–‡ä»¶ä¸­çš„æ‰€æœ‰è·¯å¾„:**")
                        for key in all_mappings.keys():
                            st.code(key)
                else:
                    st.error("æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨")
        
        # æ£€æŸ¥convert_cvimodel.shæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        convert_script_exists = os.path.exists("convert_cvimodel.sh")
        if convert_script_exists:
            st.success("âœ… æ‰¾åˆ°è½¬æ¢è„šæœ¬: convert_cvimodel.sh")
        else:
            st.warning("âš ï¸ æœªæ‰¾åˆ°è½¬æ¢è„šæœ¬: convert_cvimodel.sh")
            st.info("è¯·ç¡®ä¿convert_cvimodel.shæ–‡ä»¶å­˜åœ¨äºåº”ç”¨æ ¹ç›®å½•ï¼Œå¦åˆ™å°†è·³è¿‡CviModelè½¬æ¢æ­¥éª¤")
        
        # æ˜¾ç¤ºæ•°æ®é›†æ ‡ç­¾é¢„è§ˆ
        labels = get_dataset_labels()
        if labels:
            st.success(f"âœ… æ£€æµ‹åˆ°æ•°æ®é›†æ ‡ç­¾: {', '.join(labels[:5])}{'...' if len(labels) > 5 else ''}")
        else:
            st.warning("âš ï¸ æœªæ‰¾åˆ°æ•°æ®é›†æ ‡ç­¾ï¼ŒMUDæ–‡ä»¶å°†ä½¿ç”¨é»˜è®¤æ ‡ç­¾")
        
        # ONNXç›¸å…³å‚æ•°è®¾ç½®
        st.markdown("### âš™ï¸ ONNXè½¬æ¢å‚æ•°")
        
        # ONNX Opsetç‰ˆæœ¬
        opset_version = st.slider(
            "ONNX Opsetç‰ˆæœ¬:",
            min_value=11,
            max_value=18,
            value=18,
            step=1,
            help="ONNX Opsetç‰ˆæœ¬ï¼ŒMaixCamé€šå¸¸æ¨èä½¿ç”¨18"
        )
        
        # æ˜¾ç¤ºé«˜çº§å‚æ•°
        with st.expander("é«˜çº§å‚æ•°è®¾ç½®"):
            st.markdown("ONNXè½¬æ¢çš„é«˜çº§å‚æ•°")
            
            # è¿™äº›å‚æ•°æš‚æ—¶ä¸ä¼šå®é™…ä½¿ç”¨ï¼Œä½†ä¿ç•™UIå…ƒç´ ä¾›æœªæ¥æ‰©å±•
            st.markdown("ä»¥ä¸‹å‚æ•°å½“å‰å›ºå®š:")
            st.code("""
batch=1                  # æ‰¹æ¬¡å¤§å°å›ºå®šä¸º1ï¼Œé€‚åˆè®¾å¤‡æ¨ç†
include=['onnx']         # ä»…å¯¼å‡ºONNXæ ¼å¼
half=True                # ä½¿ç”¨FP16åŠç²¾åº¦
int8=False               # ä¸ä½¿ç”¨INT8é‡åŒ–
device=0                 # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPUè®¾å¤‡
""", language="bash")
        
        # è½¬æ¢æŒ‰é’®
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if current_status not in ["converting", "running"]:
                if st.button("ğŸš€ å¼€å§‹è½¬æ¢", type="primary", key="start_conversion_btn"):
                    # æ‰§è¡Œè½¬æ¢
                    run_model_conversion(
                        model_path=selected_model["path"], 
                        format="onnx", 
                        opset=opset_version
                    )
                    st.success("æ¨¡å‹è½¬æ¢å·²å¼€å§‹!")
                    st.rerun()
            else:
                st.button("ğŸš€ å¼€å§‹è½¬æ¢", disabled=True, key="start_conversion_btn_disabled")
        
        with col2:
            if current_status == "converting":
                if st.button("â¹ï¸ åœæ­¢è½¬æ¢", type="secondary", key="stop_conversion_btn"):
                    stop_conversion()
                    st.rerun()
            else:
                st.button("â¹ï¸ åœæ­¢è½¬æ¢", disabled=True, key="stop_conversion_btn_disabled")
        
        with col3:
            if st.button("ğŸ”„ åˆ·æ–°çŠ¶æ€", key="refresh_conversion_status_btn"):
                st.rerun()
        
        # æ˜¾ç¤ºè½¬æ¢è¿‡ç¨‹è¯´æ˜
        st.markdown("### ğŸ“‹ è½¬æ¢æµç¨‹è¯´æ˜")
        st.info(f"""
        è½¬æ¢è¿‡ç¨‹å°†åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š
        1. **æ•°æ®é›†å›¾ç‰‡æ”¶é›†**: ä»trainå’Œvalæ–‡ä»¶å¤¹æ”¶é›†å›¾ç‰‡
        2. **å›¾ç‰‡å¤åˆ¶**: å¤åˆ¶200å¼ å›¾ç‰‡åˆ°transfer/export_æ—¶é—´æˆ³/images/ç›®å½•
        3. **æµ‹è¯•å›¾ç‰‡ç”Ÿæˆ**: åˆ›å»ºtest.png/jpgæ–‡ä»¶
        4. **ONNXè½¬æ¢**: å°†ptæ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼
        5. **æ¨¡å‹å¤åˆ¶**: å°†è½¬æ¢åçš„ONNXæ¨¡å‹å¤åˆ¶åˆ°transferç›®å½•
        6. **è„šæœ¬å¤åˆ¶**: å¤åˆ¶convert_cvimodel.shåˆ°transferç›®å½• {'âœ…' if convert_script_exists else 'âŒ (è„šæœ¬ä¸å­˜åœ¨)'}
        7. **CviModelè½¬æ¢**: æ‰§è¡Œconvert_cvimodel.shç”ŸæˆMaixCamæ¨¡å‹ {'âœ…' if convert_script_exists else 'âŒ (å°†è·³è¿‡)'}
        8. **MUDæ–‡ä»¶åˆ›å»º**: è‡ªåŠ¨ç”ŸæˆMUDé…ç½®æ–‡ä»¶ {'âœ…' if convert_script_exists else 'âŒ (å¦‚æœè„šæœ¬å­˜åœ¨)'}
        9. **æ¨¡å‹æ‰“åŒ…**: å°†.cvimodelå’Œ.mudæ–‡ä»¶æ‰“åŒ…æˆZIPæ–‡ä»¶ {'âœ…' if convert_script_exists else 'âŒ (å¦‚æœè„šæœ¬å­˜åœ¨)'}
        
        æœ€ç»ˆåœ¨transfer/export_æ—¶é—´æˆ³/ç›®å½•ä¸‹å°†åŒ…å«ï¼š
        - images/ æ–‡ä»¶å¤¹ (200å¼ è®­ç»ƒå›¾ç‰‡)
        - test.png/jpg (æµ‹è¯•å›¾ç‰‡)
        - *.onnx (è½¬æ¢åçš„ONNXæ¨¡å‹æ–‡ä»¶)
        - convert_cvimodel.sh (è½¬æ¢è„šæœ¬)
        - export_æ—¶é—´æˆ³_int8.cvimodel (é‡å‘½ååçš„MaixCamæ¨¡å‹) {'âœ…' if convert_script_exists else 'âŒ (å¦‚æœè„šæœ¬å­˜åœ¨)'}
        - export_æ—¶é—´æˆ³_int8.mud (MUDé…ç½®æ–‡ä»¶) {'âœ…' if convert_script_exists else 'âŒ (å¦‚æœè„šæœ¬å­˜åœ¨)'}
        - export_æ—¶é—´æˆ³_int8.zip (å®Œæ•´æ¨¡å‹åŒ…) {'âœ…' if convert_script_exists else 'âŒ (å¦‚æœè„šæœ¬å­˜åœ¨)'}
        """)
        
        # æ˜¾ç¤ºè½¬æ¢è¾“å‡º
        st.markdown("### ğŸ“„ è½¬æ¢è¾“å‡ºæ—¥å¿—")
        conversion_output = read_conversion_output()
        
        if conversion_output:
            with st.expander("æŸ¥çœ‹è½¬æ¢æ—¥å¿—", expanded=True):
                st.text_area(
                    "è½¬æ¢æ—¥å¿—:",
                    value=conversion_output,
                    height=400,
                    key="conversion_output_area"
                )
            
            if current_status == "converting":
                auto_refresh = st.checkbox("è‡ªåŠ¨åˆ·æ–°æ—¥å¿—", value=True, key="auto_refresh_conversion_logs")
                if auto_refresh:
                    time.sleep(2)  # æ¯2ç§’åˆ·æ–°ä¸€æ¬¡
                    st.rerun()
        else:
            st.info("æš‚æ— è½¬æ¢æ—¥å¿—")
        
        # æ˜¾ç¤ºè½¬æ¢ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
        if current_status not in ["converting", "running"]:
            # æŸ¥æ‰¾transferç›®å½•ä¸­çš„ç»“æœ
            transfer_dir = "transfer"
            if os.path.exists(transfer_dir):
                export_dirs = [d for d in os.listdir(transfer_dir) if d.startswith('export_')]
                if export_dirs:
                    # æŒ‰æ—¶é—´æ’åºï¼Œæ˜¾ç¤ºæœ€æ–°çš„
                    export_dirs.sort(reverse=True)
                    latest_export = export_dirs[0]
                    export_path = os.path.join(transfer_dir, latest_export)
                    
                    st.markdown("### ğŸ“‹ è½¬æ¢ç»“æœ")
                    st.success(f"âœ… æœ€æ–°è½¬æ¢ç»“æœ: {latest_export}")
                    
                    # åˆ—å‡ºç›®å½•å†…å®¹
                    if os.path.exists(export_path):
                        st.markdown(f"**ğŸ“ {export_path} ç›®å½•å†…å®¹:**")
                        for item in os.listdir(export_path):
                            item_path = os.path.join(export_path, item)
                            if os.path.isdir(item_path):
                                item_count = len(os.listdir(item_path))
                                st.write(f"ğŸ“‚ {item}/ ({item_count} ä¸ªæ–‡ä»¶)")
                            else:
                                item_size = os.path.getsize(item_path) / (1024 * 1024)
                                st.write(f"ğŸ“„ {item} ({item_size:.2f} MB)")
                    
                    st.markdown("### ğŸ” MaixCamä½¿ç”¨è¯´æ˜")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„æ¨¡å‹åŒ…ï¼ˆZIPæ–‡ä»¶ï¼‰
                    zip_files = [f for f in os.listdir(export_path) if f.endswith('_int8.zip')]
                    has_zip_package = len(zip_files) > 0
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰cvimodelæ–‡ä»¶
                    has_cvimodel = any(f.endswith('.cvimodel') for f in os.listdir(export_path) if os.path.isfile(os.path.join(export_path, f)))
                    
                    if has_zip_package:
                        st.success("ğŸ‰ **å®Œæ•´çš„MaixCamæ¨¡å‹åŒ…å·²ç”Ÿæˆå¹¶æ‰“åŒ…!**")
                        
                        # æ˜¾ç¤ºZIPåŒ…ä¸‹è½½
                        for zip_file in zip_files:
                            zip_path = os.path.join(export_path, zip_file)
                            zip_size = os.path.getsize(zip_path) / (1024 * 1024)
                            
                            st.markdown(f"**ğŸ“¦ å®Œæ•´æ¨¡å‹åŒ…: {zip_file} ({zip_size:.2f} MB)**")
                            
                            # ç”Ÿæˆä¸‹è½½é“¾æ¥
                            download_link = get_download_link(zip_path, zip_file)
                            st.markdown(download_link, unsafe_allow_html=True)
                            
                            st.info(f"""
                            âœ¨ **æ¨èä½¿ç”¨æ¨¡å‹åŒ…ä¸‹è½½**
                            
                            **æ¨¡å‹åŒ…åŒ…å«:**
                            - ğŸ¯ {zip_file.replace('.zip', '.cvimodel')} (MaixCamä¼˜åŒ–æ¨¡å‹)
                            - ğŸ“‹ {zip_file.replace('.zip', '.mud')} (MUDé…ç½®æ–‡ä»¶)
                            
                            **ä½¿ç”¨æ–¹æ³•:**
                            1. ä¸‹è½½ä¸Šæ–¹çš„ZIPæ¨¡å‹åŒ…
                            2. è§£å‹åå°†.cvimodelå’Œ.mudæ–‡ä»¶æ”¾åœ¨åŒä¸€ç›®å½•
                            3. åœ¨MaixCamä»£ç ä¸­ç›´æ¥åŠ è½½.cvimodelæ–‡ä»¶
                            4. MUDæ–‡ä»¶åŒ…å«æ¨¡å‹çš„é…ç½®ä¿¡æ¯ï¼ŒMaixCamä¼šè‡ªåŠ¨è¯»å–
                            """)
                        
                    elif has_cvimodel:
                        st.success("ğŸ‰ **å®Œæ•´çš„MaixCamæ¨¡å‹åŒ…å·²ç”Ÿæˆ!**")
                        
                        # æŸ¥æ‰¾.cvimodelæ–‡ä»¶å¹¶æä¾›ä¸‹è½½
                        cvimodel_files = [f for f in os.listdir(export_path) if f.endswith('.cvimodel')]
                        mud_files = [f for f in os.listdir(export_path) if f.endswith('.mud')]
                        
                        if cvimodel_files:
                            cvimodel_file = cvimodel_files[0]  # å–ç¬¬ä¸€ä¸ª.cvimodelæ–‡ä»¶
                            cvimodel_path = os.path.join(export_path, cvimodel_file)
                            
                            st.markdown(f"**ğŸ¯ MaixCamä¼˜åŒ–æ¨¡å‹: {cvimodel_file}**")
                            
                            # ç”Ÿæˆä¸‹è½½é“¾æ¥
                            download_link = get_download_link(cvimodel_path, cvimodel_file)
                            st.markdown(download_link, unsafe_allow_html=True)
                        
                        # æ˜¾ç¤ºMUDæ–‡ä»¶ä¸‹è½½
                        if mud_files:
                            mud_file = mud_files[0]
                            mud_path = os.path.join(export_path, mud_file)
                            
                            st.markdown(f"**ğŸ“‹ MUDé…ç½®æ–‡ä»¶: {mud_file}**")
                            
                            # ç”Ÿæˆä¸‹è½½é“¾æ¥
                            download_link = get_download_link(mud_path, mud_file)
                            st.markdown(download_link, unsafe_allow_html=True)
                        
                        st.markdown(f"""
                        è½¬æ¢å®Œæˆçš„æ–‡ä»¶å·²æ‰“åŒ…åœ¨: `{export_path}`
                        
                        **ä½¿ç”¨æ­¥éª¤:**
                        1. ç‚¹å‡»ä¸Šæ–¹ä¸‹è½½é“¾æ¥è·å–MaixCamä¼˜åŒ–æ¨¡å‹å’Œé…ç½®æ–‡ä»¶
                        2. **ä¼˜å…ˆä½¿ç”¨ä¸‹è½½çš„ *.cvimodel æ–‡ä»¶** (ä¸“ä¸ºMaixCamä¼˜åŒ–)
                        3. å°†.cvimodelå’Œ.mudæ–‡ä»¶æ”¾åœ¨åŒä¸€ç›®å½•
                        4. å¦‚éœ€å®Œæ•´åŒ…ï¼Œå°†æ•´ä¸ª `{latest_export}` æ–‡ä»¶å¤¹å¤åˆ¶åˆ°MaixCamè®¾å¤‡
                        5. imagesæ–‡ä»¶å¤¹åŒ…å«200å¼ è®­ç»ƒå›¾ç‰‡ä¾›æµ‹è¯•
                        6. testå›¾ç‰‡å¯ç”¨äºå¿«é€ŸéªŒè¯æ¨¡å‹æ•ˆæœ
                        
                        **æ³¨æ„:** MaixCamå¯ä»¥ç›´æ¥ä½¿ç”¨.cvimodelæ–‡ä»¶ï¼Œé…åˆ.mudé…ç½®æ–‡ä»¶æ€§èƒ½æ›´ä½³ï¼
                        """)
                    else:
                        st.info("â„¹ï¸ **ONNXæ¨¡å‹åŒ…å·²ç”Ÿæˆ**")
                        st.markdown(f"""
                        è½¬æ¢å®Œæˆçš„æ–‡ä»¶å·²æ‰“åŒ…åœ¨: `{export_path}`
                        
                        **ä½¿ç”¨æ­¥éª¤:**
                        1. å°†æ•´ä¸ª `{latest_export}` æ–‡ä»¶å¤¹å¤åˆ¶åˆ°MaixCamè®¾å¤‡
                        2. ONNXæ¨¡å‹æ–‡ä»¶å¯ç›´æ¥åœ¨MaixCamä¸Šä½¿ç”¨
                        3. imagesæ–‡ä»¶å¤¹åŒ…å«200å¼ è®­ç»ƒå›¾ç‰‡ä¾›æµ‹è¯•
                        4. testå›¾ç‰‡å¯ç”¨äºå¿«é€ŸéªŒè¯æ¨¡å‹æ•ˆæœ
                        
                        **æ³¨æ„:** å¦‚éœ€.cvimodelæ–‡ä»¶ï¼Œè¯·ç¡®ä¿convert_cvimodel.shè„šæœ¬å­˜åœ¨å¹¶é‡æ–°è½¬æ¢ã€‚
                        """)
        
        # æ˜¾ç¤ºæ‰€æœ‰å†å²è½¬æ¢è®°å½•å’Œä¸‹è½½
        if os.path.exists("transfer"):
            export_dirs = [d for d in os.listdir("transfer") if d.startswith('export_')]
            if export_dirs:
                with st.expander("ğŸ“š å†å²è½¬æ¢è®°å½•ä¸ä¸‹è½½"):
                    export_dirs.sort(reverse=True)
                    
                    for export_dir in export_dirs:
                        export_path = os.path.join("transfer", export_dir)
                        if os.path.exists(export_path):
                            # è·å–ç›®å½•ä¿¡æ¯
                            dir_size = sum(os.path.getsize(os.path.join(export_path, f)) 
                                         for f in os.listdir(export_path) 
                                         if os.path.isfile(os.path.join(export_path, f)))
                            dir_size_mb = dir_size / (1024 * 1024)
                            
                            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
                            zip_files = [f for f in os.listdir(export_path) if f.endswith('_int8.zip')]
                            cvimodel_files = [f for f in os.listdir(export_path) if f.endswith('.cvimodel')]
                            mud_files = [f for f in os.listdir(export_path) if f.endswith('.mud')]
                            
                            has_zip = len(zip_files) > 0
                            has_cvimodel = len(cvimodel_files) > 0
                            has_mud = len(mud_files) > 0
                            
                            # æ˜¾ç¤ºæŒ‡ç¤ºå™¨
                            indicators = []
                            if has_zip:
                                indicators.append("ğŸ“¦")
                            if has_cvimodel:
                                indicators.append("ğŸ¯")
                            if has_mud:
                                indicators.append("ğŸ“‹")
                            if not any([has_zip, has_cvimodel, has_mud]):
                                indicators.append("ğŸ“„")
                            
                            indicator_str = "".join(indicators)
                            
                            st.write(f"**{export_dir}** ({dir_size_mb:.1f} MB) {indicator_str}")
                            
                            # ä¼˜å…ˆæ˜¾ç¤ºZIPåŒ…ä¸‹è½½
                            if has_zip:
                                for zip_file in zip_files:
                                    zip_path = os.path.join(export_path, zip_file)
                                    download_link = get_download_link(zip_path, zip_file)
                                    st.markdown(f"  â””â”€ {download_link} (æ¨è)", unsafe_allow_html=True)
                            
                            # ç„¶åæ˜¾ç¤ºå•ç‹¬çš„.cvimodelæ–‡ä»¶
                            elif has_cvimodel:
                                for cvimodel_file in cvimodel_files:
                                    cvimodel_path = os.path.join(export_path, cvimodel_file)
                                    download_link = get_download_link(cvimodel_path, cvimodel_file)
                                    st.markdown(f"  â””â”€ {download_link}", unsafe_allow_html=True)
                                
                                # æ˜¾ç¤ºå¯¹åº”çš„MUDæ–‡ä»¶
                                if has_mud:
                                    for mud_file in mud_files:
                                        mud_path = os.path.join(export_path, mud_file)
                                        download_link = get_download_link(mud_path, mud_file)
                                        st.markdown(f"  â””â”€ {download_link}", unsafe_allow_html=True)
                            
                            st.markdown("---")

def main():
    st.set_page_config(
        page_title="MaixCamçš„YOLOv11è®­ç»ƒå¹³å°",
        page_icon="ğŸ§ª",
        layout="wide"
    )
    
    st.title("ğŸ§ª MaixCamçš„YOLOv11è®­ç»ƒå¹³å°")
    st.markdown("æ”¯æŒæ•°æ®é›†ä¸Šä¼ /ä¸‹è½½ã€å‚æ•°è®¾ç½®ã€æ¨¡å‹è½¬æ¢å’ŒMaixCam CviModelç”Ÿæˆçš„å¢å¼ºç‰ˆè®­ç»ƒå¹³å°")
    
    # åˆå§‹åŒ–
    init_status()
    current_status = get_status()
    
    # ä¸»è¦å†…å®¹åŒºåŸŸ
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ“¦ æ•°æ®é›†ç®¡ç†", "ğŸš€ è®­ç»ƒæ§åˆ¶", "ğŸ“º å®æ—¶è¾“å‡º", "ğŸ“Š è®­ç»ƒç»“æœ", "ğŸ“¤ è½¬æ¢ptä¸ºMaixCamæ¨¡å‹"])
    
    with tab1:
        dataset_management_section()
    
    with tab2:
        st.subheader("ğŸš€ è®­ç»ƒæ§åˆ¶")
        
        # çŠ¶æ€æ˜¾ç¤º
        status_icons = {
            "idle": "âšª å¾…æœºä¸­",
            "running": "ğŸŸ¢ è®­ç»ƒä¸­...",
            "converting": "ğŸ”„ è½¬æ¢ä¸­...",
            "completed": "âœ… è®­ç»ƒå®Œæˆ",
            "failed": "âŒ è®­ç»ƒå¤±è´¥",
            "stopped": "â¹ï¸ å·²åœæ­¢"
        }
        
        status_text = status_icons.get(current_status["status"], current_status["status"])
        st.write(f"**å½“å‰çŠ¶æ€:** {status_text}")
        
        # æ·»åŠ è®­ç»ƒå‚æ•°è®¾ç½®
        st.markdown("### âš™ï¸ è®­ç»ƒå‚æ•°è®¾ç½®")
        
        # æ¨¡å‹é€‰æ‹©
        model_options = ["yolo11n.pt", "yolo11s.pt", "yolo11m.pt", "yolo11l.pt", "yolo11x.pt"]
        selected_model = st.selectbox(
            "é€‰æ‹©æ¨¡å‹:",
            model_options,
            index=0,
            help="é€‰æ‹©YOLOv11æ¨¡å‹ç‰ˆæœ¬ï¼Œn(nano)æœ€å°ï¼Œx(xlarge)æœ€å¤§"
        )
        
        # Epochè®¾ç½®
        epochs = st.slider(
            "è®­ç»ƒè½®æ•° (Epochs):",
            min_value=5,
            max_value=300,
            value=20,
            step=5,
            help="è®­ç»ƒå¾ªç¯çš„æ€»è½®æ•°ï¼Œæ›´å¤šçš„è½®æ•°å¯èƒ½è·å¾—æ›´å¥½çš„ç»“æœï¼Œä½†è®­ç»ƒæ—¶é—´æ›´é•¿"
        )
        
        # å›¾ç‰‡å°ºå¯¸è®¾ç½®
        img_size_options = [320, 416, 512, 640, 768, 896, 1024, 1280]
        selected_img_size = st.select_slider(
            "å›¾ç‰‡å°ºå¯¸ (Image Size):",
            options=img_size_options,
            value=640,
            help="è®­ç»ƒå›¾ç‰‡å°ºå¯¸ï¼Œæ›´å¤§çš„å°ºå¯¸å¯èƒ½æé«˜å‡†ç¡®ç‡ï¼Œä½†ä¼šå¢åŠ æ˜¾å­˜éœ€æ±‚å’Œè®­ç»ƒæ—¶é—´"
        )
        
        # é«˜çº§å‚æ•°
        with st.expander("é«˜çº§å‚æ•°è®¾ç½®"):
            st.markdown("ä»¥ä¸‹æ˜¯å½“å‰å›ºå®šçš„é«˜çº§å‚æ•°ï¼Œå°†åœ¨æœªæ¥ç‰ˆæœ¬ä¸­å¼€æ”¾è®¾ç½®")
            st.code("""
batch=16                 # æ‰¹æ¬¡å¤§å°
patience=50              # æ—©åœè€å¿ƒå€¼
optimizer='auto'         # ä¼˜åŒ–å™¨
lr0=0.01                 # åˆå§‹å­¦ä¹ ç‡
cos_lr=True              # æ˜¯å¦ä½¿ç”¨ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦
weight_decay=0.0005      # æƒé‡è¡°å‡
dropout=0.0              # ä¸¢å¼ƒç‡
label_smoothing=0.0      # æ ‡ç­¾å¹³æ»‘
""", language="bash")
        
        # æ§åˆ¶æŒ‰é’®
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if current_status["status"] in ["idle", "completed", "failed", "stopped"]:
                if st.button("ğŸš€ å¼€å§‹è®­ç»ƒ", type="primary", key="start_training_btn"):
                    # æ£€æŸ¥åŸºæœ¬æ–‡ä»¶
                    if not os.path.exists("data/data.yaml"):
                        st.error("âŒ æœªæ‰¾åˆ° data/data.yaml æ–‡ä»¶!")
                        st.info("è¯·å…ˆåœ¨'æ•°æ®é›†ç®¡ç†'æ ‡ç­¾é¡µé…ç½®æ•°æ®é›†")
                    else:
                        run_docker_training(selected_model, epochs, selected_img_size)
                        st.success("è®­ç»ƒå·²å¼€å§‹!")
                        st.rerun()
            else:
                st.button("ğŸš€ å¼€å§‹è®­ç»ƒ", disabled=True, key="start_training_btn_disabled")
        
        with col2:
            if current_status["status"] == "running":
                if st.button("â¹ï¸ åœæ­¢è®­ç»ƒ", type="secondary", key="stop_training_btn"):
                    stop_training()
                    st.rerun()
            else:
                st.button("â¹ï¸ åœæ­¢è®­ç»ƒ", disabled=True, key="stop_training_btn_disabled")
        
        with col3:
            if st.button("ğŸ”„ åˆ·æ–°çŠ¶æ€", key="refresh_training_status_btn"):
                st.rerun()
        
        with col4:
            if st.button("ğŸ§¹ æ¸…ç©ºæ—¥å¿—", key="clear_logs_btn"):
                clear_output()
                st.success("æ—¥å¿—å·²æ¸…ç©º")
                st.rerun()
        
        # æ˜¾ç¤ºDockerå‘½ä»¤
        with st.expander("ğŸ” æŸ¥çœ‹æ‰§è¡Œçš„Dockerå‘½ä»¤"):
            current_dir = os.getcwd()
            data_path = os.path.join(current_dir, "data")
            models_path = os.path.join(current_dir, "models")
            outputs_path = os.path.join(current_dir, "outputs")
            
            # ä½¿ç”¨å½“å‰é€‰æ‹©çš„å‚æ•°ç”Ÿæˆå‘½ä»¤é¢„è§ˆ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            run_name = f"train_{timestamp}"
            
            docker_cmd = f'''docker run --gpus all --name yolov11-{run_name} --rm --shm-size=4g \\
    -v "{data_path}:/workspace/data" \\
    -v "{models_path}:/workspace/models" \\
    -v "{outputs_path}:/workspace/outputs" \\
    yolov11-trainer:latest bash -c "
    cd /workspace/models && yolo train \\
    data=/workspace/data/data.yaml \\
    model={selected_model} \\
    epochs={epochs} \\
    imgsz={selected_img_size} \\
    project=/workspace/outputs \\
    name={run_name}
"'''
            st.code(docker_cmd, language='bash')
    
    with tab3:
        st.subheader("ğŸ“º è®­ç»ƒè¾“å‡º")
        
        output_content = read_output()
        if output_content:
            # æå–è®­ç»ƒå…³é”®ä¿¡æ¯
            training_info = extract_training_info(output_content)
            
            # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦æ‘˜è¦
            if training_info["current_epoch"]:
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ğŸ“Š å½“å‰Epoch", f"{training_info['current_epoch']}/{training_info['total_epochs']}")
                with col2:
                    st.metric("ğŸ“ˆ è®­ç»ƒè¿›åº¦", f"{training_info['progress_percentage']:.1f}%")
                with col3:
                    if training_info["latest_metrics"]:
                        # ç®€åŒ–æ˜¾ç¤ºæœ€æ–°æŒ‡æ ‡
                        if "mAP50-95" in training_info["latest_metrics"]:
                            try:
                                map_value = training_info["latest_metrics"].split()[-1]
                                st.metric("ğŸ¯ mAP50-95", map_value)
                            except:
                                st.metric("ğŸ¯ æœ€æ–°æŒ‡æ ‡", "è®¡ç®—ä¸­...")
                
                # æ˜¾ç¤ºè¿›åº¦æ¡
                progress_bar = st.progress(training_info['progress_percentage'] / 100)
            
            # æ˜¾ç¤ºæœ€æ–°çš„å‡ è¡Œæ—¥å¿—ï¼ˆç½®é¡¶æ˜¾ç¤ºï¼‰
            st.markdown("**ğŸ”¥ æœ€æ–°æ—¥å¿—:**")
            lines = output_content.split('\n')
            
            # è¿‡æ»¤æ‰ç©ºè¡Œï¼Œå–æœ€å10è¡Œæœ‰å†…å®¹çš„æ—¥å¿—
            non_empty_lines = [line for line in lines if line.strip()]
            recent_lines = non_empty_lines[-10:] if len(non_empty_lines) >= 10 else non_empty_lines
            
            # åè½¬æ˜¾ç¤ºé¡ºåºï¼Œæœ€æ–°çš„åœ¨ä¸Šé¢
            recent_lines_reversed = list(reversed(recent_lines))
            recent_content = '\n'.join(recent_lines_reversed)
            
            # ä½¿ç”¨ä¸åŒçš„æ˜¾ç¤ºæ–¹å¼
            log_container = st.container()
            with log_container:
                st.code(recent_content, language=None)
            
            # æ˜¾ç¤ºé€‰é¡¹
            col1, col2 = st.columns(2)
            with col1:
                show_full_log = st.checkbox("æ˜¾ç¤ºå®Œæ•´æ—¥å¿—", value=False, key="show_full_logs_checkbox")
            with col2:
                auto_scroll = st.checkbox("è‡ªåŠ¨åˆ·æ–°", value=True, key="auto_refresh_logs_checkbox")
            
            # æ˜¾ç¤ºå®Œæ•´æ—¥å¿—ï¼ˆå¯é€‰ï¼‰
            if show_full_log:
                st.markdown("**ğŸ“‹ å®Œæ•´è®­ç»ƒæ—¥å¿—:**")
                st.text_area(
                    "æ‰€æœ‰æ—¥å¿—å†…å®¹:",
                    value=output_content,
                    height=300,
                    key="full_output_area"
                )
            
            # æ˜¾ç¤ºæ—¥å¿—ç»Ÿè®¡
            total_lines = len([line for line in lines if line.strip()])
            st.caption(f"ğŸ“Š æ€»è®¡ {total_lines} è¡Œæœ‰æ•ˆæ—¥å¿— | ğŸ•’ æœ€åæ›´æ–°: {datetime.now().strftime('%H:%M:%S')}")
            
        else:
            st.info("æš‚æ— è¾“å‡ºå†…å®¹")
        
        # å¦‚æœæ­£åœ¨è¿è¡Œï¼Œè‡ªåŠ¨åˆ·æ–°ï¼ˆé»˜è®¤å¼€å¯ï¼‰
        if current_status["status"] == "running" and 'auto_scroll' in locals() and auto_scroll:
            time.sleep(2)  # æ¯2ç§’åˆ·æ–°ä¸€æ¬¡
            st.rerun()
    
    with tab4:
        display_results()
    
    with tab5:
        model_conversion_section()
    
    # åº•éƒ¨ä¿¡æ¯
    st.markdown("---")
    st.markdown("ğŸ’¡ **ä½¿ç”¨è¯´æ˜:**")
    st.markdown("1. åœ¨ 'æ•°æ®é›†ç®¡ç†' æ ‡ç­¾é¡µä¸Šä¼ ZIPæ–‡ä»¶æˆ–æä¾›ä¸‹è½½URL")
    st.markdown("2. ç³»ç»Ÿä¼šè‡ªåŠ¨æŸ¥æ‰¾å¹¶éªŒè¯data.yamlé…ç½®æ–‡ä»¶")
    st.markdown("3. åœ¨ 'è®­ç»ƒæ§åˆ¶' æ ‡ç­¾é¡µè®¾ç½®è®­ç»ƒå‚æ•°")
    st.markdown("4. ç¡®ä¿ Docker é•œåƒ `yolov11-trainer:latest` å·²æ„å»º")
    st.markdown("5. åœ¨ 'è®­ç»ƒæ§åˆ¶' æ ‡ç­¾é¡µå¯åŠ¨è®­ç»ƒè¿‡ç¨‹")
    st.markdown("6. åœ¨ 'å®æ—¶è¾“å‡º' æ ‡ç­¾é¡µæŸ¥çœ‹è®­ç»ƒè¿›åº¦")
    st.markdown("7. è®­ç»ƒå®Œæˆååœ¨ 'è®­ç»ƒç»“æœ' æ ‡ç­¾é¡µæŸ¥çœ‹ç»“æœ")
    st.markdown("8. åœ¨ 'è½¬æ¢ptä¸ºMaixCamæ¨¡å‹' æ ‡ç­¾é¡µå°†è®­ç»ƒå¥½çš„æ¨¡å‹è½¬æ¢å¹¶æ‰“åŒ…")
    st.markdown("9. è½¬æ¢å®Œæˆåå¯ç›´æ¥ä¸‹è½½å®Œæ•´çš„.zipæ¨¡å‹åŒ…ç”¨äºMaixCamè®¾å¤‡")
    
    st.markdown("### ğŸ“‹ æ•°æ®é›†æ ¼å¼è¦æ±‚:")
    st.markdown("- æ•°æ®é›†åº”ä¸ºZIPæ ¼å¼")
    st.markdown("- åŒ…å«æ ‡å‡†çš„YOLOæ ¼å¼ç›®å½•ç»“æ„")
    st.markdown("- å¿…é¡»åŒ…å«data.yamlé…ç½®æ–‡ä»¶")
    st.markdown("- æ”¯æŒçš„é…ç½®å­—æ®µ: train, val, names, nc(å¯é€‰)")
    
    st.markdown("### ğŸ”„ æ¨¡å‹è½¬æ¢åŠŸèƒ½:")
    st.markdown("- è‡ªåŠ¨å»ºç«‹PTæ–‡ä»¶ä¸æ•°æ®é›†çš„æ˜ å°„å…³ç³»")
    st.markdown("- ä»è®­ç»ƒæ•°æ®é›†ä¸­å¤åˆ¶200å¼ å›¾ç‰‡ä¾›æµ‹è¯•")
    st.markdown("- åˆ›å»ºæµ‹è¯•å›¾ç‰‡æ–‡ä»¶")
    st.markdown("- è½¬æ¢PTæ¨¡å‹ä¸ºONNXæ ¼å¼")
    st.markdown("- å¤åˆ¶convert_cvimodel.shè„šæœ¬å¹¶æ‰§è¡ŒCviModelè½¬æ¢")
    st.markdown("- è‡ªåŠ¨é‡å‘½å.cvimodelæ–‡ä»¶ä¸ºexport_æ—¶é—´æˆ³_int8.cvimodelæ ¼å¼")
    st.markdown("- **æ–°å¢ï¼šè‡ªåŠ¨ç”Ÿæˆ.mudé…ç½®æ–‡ä»¶ï¼ŒåŒ…å«æ¨¡å‹é…ç½®å’Œæ•°æ®é›†æ ‡ç­¾**")
    st.markdown("- **æ–°å¢ï¼šå°†.cvimodelå’Œ.mudæ–‡ä»¶æ‰“åŒ…æˆå®Œæ•´çš„.zipæ¨¡å‹åŒ…**")
    st.markdown("- æä¾›ç›´æ¥ä¸‹è½½é“¾æ¥ï¼Œæ”¯æŒMaixCamç›´æ¥ä½¿ç”¨")
    
    st.markdown("### ğŸ“ MaixCamè½¬æ¢è¦æ±‚:")
    st.markdown("- éœ€è¦åœ¨åº”ç”¨æ ¹ç›®å½•æ”¾ç½® `convert_cvimodel.sh` è„šæœ¬")
    st.markdown("- éœ€è¦ `lintheyoung/tpuc_dev_env_build` Dockeré•œåƒ")
    st.markdown("- ç”Ÿæˆçš„ `.cvimodel` æ–‡ä»¶ä¸“ä¸ºMaixCamä¼˜åŒ–ï¼Œæ€§èƒ½æ›´ä½³")
    st.markdown("- **æ–°å¢ï¼šè‡ªåŠ¨ç”Ÿæˆçš„.mudæ–‡ä»¶åŒ…å«å®Œæ•´çš„æ¨¡å‹é…ç½®ä¿¡æ¯**")
    st.markdown("- **æ–°å¢ï¼šå®Œæ•´çš„.zipæ¨¡å‹åŒ…åŒ…å«.cvimodelå’Œ.mudæ–‡ä»¶ï¼Œå³ä¸‹å³ç”¨**")
    st.markdown("- è½¬æ¢å®Œæˆåæ–‡ä»¶ä¼šè‡ªåŠ¨é‡å‘½åå¹¶å¯ç›´æ¥ä¸‹è½½å®Œæ•´åŒ…")
    
    st.markdown("### ğŸ“¦ MUDæ–‡ä»¶è¯´æ˜:")
    st.markdown("- MUDæ–‡ä»¶æ˜¯MaixCamæ¨¡å‹çš„é…ç½®æ–‡ä»¶")
    st.markdown("- åŒ…å«æ¨¡å‹ç±»å‹ã€è¾“å…¥æ ¼å¼ã€é¢„å¤„ç†å‚æ•°ç­‰ä¿¡æ¯")
    st.markdown("- è‡ªåŠ¨ä»data.yamlä¸­æå–ç±»åˆ«æ ‡ç­¾ä¿¡æ¯")
    st.markdown("- ä¸.cvimodelæ–‡ä»¶é…å¥—ä½¿ç”¨ï¼Œç®€åŒ–MaixCaméƒ¨ç½²æµç¨‹")

if __name__ == "__main__":
    main()